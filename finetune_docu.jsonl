{"messages": [{"role": "system", "content": "You are a coding expert and know how to write PyCRAM Plan Code."}, {"role": "user", "content": "What information can I find on the PyCRAM tutorial page?"}, {"role": "assistant", "content": "Welcome to pycram\n\n==================================\nWelcome to pycram's documentation!\n==================================\n\n.. image:: ../images/pycram_logo.png\n   :alt: Pycram Logo\n\nWhat is PyCRAM?\n===============\n\nPyCRAM is the Python 3 re-implementation of `CRAM <https://github.com/cram2/cram>`_.\nPyCRAM is a toolbox for designing, implementing and deploying software on autonomous robots.\nThe framework provides various tools and libraries for aiding in robot software development as well as geometric\nreasoning and fast simulation mechanisms to develop cognition-enabled control programs that achieve high levels of robot\nautonomy.\n\nPyCRAM is developed in Python with support for the ROS middleware which is used for communication with different\nsoftware components as well as the robot.\n\nThis framework is tested with Ubuntu 20.04, ROS Noetic and Python 3.8\n\n\nSimple Demonstration\n\nPyCRAM allows the execution of the same high-level plan on different robot platforms. Below you can see an example of\nthis where the plan is executed on the PR2 and the IAIs Boxy.\n\n.. list-table::\n   :widths: 50 50\n   :header-rows: 1\n\n   *  - Boxy\n      - PR2\n   *  - .. image:: ../images/boxy.gif\n            :alt: Boxy robot performing tasks using pycram\n      - .. image:: ../images/pr2.gif\n            :alt: PR2 robot performing tasks using pycram\n\n\nThe plan that both robots execute is a relatively simple pick and place plan:\n\n * They start at the world origin\n * park their arms\n * move to the counter\n * observe the object\n * pickup the object\n * move to the kitchen island\n * place the object\n * move to the world origin\n\nThe code for this plan can be seen below.\n\n.. code-block:: python\n\n    from pycram.world.bullet_world import BulletWorld\n    from pycram.world_concepts.world_concepts import Object\n    from pycram.process_module import simulated_robot\n    from pycram.designators.motion_designator import *\n    from pycram.designators.location_designator import *\n    from pycram.designators.action_designator import *\n    from pycram.designators.object_designator import *\n    from pycram.datastructures.enums import ObjectType, Arms, Grasps\n\n    world = BulletWorld()\n    kitchen = Object(\"kitchen\", ObjectType.ENVIRONMENT, \"kitchen.urdf\")\n    robot = Object(\"pr2\", ObjectType.ROBOT, \"pr2.urdf\")\n    cereal = Object(\"cereal\", ObjectType.BREAKFAST_CEREAL, \"breakfast_cereal.stl\", position=[1.4, 1, 0.95])\n\n    cereal_desig = ObjectDesignatorDescription(names=[\"cereal\"])\n    kitchen_desig = ObjectDesignatorDescription(names=[\"kitchen\"])\n    robot_desig = ObjectDesignatorDescription(names=[\"pr2\"]).resolve()\n\n    with simulated_robot:\n        ParkArmsAction([Arms.BOTH]).resolve().perform()\n\n        MoveTorsoAction([0.3]).resolve().perform()\n\n        pickup_pose = CostmapLocation(target=cereal_desig.resolve(), reachable_for=robot_desig).resolve()\n        pickup_arm = pickup_pose.reachable_arms[0]\n\n        NavigateAction(target_locations=[pickup_pose.pose]).resolve().perform()\n\n        PickUpAction(object_designator_description=cereal_desig, arms=[pickup_arm], grasps=[Grasps.FRONT]).resolve().perform()\n\n        ParkArmsAction([Arms.BOTH]).resolve().perform()\n\n        place_island = SemanticCostmapLocation(\"kitchen_island_surface\", kitchen_desig.resolve(), cereal_desig.resolve()).resolve()\n\n        place_stand = CostmapLocation(place_island.pose, reachable_for=robot_desig, reachable_arm=pickup_arm).resolve()\n\n        NavigateAction(target_locations=[place_stand.pose]).resolve().perform()\n\n        PlaceAction(cereal_desig, target_locations=[place_island.pose], arms=[pickup_arm]).resolve().perform()\n\n        ParkArmsAction([Arms.BOTH]).resolve().perform()\n\n    world.exit()"}]}
{"messages": [{"role": "system", "content": "You are a coding expert and know how to write PyCRAM Plan Code."}, {"role": "user", "content": "What are the key concepts covered in the Designators section of the PyCRAM documentation?"}, {"role": "assistant", "content": "Designators\n\n\nDesignators are CRAMs and PyCRAMs way of representing actions, motions, objects and locations.\n\nIn general, PyCRAM-Designators consist of a description and a specified element.\nDescriptions describe sets of designators and designators are one thing in the described set.\nFor example, such a description could describe an action where the robot moves to a location\nfrom where it can grasp an object. The specific location in this case is not relevant as long\nas the robot can reach the object.\n\nThe designator description will be resolved during runtime which results in a designator with\nspecific parameter, the resulting designator can also be performed to let the robot perform the\ndesired behaviour.\n\nTo stay on the example of an action designator which should move the robot to a location from\nwhere it can grasp an object, we will create a NavigateAction description with a list of possible\nposes.\n\n.. code-block:: python\n\n    poses = [Pose([1, 0, 0], [0, 0, 0, 1]), Pose([1.2, 0.2, 0], [0, 0, 1, 0])]\n    NavigateAction(target_locations=poses)\n\nThis is a description of an action which moves the robot to a pose in the environment.\nIn this case the description describes a set of action designators which have a pose of the ``poses``\nlist as target location.\n\nBy resolving the description it will return a single designator with specific parameter.\nThis can look like this:\n\n.. code-block:: python\n\n    designator = NavigateAction(target_locations=poses).resolve()\n\nThe resolver will check the available parameter and return a designator with appropriate parameter\nwhich can then be performed. The designator returned by the resolver will always be a sub-class\nof the description, in this way the designator can be assigned the type of action it performs.\nThe resolved designator can also contain new parameter which might be relevant for performing the\ndesignator and will be inferred while resolving. A resolved action designator for a navigate\naction looks like this:\n\n.. code-block:: python\n\n    NavigateActionPerformable(robot_position=(Pose([0.0, 0.0, 0.0] [0.0, 0.0, 0.0, 1.0]), target_location=Pose([1, 0, 0], [0, 0, 0, 1]))\n\n\nA visual representation of the whole idea of designator and designator descriptions can be\nseen in the following image.\n\n.. image:: ../images/designators.png\n   :alt: Schematic representation of Designators.\n\n\n\nThere are four types of designators in PyCRAM:\n\n - :mod:`pycram.designators.action_designator`\n - :mod:`pycram.designators.object_designator`\n - :mod:`pycram.designators.location_designator`\n - :mod:`pycram.designators.motion_designator`\n\nObject Designator\n=================\n\nObject designators represent objects in (simulated) world.\nThe description of object designators can take names and types that the object should match.\nThe :meth:`~pycram.designator.ObjectDesignatorDescription.ground` method returns an object with all\nits data attached that matches the description.\nThe :meth:`~pycram.designator.ObjectDesignatorDescription.__iter__` method iterates over all objects\nthat match the description.\n\nContributing Object Designators\n\nObject Designators should always be part of an object designator description.\nThe general class structure is seen in :mod:`~pycram.designator.ObjectDesignatorDescription`.\nNew object description need to inherit from the general object description. If the object they ground to differs from\nthe base object, a `dataclass <https://docs.python.org/3/library/dataclasses.html>`_. should be created inside the new\ndescription. The dataclass is one element that matches the description.\nIf ORM logging of the new objects is wanted a ``to_sql()`` and ``insert()`` method has to be implemented\n(see :ref:`orm` for more details).\n\n\nAction Designator\n=================\nAction designators describe complex actions that are executable for an agent. Action designators can be seen as higher\nlevel plans that include failure handling and parametrization.\nAn action designator description always takes the parameter as a list of possible parameter, when\nresolving the description to a single designator one parameter out of the given list will be picked.\n\nMotion Designator\n=================\nMotion designators describe atomic actions that are executable for an agent. In contrast to action\ndesignators there is no failure handling or other action designators. Furthermore, the :meth:`~pycram.designator.MotionDesignatorDescription.Motion.perform`\nmethod passes the resolved motion designator to the respective Process Module for execution on the robot.\n\nAnother difference to action designator is that motion designators only take a single parameter instead of a\nlist, this parameter is also strictly typed.\n\nLocation Designator\n===================\nLocation designator describe a set of locations in regards to specific constrains. These constrains can be things\nlike ``reachable`` or ``visible``. The pose returned by a location designator is a single pose of the set defined\nby the constrains given to the location designator description.\n\nSimilar to object designator poses location designator also a :meth:`~pycram.designators.location_designator.CostmapLocation.__iter__`\nmethod which can iterate over all possible solutions for this description.\n\nCreating your own Designator\n============================\nCreating your own designator is fairly easy, you only need to extend the base class of the respective description.\n\n - :mod:`~pycram.designator.ActionDesignatorDescription`\n - :mod:`~pycram.designator.ObjectDesignatorDescription`\n - :mod:`~pycram.designator.LocationDesignatorDescription`\n - :mod:`~pycram.designator.BaseMotion`\n\nAfterwards you need to implement your own ``ground`` method which is the default resolver and for location and object\ndesignator it makes sense to also implement a ``__iter__`` method. The ``ground`` and ``__iter__`` methods should return\nthe designator sub-class so you also need to implement these with the parameter your designator needs.\n\nThe sub-class can already contain some parameters, this is usually the case if the parameter is the same for every designator\nof this type. For example, :class:`~pycram.designator.LocationDesignatorDescription.Location`\ncontains a ``pose`` parameter since every location designator contains a resolved pose.\n\nFor action and motion designator the sub-class is also the place where the ``perform`` method is written which contains\nthe behaviour of the designator."}]}
{"messages": [{"role": "system", "content": "You are a coding expert and know how to write PyCRAM Plan Code."}, {"role": "user", "content": "What common issues are addressed in the Troubleshooting section of the PyCRAM documentation?"}, {"role": "assistant", "content": "Troubleshooting\n\nThis page contains the most common errors that could happen when using PyCRAM and how to resolve them.\n\n\nRos Init Exception\n\n.. code-block:: Python\n\n    ROSInitException: time is not initialized. Have you called init_node()\n\nThis exception usually occurs when trying to load an Object into the BulletWorld or when creating a Pose. The reason for\nthis exception is that the ROS node of PyCRAM could not be initialized, this is usually the case when the PyCRAM launch\nfile was not launched.\n\nTo solve this problem you just have to launch the ``ik_and_description`` file of PyCRAM. This can be done via the\nfollowing command.\n\n.. code-block:: shell\n\n    roslaunch pycram ik_and_description\n\n\nRobot Description not Loaded\n\nIn PyCRAM a lot of things are base on the robot description that is currently loaded and in turn the robot description\nthat will be loaded depends on the robot description that is on the ROS parameter server at the time PyCRAM is started.\n\nIf you get an error that looks similar to the following exception the most likely case is that the robot description was\nnot loaded.\n\n.. code-block:: python\n   :emphasize-lines: 3\n\n        763 with open(self.path) as f:\n        764     self.urdf_object = URDF.from_xml_string(f.read())\n    --> 765     if self.urdf_object.name == robot_description.name and not BulletWorld.robot:\n        766         BulletWorld.robot = self\n        768 self.links[self.urdf_object.get_root()] = -1\n\n    AttributeError: 'NoneType' object has no attribute 'name'\n\n\nThere could be a few reasons for this behaviour:\n   * The name of the URDF on the parameter server and the name in the PyCRAM robot description are different\n      * The PyCRAM robot description is matched by the name of the robot in the URDF\n      * You can check the assignment of the PyCRAM robot description in this function :func:`~pycram.robot_descriptions.update_robot_description` (there is a link to the source code)\n   * You only started a roscore\n   * You did not start the ``ik_and_description`` launch file\n\n\nStop Iteration\n\nStop iterations usually happen when you try to resolve a designator for which there is no solution or if you iterate over a\ndesignator and reached the end of all possible solutions.\n\nWhen you try to resolve a designator which has no solution the error will look something like this.\n\n.. code-block:: python\n   :emphasize-lines: 7\n\n       753 def ground(self) -> Union[Object, bool]:\n       754     \"\"\"\n       755     Return the first object from the bullet world that fits the description.\n       756\n       757     :return: A resolved object designator\n       758     \"\"\"\n   --> 759     return next(iter(self))\n\n   StopIteration:\n\nIf you encounter such an error the most likely reason is that you put the wrong arguments into your DesignatorDescription.\nThe best solution is to double check the input arguments of the DesignatorDescription.\n\n\n\nBulletWorld crashes after loading an URDF\n\nIt can happen that the BulletWorld crashes when loading a URDF. This happens when loading a URDF with more than 127 links.\nThis is a limitation of PyBullet which can not deal with objects with more links.\n\nOnly real solution here is to either delete links such that you have at max 127 links or split up the URDF and load the\nsplit parts individually.\n\n\nError when performing Actions or Motions\n\n.. code-block:: python\n\n        30 def perform(self):\n        31     pm_manager = ProcessModuleManager.get_manager()\n   ---> 32     return pm_manager.navigate().execute(self)\n\n   AttributeError: 'NoneType' object has no attribute 'navigate'\n\nIf you get an error like this when trying to perform an action or motion designator, then you did not specify how the\ndesignator should be executed. You can specify how the designator should be performed by using the simulated_robot or\nreal_robot environments. This is also explained in the `Action Designator Example <https://pycram.readthedocs.io/en/latest/notebooks/action_designator.html#Navigate-Action>`_.\n\n.. code-block:: python\n\n   with simulated_robot:\n      NavigateAction([Pose()]).resolve().perform()\n\nMissing pr2_arm_kinematics\n==========================\n\nAptitudes autoremove likes to also remove the arm kinematics. The error message looks similar to this. Important is the\nmissing library libmoveit_kinematics_base.so. This can be fixed by reinstalling the missing libraries.\n\n.. code-block:: shell\n\n   process[pr2_left_arm_kinematics-3]: started with pid [26862]\n   pr2_arm_kinematics_node: error while loading shared libraries: libmoveit_kinematics_base.so.1.1.12: cannot open shared object file: No such file or directory\n   process[pr2_right_arm_kinematics-4]: started with pid [26863]\n   [pr2_left_arm_kinematics-3] process has died [pid 26862, exit code 127, cmd ~/pycram/devel/lib/pr2_arm_kinematics/pr2_arm_kinematics_node __name:=pr2_left_arm_kinematics __log:=~/.ros/log/ba5e95de-384f-11ee-ab53-97c8787037e5/pr2_left_arm_kinematics-3.log].\n   log file: ~/.ros/log/ba5e95de-384f-11ee-ab53-97c8787037e5/pr2_left_arm_kinematics-3*.log\n   pr2_arm_kinematics_node: error while loading shared libraries: libmoveit_kinematics_base.so.1.1.12: cannot open shared object file: No such file or directory\n   [pr2_right_arm_kinematics-4] process has died [pid 26863, exit code 127, cmd ~/pycram/devel/lib/pr2_arm_kinematics/pr2_arm_kinematics_node __name:=pr2_right_arm_kinematics __log:=~/.ros/log/ba5e95de-384f-11ee-ab53-97c8787037e5/pr2_right_arm_kinematics-4.log].\n   log file: ~/.ros/log/ba5e95de-384f-11ee-ab53-97c8787037e5/pr2_right_arm_kinematics-4*.log\n   IK server ready.\n\n\nReinstall the missing libraries with\n\n.. code-block:: shell\n\n    sudo apt-get install ros-noetic-moveit\n\nThen rebuild your workspace."}]}
{"messages": [{"role": "system", "content": "You are a coding expert and know how to write PyCRAM Plan Code."}, {"role": "user", "content": "What are the key features of PyCRAM mentioned in the introduction of the documentation?"}, {"role": "assistant", "content": "PyCRAM Introduction\n\n```python\nimport pycram\n```\n\n# Bullet World\n\nThe BulletWorld is the internal simulation of PyCRAM. You can simulate different actions and reason about the outcome of\ndifferent actions.\n\nIt is possible to spawn objects and robots into the BulletWorld, these objects can come from URDF, OBJ or STL files.\n\nA BulletWorld can be created by simply creating an object of the BulletWorld class.\n\n```python\nfrom pycram.worlds.bullet_world import BulletWorld\nfrom pycram.world_concepts.world_object import Object\nfrom pycram.datastructures.enums import ObjectType\nfrom pycram.datastructures.pose import Pose\n\nworld = BulletWorld()\n```\n\nThe BulletWorld allows to render images from arbitrary positions. In the following example we render images with the\ncamera at the position [0.3, 0, 1] and pointing towards [1, 0, 1], so we are looking upwards along the x-axis.\n\nThe renderer returns 3 different kinds of images which are also shown on the left side of the BulletWorld window. (If\nthese winodws are missing, click the BulletWorld window to focus it, and press \"g\") These images are:\n\n* An RGB image which shows everything like it is rendered in the BulletWorld window, just from another perspective.\n* A depth image which consists of distance values from the camera towards the objects in the field of view.\n* A segmentation mask image which segments the image into the different objects displayed. The segmentation is done by\n  assigning every pixel the unique id of the object that is displayed there.\n\n```python\nworld.get_images_for_target(Pose([1, 0, 1], [0, 0, 0, 1]), Pose([0.3, 0, 1], [0, 0, 0, 1]))\n```\n\n## Objects\n\nEverything that is located inside the BulletWorld is an Object.\nObjects can be created from URDF, OBJ or STL files. Since everything is of type Object a robot might share the same\nmethods as a milk (with some limitations).\n\nSignature:\nObject:\n\n* Name\n* Type\n* Filename or Filepath\n\nOptional:\n\n* Position\n* Orientation\n* World\n* Color\n* Ignore Cached Files\n\nIf there is only a filename and no path, PyCRAM will check in the resource directory if there is a matching file.\n\n```python\nmilk = Object(\"Milk\", ObjectType.MILK, \"milk.stl\")\n```\n\nObjects provide methods to change the position and rotation, change the color, attach other objects, set the state of\njoints if the objects has any or get the position and orientation of a link.\n\nThese methods are the same for every Object, however since some Objects may not have joints or more than one link\nmethods related to these will not work.\n\n```python\nmilk.set_position(Pose([1, 0, 0]))\n```\n\nTo remove an Object from the BulletWorld just call the 'remove' method on the Object.\n\n```python\nmilk.remove()\n```\n\nSince everything inside the BulletWorld is an Object, even a complex environment Object like the kitchen can be spawned\nin the same way as the milk.\n\n```python\nkitchen = Object(\"kitchen\", ObjectType.ENVIRONMENT, \"kitchen.urdf\")\n```\n\n## Costmaps\n\nCostmaps are a way to get positions with respect to certain criterias.\nThe currently available costmaps are:\n\n* {class}`~pycram.costmaps.OccupancyCostmap`\n* {class}`~pycram.costmaps.VisibilityCostmap`\n* {class}`~pycram.costmaps.SemanticCostmap`\n* {class}`~pycram.costmaps.GaussianCostmap`\n\nIt is also possible to merge multiple costmaps to combine different criteria.\n\n### Visibility Costmaps\n\nVisibility costmaps determine every position, around a target position, from which the target is visible. Visibility\nCostmaps are able to work with cameras that are movable in height for example, if the robot has a movable torso.\n\n```python\nimport pycram.costmaps as cm\n\nv = cm.VisibilityCostmap(1.27, 1.60, size=300, resolution=0.02, origin=Pose([0, 0, 0.1], [0, 0, 0, 1]))\n```\n\n```python\nv.visualize()\n```\n\n```python\nv.close_visualization()\n```\n\n### Occupancy Costmap\n\nIs valid for every position where the robot can be placed without colliding with an object.\n\n```python\no = cm.OccupancyCostmap(0.2, from_ros=False, size=300, resolution=0.02, origin=Pose([0, 0, 0.1], [0, 0, 0, 1]))\n```\n\n```python\ns = cm.SemanticCostmap(kitchen, \"kitchen_island_surface\", size=100, resolution=0.02)\n\ng = cm.GaussianCostmap(200, 15, resolution=0.02)\n```\n\nYou can visualize the costmap in the BulletWorld to get an impression what information is actually contained in the\ncostmap. With this you could also check if the costmap was created correctly.\nVisualization can be done via the 'visualize' method of each costmap.\n\n```python\no.visualize()\n```\n\n```python\no.close_visualization()\n```\n\nIt is also possible to combine two costmap, this will result in a new costmap with the same size which contains the\ninformation of both previous costmaps. Combination is done by checking for each position in the two costmaps if they are\nzero, in this case to same position in the new costmap will also be zero in any other case the new position will be the\nnormalized product of the two combined costmaps.\n\n```python\nov = o + v\n```\n\n```python\nov.visualize()\n```\n\n```python\nov.close_visualization()\n```\n\n## Bullet World Reasoning\n\nAllows for geometric reasoning in the BulletWorld. At the moment the following types of reasoning are supported:\n\n* {meth}`~pycram.world_reasoning.stable`\n* {meth}`~pycram.world_reasoning.contact`\n* {meth}`~pycram.world_reasoning.visible`\n* {meth}`~pycram.world_reasoning.occluding`\n* {meth}`~pycram.world_reasoning.reachable`\n* {meth}`~pycram.world_reasoning.blocking`\n* {meth}`~pycram.world_reasoning.supporting`\n\nTo show the geometric reasoning we first spawn a robot as well as the milk Object again.\n\n```python\nimport pycram.world_reasoning as btr\n\nmilk = Object(\"Milk\", ObjectType.MILK, \"milk.stl\", pose=Pose([1, 0, 1]))\npr2 = Object(\"pr2\", ObjectType.ROBOT, \"pr2.urdf\")\n```\n\nWe start with testing for visibility\n\n```python\nmilk.set_position(Pose([1, 0, 1]))\nvisible = btr.visible(milk, pr2.get_link_pose(\"wide_stereo_optical_frame\"))\nprint(f\"Milk visible: {visible}\")\n```\n\n```python\nmilk.set_position(Pose([1, 0, 0.05]))\n\nplane = BulletWorld.current_bullet_world.objects[0]\ncontact = btr.contact(milk, plane)\nprint(f\"Milk is in contact with the floor: {contact}\")\n```\n\n```python\nmilk.set_position(Pose([0.6, -0.5, 0.7]))\n\nreachable = btr.reachable(milk, pr2, \"r_gripper_tool_frame\")\nprint(f\"Milk is reachable for the PR2: {reachable}\")\n```\n\n# Designators\n\nDesignators are symbolic descriptions of Actions, Motions, Objects or Locations. In PyCRAM the different types of\ndesignators are represented by a class which takes a description, the description then tells the designator what to do.\n\nFor example, let's look at a Motion Designator to move the robot to a specific location.\n\n## Motion Designators\n\nWhen using a Motion Designator you need to specify which Process Module needs to be used, either the Process Module for\nthe real or the simulated robot. A Process Module is the interface between a real or simulated robot, and PyCRAM\ndesignators. By exchanging the Process Module, one can quickly change the robot the plan is executed on, allowing PyCRAM\nplans to be re-used across multiple robot platforms. This can be done either with a decorator which can be added to a\nfunction and then every designator executed within this function, will be executed on the specified robot. The other\npossibility is a \"with\" scope which wraps a code piece.\n\nThese two ways can also be combined, you could write a function which should be executed on the real robot and the\nfunction contains a \"with\" scope which executes something on the simulated robot for reasoning purposes.\n\n```python\nfrom pycram.designators.motion_designator import *\nfrom pycram.process_module import simulated_robot, with_simulated_robot\n\ndescription = MoveMotion(target=Pose([1, 0, 0], [0, 0, 0, 1]))\n\nwith simulated_robot:\n    description.perform()\n\n\n```\n\n```python\nfrom pycram.process_module import with_simulated_robot\n\n\n@with_simulated_robot\ndef move():\n    MoveMotion(target=Pose([0, 0, 0], [0, 0, 0, 1])).perform()\n\n\nmove()\n```\n\nOther implemented Motion Designator descriptions are:\n\n* Accessing\n* Move TCP\n* Looking\n* Move Gripper\n* Detecting\n* Move Arm Joint\n* World State Detecting\n\n## Object Designators\n\nAn Object Designator represents objects. These objects could either be from the BulletWorld or the real world. Object\nDesignators are used, for example, by the PickUpAction to know which object should be picked up.\n\n```python\nfrom pycram.designators.object_designator import *\n\nmilk_desig = BelieveObject(names=[\"Milk\"])\nmilk_desig.resolve()\n```\n\n## Location Designator\n\nLocation Designator can create a position in cartisian space from a symbolic desctiption\n\n```python\nfrom pycram.designators.object_designator import *\n\nmilk_desig = BelieveObject(names=[\"Milk\"])\nmilk_desig.resolve()\n```\n\n## Location Designators\n\nLocation Designators can create a position in cartesian space from a symbolic description.\n\n```python\nfrom pycram.designators.location_designator import *\nfrom pycram.designators.object_designator import *\n\nrobot_desig = BelieveObject(types=[ObjectType.ROBOT]).resolve()\nmilk_desig = BelieveObject(names=[\"Milk\"]).resolve()\nlocation_desig = CostmapLocation(target=milk_desig, visible_for=robot_desig)\n\nprint(f\"Resolved: {location_desig.resolve()}\")\nprint()\n\nfor pose in location_desig:\n    print(pose)\n\n```\n\n# Action Designator\n\nAction Designators are used to describe high-level actions. Action Designators are usually composed of other Designators\nto describe the high-level action in detail.\n\n```python\nfrom pycram.designators.action_designator import *\nfrom pycram.datastructures.enums import Arms\n\nwith simulated_robot:\n    ParkArmsAction([Arms.BOTH]).resolve().perform()\n\n```\n\n# Making a simple plan\n\nTo get familiar with the PyCRAM Framework we will write a simple pick and place plan. This plan will let the robot grasp\na cereal box from the kitchen counter and place it on the kitchen island. This is a simple pick and place plan.\n\n```python\nfrom pycram.designators.object_designator import *\n\ncereal = Object(\"cereal\", ObjectType.BREAKFAST_CEREAL, \"breakfast_cereal.stl\", pose=Pose([1.4, 1, 0.95]))\n\n```\n\n```python\ncereal_desig = ObjectDesignatorDescription(names=[\"cereal\"])\nkitchen_desig = ObjectDesignatorDescription(names=[\"kitchen\"])\nrobot_desig = ObjectDesignatorDescription(names=[\"pr2\"]).resolve()\nwith simulated_robot:\n    ParkArmsAction([Arms.BOTH]).resolve().perform()\n\n    MoveTorsoAction([0.3]).resolve().perform()\n\n    pickup_pose = CostmapLocation(target=cereal_desig.resolve(), reachable_for=robot_desig).resolve()\n    pickup_arm = pickup_pose.reachable_arms[0]\n\n    NavigateAction(target_locations=[pickup_pose.pose]).resolve().perform()\n\n    PickUpAction(object_designator_description=cereal_desig, arms=[pickup_arm], grasps=[\"front\"]).resolve().perform()\n\n    ParkArmsAction([Arms.BOTH]).resolve().perform()\n\n    place_island = SemanticCostmapLocation(\"kitchen_island_surface\", kitchen_desig.resolve(),\n                                           cereal_desig.resolve()).resolve()\n\n    place_stand = CostmapLocation(place_island.pose, reachable_for=robot_desig, reachable_arm=pickup_arm).resolve()\n\n    NavigateAction(target_locations=[place_stand.pose]).resolve().perform()\n\n    PlaceAction(cereal_desig, target_locations=[place_island.pose], arms=[pickup_arm]).resolve().perform()\n\n    ParkArmsAction([Arms.BOTH]).resolve().perform()\n\n\n\n```\n\n# Task Trees\n\nTask trees are a hierarchical representation of all Actions involved in a plan. The Task tree can later be used to\ninspect and restructure the execution order of Actions in the plan.\n\n```python\nimport pycram.task\nimport anytree\n\ntt = pycram.task.task_tree\nprint(anytree.RenderTree(tt))\n```\n\n```python\nfrom anytree.dotexport import RenderTreeGraph, DotExporter\n\nRenderTreeGraph(tt).to_picture(\"tree.png\")\n```\n\n# ORM\n\n```python\nimport sqlalchemy.orm\nimport pycram.orm.base\nimport pycram.orm.action_designator\n\n# set description of what we are doing\npycram.orm.base.ProcessMetaData().description = \"Tutorial for getting familiar with the ORM.\"\n\nengine = sqlalchemy.create_engine(\"sqlite+pysqlite:///:memory:\", echo=False)\nsession = sqlalchemy.orm.Session(bind=engine)\npycram.orm.base.Base.metadata.create_all(engine)\nsession.commit()\n\ntt.insert(session)\n```\n\n```python\nfrom sqlalchemy import select\n\nnavigations = session.scalars(select(pycram.orm.action_designator.NavigateAction)).all()\nprint(*navigations, sep=\"\\n\")\n```\n\n```python\nnavigations = (session.scalars(\n    select(pycram.orm.action_designator.NavigateAction, pycram.orm.base.Position, pycram.orm.base.Quaternion).\n    join(pycram.orm.action_designator.NavigateAction.pose).\n    join(pycram.orm.base.Pose.position).\n    join(pycram.orm.base.Pose.orientation)).all())\nprint(*navigations, sep=\"\\n\")\n```\n\nThe world can also be closed with the 'exit' method\n\n```python\nworld.exit()\n```"}]}
{"messages": [{"role": "system", "content": "You are a coding expert and know how to write PyCRAM Plan Code."}, {"role": "user", "content": "What are the key features of the Bullet World in the PyCRAM documentation?"}, {"role": "assistant", "content": "Bullet World\n\nThis Notebook will show you the basics of working with the PyCRAM BulletWorld.\n\nFirst we need to import and create a BulletWorld.\n\n```python\nfrom pycram.worlds.bullet_world import BulletWorld\nfrom pycram.datastructures.pose import Pose\nfrom pycram.datastructures.enums import ObjectType, WorldMode\n\nworld = BulletWorld(mode=WorldMode.GUI)\n```\n\nThis new window is the BulletWorld, PyCRAMs internal physics simulation. You can use the mouse to move the camera\naround:\n\n* Press the left mouse button to rotate the camera\n* Press the right mouse button to move the camera\n* Press the middle mouse button (scroll wheel) and move the mouse up or down to zoom\n\nAt the moment the BulletWorld only contains a floor, this is spawned by default when creating the BulletWorld.\nFurthermore, the gravity is set to 9.8 $m^2$, which is the same gravitation as the one on earth.\n\nTo spawn new things in the BulletWorld we need to import the Object class and create and instance of it.\n\n```python\nfrom pycram.world_concepts.world_object import Object\n\nmilk = Object(\"milk\", ObjectType.MILK, \"milk.stl\", pose=Pose([0, 0, 1]))\n```\n\n<!-- #region -->\nAs you can see this spawns a milk floating in the air. What we did here was create a new Object which has the name \"\nmilk\" as well as the type {attr}`~pycram.datastructures.enums.ObjectType.MILK`, is spawned from the file \"milk.stl\" and is at the position [0, 0, 1].\n\nThe type of an Object can either be from the enum ObjectType or a string. However, it is recommended to use the enum\nsince this would make for a more consistent naming of types which makes it easier to work with types. But since the\ntypes of the enum might not fit your case you can also use strings.\n\nThe first three of these parameters are required while the position is optional. As you can see it was sufficient to\nonly specify the filename for PyCRAM to spawn the milk mesh. When only providing a filename, PyCRAM will search in its\nresource directory for a matching file and use it.\n\nFor a complete list of all parameters that can be used to crate an Object please check the documentation.\n\nSince the Object is spawned, we can now interact with it. First we want to move it around and change its orientation\n<!-- #endregion -->\n\n```python\nmilk.set_position(Pose([1, 1, 1]))\n```\n\n```python\nmilk.set_orientation(Pose(orientation=[1, 0, 0, 1]))\n```\n\n```python\nmilk.set_pose(Pose([0, 0, 1], [0, 0, 0, 1]))\n```\n\nIn the same sense as setting the position or orientation, you can also get the position and orientation.\n\n```python\nprint(f\"Position: \\n{milk.get_position()}\")\n\nprint(f\"Orientation: \\n{milk.get_orientation()}\")\n\nprint(f\"Pose: \\n{milk.get_pose()}\")\n```\n\n## Attachments\n\nYou can attach Objects to each other simply by calling the attach method on one of them and providing the other as\nparameter. Since attachments are bi-directional it doesn't matter on which Object you call the method.\n\nFirst we need another Object\n\n```python\ncereal = Object(\"cereal\", ObjectType.BREAKFAST_CEREAL, \"breakfast_cereal.stl\", pose=Pose([1, 0, 1]))\n```\n\n```python\nmilk.attach(cereal)\n```\n\nNow since they are attached to each other, if we move one of them the other will move in conjunction.\n\n```python\nmilk.set_position(Pose([1, 1, 1]))\n```\n\nIn the same way the Object can also be detached, just call the detach method on one of the two attached Objects.\n\n```python\ncereal.detach(milk)\n```\n\n## Links and Joints\n\nObjects spawned from mesh files do not have links or joints, but if you spawn things from a URDF like a robot they will\nhave a lot of links and joints. Every Object has two dictionaries as attributes, namely ```links``` and ```joints```\nwhich contain every link or joint as key and a unique id, used by PyBullet, as value.\n\nWe will see this at the example of the PR2:\n\n```python\npr2 = Object(\"pr2\", ObjectType.ROBOT, \"pr2.urdf\")\nprint(pr2.links)\n```\n\nFor links there are similar methods available as for the pose. However, you can only **get** the position and\norientation of a link.\n\n```python\nprint(f\"Position: \\n{pr2.get_link_position('torso_lift_link')}\")\n\nprint(f\"Orientation: \\n{pr2.get_link_orientation('torso_lift_link')}\")\n\nprint(f\"Pose: \\n{pr2.get_link_pose('torso_lift_link')}\")\n```\n\nMethods available for joints are:\n\n* {meth}`~pycram.world_concepts.world_object.Object.get_joint_position`\n* {meth}`~pycram.world_concepts.world_object.Object.set_joint_position`\n* {meth}`~pycram.world_concepts.world_object.Object.get_joint_limits`\n\nWe will see how these methods work at the example of the torso_lift_joint:\n\n```python\nprint(f\"Joint limits: {pr2.get_joint_limits('torso_lift_joint')}\")\n\nprint(f\"Current Joint state: {pr2.get_joint_position('torso_lift_joint')}\")\n\npr2.set_joint_position(\"torso_lift_joint\", 0.2)\n\nprint(f\"New Joint state: {pr2.get_joint_position('torso_lift_joint')}\")\n```\n\n## Misc Methods\n\nThere are a few methods that don't fit any category but could be helpful anyway. The first two are {meth}`~pycram.description.Link.get_color`\nand {meth}`~pycram.description.Link.set_color`, as the name implies they can be used to get or set the color for specific links or the whole\nObject.\n\n```python\nprint(f\"Pr2 forearm color: {pr2.get_link_color('r_forearm_link')}\")\n```\n\n```python\npr2.set_link_color(\"r_forearm_link\", [1, 0, 0])\n```\n\nLastly, there is {meth}`~pycram.description.Link.get_axis_aligned_bounding_box`, AABB stands for *A*xis *A*ligned *B*ounding *B*ox. This method returns two points in\nworld coordinates which span a rectangle representing the AABB.\n\n```python\npr2.get_axis_aligned_bounding_box()\n```\n\nTo close the BulletWorld again please use the {meth}`~pycram.datastructures.world.World.exit` method since it will also terminate threads running in the\nbackground\n\n```python\nworld.exit()\n```"}]}
{"messages": [{"role": "system", "content": "You are a coding expert and know how to write PyCRAM Plan Code."}, {"role": "user", "content": "What are the key features of the Plan Language in the PyCRAM documentation?"}, {"role": "assistant", "content": "Plan Language\n\nThe PyCRAM plan language is a way to structure the execution of your plan. In generally the plan language allows to\nexecute designators either sequential or in parallel. Furthermore, exceptions that occur during execution of a plan with\nthe plan language do not interrupt the execution instead they are caught and saved to a dictionary for later analysis.\nAll language expressions return a State, this can either be SUCCEDED or FAILED.\n\nThere are 4 language expressions:\n\n| Expression | Name             | Description                                                                                                                                                                                                                                                                                | \n|------------|------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| +          | **Sequential**   | Executes the designators one after another, if one of the designators raises an exception the execution is aborted and the state FAILED will be returned.                                                                                                                                  |\n| -          | **Try In Order** | Executes the designators one after another, if one designator raises an exception the exception is caught and saved but the execution is not interrupted and the other designators are executed. Returns the state SUCCEDED if at least one designator can be executed without exception. |\n| *          | **Repeat**       | Repeat the previous language expression a number of time. Has to be used with a language expression and an integer.                                                                                                                                                                        | \n| \\|         | **Parallel**     | Executes all designators in parallel. For each designator there will be a new thread created and the designator is executed in this thread. If one of the designators raises an exception the returned state will be FAILED.                                                               |\n| ^          | **Try All**      | Executes all designators in parallel with a designated thread for each designator. Returns the state SUCCEDED if at least one designator can be executed without an exception                                                                                                              |\n| >>         | **Monitor**      | Monitors the execution of the attached langauge expression, will interrupt the execution as soon as a given condition is fulfilled.                                                                                                                                                        | \n\nThe Sequential expression is the only one which aborts the execution once an error is raised.\n\nWhen using the plan language a tree structure of the plan is created where the language expressions are nodes and\ndesignators are leafs. This tree uses AnyTree (like the task tree) and can be rendered with the anytree Renderer.\n\n## Sequential\n\nThis language expression allows to execute designators one after another, if one of the designators raises an exception\nthe execution will be aborted and the state FAILED will be returned.\n\nWe will start with a simple example that uses an action designator for moving the robot and parking its arms.\n\n```python\nimport time\n\nfrom pycram.designators.action_designator import *\nfrom pycram.datastructures.pose import Pose\nfrom pycram.datastructures.enums import Arms\n\nnavigate = NavigateAction([Pose([1, 1, 0])])\npark = ParkArmsAction([Arms.BOTH])\n\nplan = navigate + park\n```\n\nWith this simple plan created we can inspect it and render the created tree structure.\n\n```python\nfrom anytree import RenderTree\n\nprint(RenderTree(plan))\n```\n\nAs you can see there is the root node which is the language expression and then there are the leafs which are the\ndesignators. When executing this plan the Sequential node will try to execute the NavigateAction and if that is finished\nwithout any error the ParkArmsAction will be executed.\n\nThe plan can be executed by wrapping it inside a ```with simulated_robot``` environment and calling perform on the\nplan.\n\nIf you are performing a plan with a simulated robot, you need a BulletWorld.\n\n```python\nfrom pycram.worlds.bullet_world import BulletWorld\nfrom pycram.world_concepts.world_object import Object\nfrom pycram.datastructures.enums import ObjectType\n\nworld = BulletWorld()\npr2 = Object(\"pr2\", ObjectType.ROBOT, \"pr2.urdf\")\n```\n\n```python\nfrom pycram.process_module import simulated_robot\n\nworld.reset_bullet_world()\n\nwith simulated_robot:\n    plan.perform()\n```\n\n## Try In Order\n\nTry in order is similar to Sequential, it also executes all designators one after another but the key difference is that\nan exception in one of the designators does not terminate the whole execution. Furthermore, the state FAILED will only\nbe returned if all designator executions raise an error.\n\nBesides the described difference in behaviour this language expression can be used in the same way as Sequential.\n\n```python\nfrom pycram.designators.action_designator import *\nfrom pycram.datastructures.pose import Pose\nfrom pycram.datastructures.enums import Arms\nfrom pycram.process_module import simulated_robot\n\nworld.reset_bullet_world()\n\nnavigate = NavigateAction([Pose([1, 1, 0])])\npark = ParkArmsAction([Arms.BOTH])\n\nplan = navigate - park\n\nwith simulated_robot:\n    plan.perform()\n```\n\n## Parallel\n\nParallel executes all designator at once in dedicated threads. The execution of other designators is not aborted when a\nexception is raised, this is the case since threads can not be killed from the outside and this would also cause\nunforeseen problems. The state returned will be SUCCEDED if all designators could be executed without an exception raised\nin any other case FAILED will be returned.\n\nSince executing designators in parallel can get chaotic especially with complex actions like PickUp or Transport. For\nthis reason not all action designators can be used in parallel and try all expressions. The list of action designator\nthat cannot be used in language expressions can be seen in {attr}`~pycram.language.Language.parallel_blocklist`.\n\nDesignators that cannot be used in parallel and try all:\n\n* PickUpAction\n* PlaceAction\n* OpenAction\n* CloseAction\n* TransportAction\n\nUsing the parallel expressions works like Sequential and TryInOrder.\n\n```python\nfrom pycram.designators.action_designator import *\nfrom pycram.datastructures.pose import Pose\nfrom pycram.datastructures.enums import Arms\nfrom pycram.process_module import simulated_robot\n\nworld.reset_world()\n\nnavigate = NavigateAction([Pose([1, 1, 0])])\npark = ParkArmsAction([Arms.BOTH])\n\nplan = navigate | park\n\nwith simulated_robot:\n    plan.perform()\n```\n\n## Try All\n\nTryAll is to Parallel what TryInOrder is to Sequential, meaning TryAll will also execute all designators in parallel but\nwill return SUCCEEDED if at least one designator is executed without raising an exception.\n\nTryAll can be used like any other language expression.\n\n```python\nfrom pycram.designators.action_designator import *\nfrom pycram.datastructures.pose import Pose\nfrom pycram.datastructures.enums import Arms\nfrom pycram.process_module import simulated_robot\n\nworld.reset_bullet_world()\n\nnavigate = NavigateAction([Pose([1, 1, 0])])\npark = ParkArmsAction([Arms.BOTH])\n\nplan = navigate ^ park\n\nwith simulated_robot:\n    plan.perform()\n```\n\n## Combination of Expressions\n\nYou can also combine different language expressions to further structure your plans. If you combine sequential and\nparallel expression please keep in mind that sequential expressions bind stringer than parallel ones. For example:\n\n```\nnavigate | park + move_torso\n```\n\nIn this case 'park' and 'move_torso' would form a Sequential expression and 'naviagte' would form a Parallel expression\nwith Sequential. You can try this yourself in the following cell.\n\n```python\nfrom pycram.designators.action_designator import *\nfrom pycram.datastructures.pose import Pose\nfrom pycram.datastructures.enums import Arms\nfrom pycram.process_module import simulated_robot\n\nworld.reset_world()\n\nnavigate = NavigateAction([Pose([1, 1, 0])])\npark = ParkArmsAction([Arms.BOTH])\nmove_torso = MoveTorsoAction([0.3])\n\nplan = navigate | park + move_torso\n\nwith simulated_robot:\n    plan.perform()\n```\n\n## Code Objects\n\nYou can not only use designators in the plan language but also python code. For this there is the {class}`~pycram.language.Code`  object\nwhich takes a callable and the arguments for this callable. This allows you to execute arbitrary code in a plan.\n\nThe callable that is used in the {class}`~pycram.language.Code` object can either be a lambda expression or, for more complex code, a\nfunction. If you use a function you can provide parameters as keyword-arguments.\n\n```python\nfrom pycram.designators.action_designator import *\nfrom pycram.datastructures.enums import Arms\nfrom pycram.process_module import simulated_robot\nfrom pycram.language import Code\n\n\ndef code_test(param):\n    print(\"-\" * 20)\n    print(param)\n\n\npark = ParkArmsAction([Arms.BOTH])\ncode = Code(lambda: print(\"This is from the code object\"))\ncode_func = Code(code_test, {\"param\": \"Code function\"})\n\nplan = park | code | code_func\n\nwith simulated_robot:\n    plan.perform()\n```\n\n## Exception Handling\n\nIf an exception is raised during the execution of a designator when it is used in a language expression the exception\nwill be caught and saved to a dictionary. In general all designators in a language expression are executed regardless\nof exceptions raised, the only exception from this is the Sequential expression which stops after it encountered an\nexception.\n\nThe language will only catch exceptions that are of type {class}`~pycram.plan_failures.PlanFailure` meaning errors that are defined in\nplan_failures.py in PyCRAM. This also means normal Python errors, such as KeyError, will interrupt the execution of your\ndesignators.\n\nWe will see how exceptions are handled at a simple example.\n\n```python\nfrom pycram.designators.action_designator import *\nfrom pycram.process_module import simulated_robot\nfrom pycram.language import Code\nfrom pycram.plan_failures import PlanFailure\n\n\ndef code_test():\n    raise PlanFailure\n\n\nnavigate = NavigateAction([Pose([1, 1, 0])])\ncode_func = Code(code_test)\n\nplan = navigate | code_func\n\nwith simulated_robot:\n    plan.perform()\n\nprint(plan.exceptions)\n```\n\n## Repeat\n\nRepeat simply repeats a language expression a number of times. As all other language expressions Repeat captures\nexceptions that occur during execution and saves them to the dictionary in the root of the plan.\n\nSince Repeat uses the \\* operator you should keep in mind that it will be evaluated before any other operator, so use\nparentheses to ensure the correct structure of your plan.\n\nYou can see an example of how to use Repeat below.\n\n```python\nfrom pycram.designators.action_designator import *\nfrom pycram.process_module import simulated_robot\n\nmove_torso_up = MoveTorsoAction([0.3])\nmove_torso_down = MoveTorsoAction([0.])\n\nplan = (move_torso_up + move_torso_down) * 5\n\nwith simulated_robot:\n    plan.perform()\n```\n\n## Monitor\n\nMonitor allows to monitor the execution of a language expression and interrupt it as soon as a given condition is\nfulfilled. The condition can either be a Callable which returns a boolean or a Fluent.\nWhen executed the Monitor will create a separate thread which will check if the condition is satisfied with a frequency\nof 10 Hz. If the condition is satisfied the execution of the language expression will be interrupted.\n\nFor the example on how Monitors work we will use the previous example with the robot moving up and down. We will use a\nMonitor to interrupt the execution after 2 seconds instead of executing the whole plan 5 times.\n\n```python\nfrom pycram.designators.action_designator import *\nfrom pycram.process_module import simulated_robot\nfrom pycram.language import Monitor\nimport time\n\nmove_torso_up = MoveTorsoAction([0.3])\nmove_torso_down = MoveTorsoAction([0.])\n\n\ndef monitor_func():\n    time.sleep(2)\n    return True\n\n\nplan = (move_torso_up + move_torso_down) * 5 >> Monitor(monitor_func)\n\nwith simulated_robot:\n    plan.perform()\n```\n\nIf you are finished with this example you can close the world with the cell below.\n\n```python\nworld.exit()\n```"}]}
{"messages": [{"role": "system", "content": "You are a coding expert and know how to write PyCRAM Plan Code."}, {"role": "user", "content": "What are the key concepts covered in the Action Designator section of the PyCRAM documentation?"}, {"role": "assistant", "content": "Action Designator\n\nThis example will show the different kinds of Action Designators that are available. We will see how to create Action\nDesignators and what they do.\n\nAction Designators are high-level descriptions of actions which the robot should execute.\n\nAction Designators are created from an Action Designator Description, which describes the type of action as well as the\nparameter for this action. Parameter are given as a list of possible parameters.\nFor example, if you want to describe the robot moving to a table you would need a\n{meth}`~pycram.designators.action_designator.NavigateAction` and a list of poses that are near the table. The Action\nDesignator Description will then pick one of the poses and return a performable Action Designator which contains the\npicked pose.\n\n\n<!-- #endregion -->\n\n## Navigate Action\n\nWe will start with a simple example of the {meth}`~pycram.designators.action_designator.NavigateAction`.\n\nFirst, we need a BulletWorld with a robot.\n\n```python\nfrom pycram.worlds.bullet_world import BulletWorld\nfrom pycram.world_concepts.world_object import Object\nfrom pycram.datastructures.enums import ObjectType, WorldMode\n\nworld = BulletWorld(WorldMode.GUI)\npr2 = Object(\"pr2\", ObjectType.ROBOT, \"pr2.urdf\")\n```\n\nTo move the robot we need to create a description and resolve it to an actual Designator. The description of navigation\nonly needs a list of possible poses.\n\n```python\nfrom pycram.designators.action_designator import NavigateAction\nfrom pycram.datastructures.pose import Pose\n\npose = Pose([1, 0, 0], [0, 0, 0, 1])\n\n# This is the Designator Description\nnavigate_description = NavigateAction(target_locations=[pose])\n\n# This is the performable Designator\nnavigate_designator = navigate_description.resolve()\n```\n\nWhat we now did was: create the pose where we want to move the robot, create a description describing a navigation with\na list of possible poses (in this case the list contains only one pose) and create an action designator from the\ndescription. The action designator contains the pose picked from the list of possible poses and can be performed.\n\n```python\nfrom pycram.process_module import simulated_robot\n\nwith simulated_robot:\n    navigate_designator.perform()\n```\n\nEvery designator that is performed needs to be in an environment that specifies where to perform the designator either\non the real robot or the simulated one. This environment is called {meth}`~pycram.process_module.simulated_robot`  similar there is also\na {meth}`~pycram.process_module.real_robot` environment.\n\nThere are also decorators which do the same thing but for whole methods, they are called {meth}`~pycram.process_module.with_real_robot` \nand {meth}`~pycram.process_module.with_simulated_robot`.\n\n## Move Torso\n\nThis action designator moves the torso up or down, specifically it sets the torso joint to a given value.\n\nWe start again by creating a description and resolving it to a designator. Afterwards, the designator is performed in\na {meth}`~pycram.process_module.simulated_robot` environment.\n\n```python\nfrom pycram.designators.action_designator import MoveTorsoAction\nfrom pycram.process_module import simulated_robot\n\ntorso_pose = 0.2\n\ntorso_desig = MoveTorsoAction([torso_pose]).resolve()\n\nwith simulated_robot:\n    torso_desig.perform()\n```\n\n## Set Gripper\n\nAs the name implies, this action designator is used to open or close the gripper.\n\nThe procedure is similar to the last time, but this time we will shorten it a bit.\n\n```python\nfrom pycram.designators.action_designator import SetGripperAction\nfrom pycram.process_module import simulated_robot\nfrom pycram.datastructures.enums import GripperState, Arms\n\ngripper = Arms.RIGHT\nmotion = GripperState.OPEN\n\nwith simulated_robot:\n    SetGripperAction(grippers=[gripper], motions=[motion]).resolve().perform()\n```\n\n## Park Arms\n\nPark arms is used to move one or both arms into the default parking position.\n\n```python\nfrom pycram.designators.action_designator import ParkArmsAction\nfrom pycram.process_module import simulated_robot\nfrom pycram.datastructures.enums import Arms\n\nwith simulated_robot:\n    ParkArmsAction([Arms.BOTH]).resolve().perform()\n```\n\n## Pick Up and Place\n\nSince these two are dependent on each other, meaning you can only place something when you picked it up beforehand, they\nwill be shown together.\n\nThese action designators use object designators, which will not be further explained in this tutorial so please check\nthe example on object designators for more details.\n\nTo start we need an environment in which we can pick up and place things as well as an object to pick up.\n\n```python\nkitchen = Object(\"kitchen\", ObjectType.ENVIRONMENT, \"kitchen.urdf\")\nmilk = Object(\"milk\", ObjectType.MILK, \"milk.stl\", pose=Pose([1.3, 1, 0.9]))\n\nworld.reset_world()\n```\n\n```python\nfrom pycram.designators.action_designator import PickUpAction, PlaceAction, ParkArmsAction, MoveTorsoAction,\n\nNavigateAction\nfrom pycram.designators.object_designator import BelieveObject\nfrom pycram.process_module import simulated_robot\nfrom pycram.datastructures.enums import Arms, Grasp\nfrom pycram.datastructures.pose import Pose\n\nmilk_desig = BelieveObject(names=[\"milk\"])\narm = Arms.RIGHT\n\nwith simulated_robot:\n    ParkArmsAction([Arms.BOTH]).resolve().perform()\n\n    MoveTorsoAction([0.3]).resolve().perform()\n\n    NavigateAction([Pose([0.78, 1, 0.0],\n                         [0.0, 0.0, 0.014701099828940344, 0.9998919329926708])]).resolve().perform()\n\n    PickUpAction(object_designator_description=milk_desig,\n                 arms=[arm],\n                 grasps=[Grasp.RIGHT]).resolve().perform()\n\n    NavigateAction([Pose([-1.90, 0.78, 0.0],\n                         [0.0, 0.0, 0.16439898301071468, 0.9863939245479175])]).resolve().perform()\n\n    PlaceAction(object_designator_description=milk_desig,\n                target_locations=[Pose([-1.20, 1.0192, 0.9624],\n                                       # [0.0, 0.0, 0.6339889056055381, 0.7733421413379024])], \n                                       [0, 0, 0, 1])],\n                arms=[arm]).resolve().perform()\n```\n\n```python\nworld.reset_world()\n```\n\n## Look At\n\nLook at lets the robot look at a specific point, for example if it should look at an object for detecting.\n\n```python\nfrom pycram.designators.action_designator import LookAtAction\nfrom pycram.process_module import simulated_robot\nfrom pycram.datastructures.pose import Pose\n\ntarget_location = Pose([1, 0, 0.5], [0, 0, 0, 1])\nwith simulated_robot:\n    LookAtAction(targets=[target_location]).resolve().perform()\n```\n\n## Detect\n\nDetect is used to detect objects in the field of vision (FOV) of the robot. We will use the milk used in the pick\nup/place example, if you didn't execute that example you can spawn the milk with the following cell. The detect\ndesignator will return a resolved instance of an ObjectDesignatorDescription.\n\n```python\nmilk = Object(\"milk\", ObjectType.MILK, \"milk.stl\", pose=Pose([1.3, 1, 0.9]))\n```\n\n```python\nfrom pycram.designators.action_designator import DetectAction, LookAtAction, ParkArmsAction, NavigateAction\nfrom pycram.designators.object_designator import BelieveObject\nfrom pycram.datastructures.enums import Arms\nfrom pycram.process_module import simulated_robot\nfrom pycram.datastructures.pose import Pose\n\nmilk_desig = BelieveObject(names=[\"milk\"])\n\nwith simulated_robot:\n    ParkArmsAction([Arms.BOTH]).resolve().perform()\n\n    NavigateAction([Pose([0, 1, 0], [0, 0, 0, 1])]).resolve().perform()\n\n    LookAtAction(targets=[milk_desig.resolve().pose]).resolve().perform()\n\n    obj_desig = DetectAction(milk_desig).resolve().perform()\n\n    print(obj_desig)\n```\n\n## Transporting\n\nTransporting can transport an object from its current position to another target position. It is similar to the Pick and\nPlace plan used in the Pick-up and Place example. Since we need an Object which we can transport we spawn a milk, you\ndon't need to do this if you already have spawned it in a previous example.\n\n```python\nkitchen = Object(\"kitchen\", ObjectType.ENVIRONMENT, \"kitchen.urdf\")\nmilk = Object(\"milk\", ObjectType.MILK, \"milk.stl\", pose=Pose([1.3, 1, 0.9]))\n```\n\n```python\nfrom pycram.designators.action_designator import *\nfrom pycram.designators.object_designator import *\nfrom pycram.process_module import simulated_robot\nfrom pycram.datastructures.pose import Pose\nfrom pycram.datastructures.enums import Arms\n\nmilk_desig = BelieveObject(names=[\"milk\"])\n\ndescription = TransportAction(milk_desig,\n                              [Arms.LEFT],\n                              [Pose([-1.35, 0.78, 0.95],\n                                    [0.0, 0.0, 0.16439898301071468, 0.9863939245479175])])\nwith simulated_robot:\n    MoveTorsoAction([0.2]).resolve().perform()\n    description.resolve().perform()\n```\n\n## Opening\n\nOpening allows the robot to open a drawer, the drawer is identified by an ObjectPart designator which describes the\nhandle of the drawer that should be grasped.\n\nFor the moment this designator works only in the apartment environment, therefore we remove the kitchen and spawn the\napartment.\n\n```python\nkitchen.remove()\n```\n\n```python\napartment = Object(\"apartment\", ObjectType.ENVIRONMENT, \"apartment.urdf\")\n```\n\n```python\nfrom pycram.designators.action_designator import *\nfrom pycram.designators.object_designator import *\nfrom pycram.datastructures.enums import Arms\nfrom pycram.process_module import simulated_robot\nfrom pycram.datastructures.pose import Pose\n\napartment_desig = BelieveObject(names=[\"apartment\"]).resolve()\nhandle_deisg = ObjectPart(names=[\"handle_cab10_t\"], part_of=apartment_desig)\n\nwith simulated_robot:\n    MoveTorsoAction([0.25]).resolve().perform()\n    ParkArmsAction([Arms.BOTH]).resolve().perform()\n    NavigateAction([Pose([1.7474915981292725, 2.6873629093170166, 0.0],\n                         [-0.0, 0.0, 0.5253598267689507, -0.850880163370435])]).resolve().perform()\n    OpenAction(handle_deisg, [Arms.RIGHT]).resolve().perform()\n```\n\n## Closing\n\nClosing lets the robot close an open drawer, like opening the drawer is identified by an ObjectPart designator\ndescribing the handle to be grasped.\n\nThis action designator only works in the apartment environment for the moment, therefore we remove the kitchen and spawn\nthe apartment. Additionally, we open the drawer such that we can close it with the action designator.\n\n```python\nkitchen.remove()\n```\n\n```python\napartment = Object(\"apartment\", ObjectType.ENVIRONMENT, \"apartment.urdf\")\napartment.set_joint_state(\"cabinet10_drawer_top_joint\", 0.4)\n```\n\n```python\nfrom pycram.designators.action_designator import *\nfrom pycram.designators.object_designator import *\nfrom pycram.datastructures.enums import Arms\nfrom pycram.process_module import simulated_robot\nfrom pycram.datastructures.pose import Pose\n\napartment_desig = BelieveObject(names=[\"apartment\"]).resolve()\nhandle_deisg = ObjectPart(names=[\"handle_cab10_t\"], part_of=apartment_desig)\n\nwith simulated_robot:\n    MoveTorsoAction([0.25]).resolve().perform()\n    ParkArmsAction([Arms.BOTH]).resolve().perform()\n    NavigateAction([Pose([1.7474915981292725, 2.8073629093170166, 0.0],\n                         [-0.0, 0.0, 0.5253598267689507, -0.850880163370435])]).resolve().perform()\n    CloseAction(handle_deisg, [Arms.RIGHT]).resolve().perform()\n```\n\n```python\nworld.exit()\n```"}]}
{"messages": [{"role": "system", "content": "You are a coding expert and know how to write PyCRAM Plan Code."}, {"role": "user", "content": "What are the key concepts covered in the Object Designator section of the PyCRAM documentation?"}, {"role": "assistant", "content": "Object Designator\n\nObject designators are used to describe objects located in the BulletWorld or the real environment and then resolve them\nduring runtime to concrete objects.\n\nObject designators are different from the Object class in bullet_world.py in the way that they just describe an object\nand do not create objects or provide methods to manipulate them. Nevertheless, object designators contain a reference to\nthe BulletWorld object.\n\nAn Object designator takes two parameters, of which at least one has to be provided. These parameters are:\n\n* A list of names\n* A list of types\n\nObject Designators work similar to Location designators, they get constrains describing a set of objects and when\nresolved return a specific instance.\n\nFor all following examples we need a BulletWorld, so let's create one.\n\n```python\nfrom pycram.worlds.bullet_world import BulletWorld\nfrom pycram.world_concepts.world_object import Object\nfrom pycram.datastructures.enums import ObjectType, WorldMode\nfrom pycram.datastructures.pose import Pose\n\nworld = BulletWorld(WorldMode.GUI)\n```\n\n## Believe Object\n\nThis object designator is used to describe objects that are located in the BulletWorld. So objects that are in the\nbelief state, hence the name. In the future when there is a perception interface, there will be a ```RealObject```\ndescription which will be used to describe objects in the real world.\n\nSince {meth}`~pycram.designators.object_designator.BelieveObject` describes Objects in the BulletWorld we create a few.\n\n```python\nkitchen = Object(\"kitchen\", ObjectType.ENVIRONMENT, \"kitchen.urdf\")\nmilk = Object(\"milk\", ObjectType.MILK, \"milk.stl\", pose=Pose([1.3, 1, 0.9]))\ncereal = Object(\"froot_loops\", ObjectType.BREAKFAST_CEREAL, \"breakfast_cereal.stl\", pose=Pose([1.3, 0.9, 0.95]))\nspoon = Object(\"spoon\", ObjectType.SPOON, \"spoon.stl\", pose=Pose([1.3, 1.1, 0.87]))\n```\n\nNow that we have objects we can create an object designator to describe them. For the start we want an object designator\nonly describing the milk. Since all objects have unique names we can create an object designator using a list with only\nthe name of the object.\n\n```python\nfrom pycram.designators.object_designator import BelieveObject\n\nobject_description = BelieveObject(names=[\"milk\"])\n\nprint(object_description.resolve())\n```\n\nYou can also use the type to describe objects, so now we want to have an object designator that describes every food in\nthe world.\n\n```python\nfrom pycram.designators.object_designator import BelieveObject\n\nobject_description = BelieveObject(types=[ObjectType.MILK, ObjectType.BREAKFAST_CEREAL])\n\nprint(object_description.resolve())\n```\n\n## Object Part\n\nPart of object designators can be used to describe something as part of another object. For example, you could describe\na specific drawer as part of the kitchen. This is necessary since the drawer is no single BulletWorld Object but rather\na link of the kitchen which is a BulletWorld Object.\n\nFor this example we need just need the kitchen, if you didn't spawn it in the previous example you can spawn it with the\nfollowing cell.\n\n```python\nkitchen = Object(\"kitchen\", ObjectType.ENVIRONMENT, \"kitchen.urdf\")\n```\n\n```python\nfrom pycram.designators.object_designator import ObjectPart, BelieveObject\n\nkitchen_desig = BelieveObject(names=[\"kitchen\"]).resolve()\n\nobject_description = ObjectPart(names=[\"sink_area_left_upper_drawer_main\"], part_of=kitchen_desig)\n\nprint(object_description.resolve())\n```\n\n## Object Designators as Generators\n\nSimilar to location designators object designators can be used as generators to iterate through every object that they\nare describing. We will see this at the example of an object designator describing every type of food.\n\nFor this we need some objects, so if you didn't already spawn them you can use the next cell for this.\n\n```python\nkitchen = Object(\"kitchen\", ObjectType.ENVIRONMENT, \"kitchen.urdf\")\nmilk = Object(\"milk\", ObjectType.MILK, \"milk.stl\", pose=Pose([1.3, 1, 0.9]))\ncereal = Object(\"froot_loops\", ObjectType.BREAKFAST_CEREAL, \"breakfast_cereal.stl\", pose=Pose([1.3, 0.9, 0.95]))\nspoon = Object(\"spoon\", ObjectType.SPOON, \"spoon.stl\", pose=Pose([1.3, 1.1, 0.87]))\n```\n\n```python\nfrom pycram.designators.object_designator import BelieveObject\n\nobject_description = BelieveObject(types=[ObjectType.MILK, ObjectType.BREAKFAST_CEREAL])\n\nfor obj in object_description:\n    print(obj, \"\\n\")\n```\n\nTo close the world use the following exit function.\n\n```python\nworld.exit()\n```"}]}
{"messages": [{"role": "system", "content": "You are a coding expert and know how to write PyCRAM Plan Code."}, {"role": "user", "content": "What information can I find about Location Designators in the PyCRAM documentation?"}, {"role": "assistant", "content": "Location Designator\n\nThis example will show you what location designators are, how to use them and what they are capable of.\n\nLocation Designators are used to semantically describe locations in the world. You could, for example, create a location\ndesignator that describes every position where a robot can be placed without colliding with the environment. Location\ndesignator can describe locations for:\n\n* Visibility\n* Reachability\n* Occupancy\n* URDF Links (for example a table)\n\nTo find locations that fit the given constrains, location designator create Costmaps. Costmaps are a 2D distribution\nthat have a value greater than 0 for every position that fits the costmap criteria.\n\nLocation designators work similar to other designators, meaning you have to create a location designator description\nwhich describes the location. This description can then be resolved to the actual 6D pose on runtime.\n\n## Occupancy\n\nWe will start with a simple location designator that describes a location where the robot can be placed without\ncolliding with the environment. To do this we need a BulletWorld since the costmaps are mostly created from the current\nstate of the BulletWorld.\n\n```python\nfrom pycram.worlds.bullet_world import BulletWorld\nfrom pycram.world_concepts.world_object import Object\nfrom pycram.datastructures.enums import ObjectType, WorldMode\nfrom pycram.datastructures.pose import Pose\n\nworld = BulletWorld()\nkitchen = Object(\"kitchen\", ObjectType.ENVIRONMENT, \"kitchen.urdf\")\n```\n\nNext up we will create the location designator description, the {meth}`~pycram.designators.location_designator.CostmapLocation` that we will be using needs a\ntarget as a parameter. This target describes what the location designator is for, this could either be a pose or object\nthat the robot should be able to see or reach.\n\nIn this case we only want poses where the robot can be placed, this is the default behaviour of the location designator\nwhich we will be extending later.\n\n```python\nfrom pycram.designators.location_designator import CostmapLocation\n\ntarget = kitchen.get_pose()\n\nlocation_description = CostmapLocation(target)\n\npose = location_description.resolve()\n\nprint(pose)\n```\n\n## Reachable\n\nNext we want to locations from where the robot can reach a specific point, like an object the robot should pick up. This\ncan also be done with the {meth}`~pycram.designators.location_designator.CostmapLocation` description, but this time we need to provide an additional argument.\nThe additional argument is the robo which should be able to reach the pose.\n\nSince a robot is needed we will use the PR2 and use a milk as a target point for the robot to reach. The torso of the\nPR2 will be set to 0.2 since otherwise the arms of the robot will be too low to reach on the countertop.\n\n```python\npr2 = Object(\"pr2\", ObjectType.ROBOT, \"pr2.urdf\")\npr2.set_joint_state(\"torso_lift_joint\", 0.2)\nmilk = Object(\"milk\", ObjectType.MILK, \"milk.stl\", pose=Pose([1.3, 1, 0.9]))\n\n```\n\n```python\nfrom pycram.designators.location_designator import CostmapLocation\nfrom pycram.designators.object_designator import BelieveObject\n\ntarget = BelieveObject(names=[\"milk\"]).resolve()\nrobot_desig = BelieveObject(names=[\"pr2\"]).resolve()\n\nlocation_description = CostmapLocation(target=target, reachable_for=robot_desig)\n\nprint(location_description.resolve())\n```\n\nAs you can see we get a pose near the countertop where the robot can be placed without colliding with it. Furthermore,\nwe get a list of arms with which the robot can reach the given object.\n\n## Visibile\n\nThe {meth}`~pycram.designators.location_designator.CostmapLocation` can also find position from which the robot can see a given object or location. This is very\nsimilar to how reachable locations are described, meaning we provide a object designator or a pose and a robot\ndesignator but this time we use the ```visible_for``` parameter.\n\nFor this example we need the milk as well as the PR2, so if you did not spawn them during the previous location\ndesignator you can spawn them with the following cell.\n\n```python\npr2 = Object(\"pr2\", ObjectType.ROBOT, \"pr2.urdf\")\nmilk = Object(\"milk\", ObjectType.MILK, \"milk.stl\", pose=Pose([1.3, 1, 0.9]))\n```\n\n```python\nfrom pycram.designators.location_designator import CostmapLocation\nfrom pycram.designators.object_designator import BelieveObject\n\ntarget = BelieveObject(names=[\"milk\"]).resolve()\nrobot_desig = BelieveObject(names=[\"pr2\"]).resolve()\n\nlocation_description = CostmapLocation(target=target, visible_for=robot_desig)\n\nprint(location_description.resolve())\n```\n\n## Semantic\n\nSemantic location designator are used to create location descriptions for semantic entities, like a table. An example of\nthis is: You have a robot that picked up an object and should place it on a table. Semantic location designator then\nallows to find poses that are on this table.\n\nSemantic location designator need an object from which the target entity is a part and the URDF link representing the\nentity. In this case we want a position on the kitchen island, so we have to provide the kitchen object designator since\nthe island is a part of the kitchen and the link name of the island surface.\n\nFor this example we need the kitchen as well as the milk. If you spawned them in one of the previous examples you don't\nneed to execute the following cell.\n\n```python\nkitchen = Object(\"kitchen\", ObjectType.ENVIRONMENT, \"kitchen.urdf\")\nmilk = Object(\"milk\", ObjectType.MILK, \"milk.stl\")\n```\n\n```python\nfrom pycram.designators.location_designator import SemanticCostmapLocation\nfrom pycram.designators.object_designator import BelieveObject\n\nkitchen_desig = BelieveObject(names=[\"kitchen\"]).resolve()\nmilk_desig = BelieveObject(names=[\"milk\"]).resolve()\n\nlocation_description = SemanticCostmapLocation(urdf_link_name=\"kitchen_island_surface\",\n                                               part_of=kitchen_desig,\n                                               for_object=milk_desig)\n\nprint(location_description.resolve())\n```\n\n## Location Designator as Generator\n\nLocation designator descriptions implement an iter method, so they can be used as generators which generate valid poses\nfor the location described in the description. This can be useful if the first pose does not work for some reason.\n\nWe will see this at the example of a location designator for visibility. For this example we need the milk, if you\nalready have a milk spawned in you world you can ignore the following cell.\n\n```python\nmilk = Object(\"milk\", ObjectType.MILK, \"milk.stl\")\n```\n\n```python\nfrom pycram.designators.location_designator import CostmapLocation\nfrom pycram.designators.object_designator import BelieveObject\n\ntarget = BelieveObject(names=[\"milk\"]).resolve()\nrobot_desig = BelieveObject(names=[\"pr2\"]).resolve()\n\nlocation_description = CostmapLocation(target=target, visible_for=robot_desig)\n\nfor pose in location_description:\n    print(pose.pose)\n```\n\n## Accessing Locations\n\nAccessing describes a location from which the robot can open a drawer. The drawer is specified by a ObjetcPart\ndesignator which describes the handle of the drawer.\n\nAt the moment this location designator only works in the apartment environment, so please remove the kitchen if you\nspawned it in a previous example. Furthermore, we need a robot, so we also spawn the PR2 if it isn't spawned already.\n\n```python\nkitchen.remove()\n```\n\n```python\napartment = Object(\"apartment\", ObjectType.ENVIRONMENT, \"apartment.urdf\")\n```\n\n```python\npr2 = Object(\"pr2\", ObjectType.ROBOT, \"pr2.urdf\")\npr2.set_joint_state(\"torso_lift_joint\", 0.25)\n```\n\n```python\nfrom pycram.designators.object_designator import *\nfrom pycram.designators.location_designator import *\n\napartment_desig = BelieveObject(names=[\"apartment\"])\nhandle_desig = ObjectPart(names=[\"handle_cab10_t\"], part_of=apartment_desig.resolve())\nrobot_desig = BelieveObject(names=[\"pr2\"])\n\naccess_location = AccessingLocation(handle_desig.resolve(), robot_desig.resolve()).resolve()\nprint(access_location.pose)\n```\n\n## Giskard Location\n\nSome robots like the HSR or the Stretch2 need a full-body ik solver to utilize the whole body. For this case robots\nthe {meth}`~pycram.designators.specialized_designators.location.giskard_location.GiskardLocation` can be used. This location designator uses giskard as an ik solver to find a pose for the\nrobot to reach a target pose.\n\n**Note:** The GiskardLocation relies on Giskard, therefore Giskard needs to run in order for this Location Designator to\nwork.\n\n```python\nfrom pycram.designators.specialized_designators.location.giskard_location import GiskardLocation\n\nrobot_desig = BelieveObject(names=[\"pr2\"]).resolve()\n\nloc = GiskardLocation(target=Pose([1, 1, 1]), reachable_for=robot_desig).resolve()\nprint(loc.pose)\n```\n\nIf you are finished with this example you can close the world with the following cell:\n\n```python\nworld.exit()\n```"}]}
{"messages": [{"role": "system", "content": "You are a coding expert and know how to write PyCRAM Plan Code."}, {"role": "user", "content": "What are the key features of the Motion Designator in the PyCRAM documentation?"}, {"role": "assistant", "content": "Motion Designator\n\nMotion designators are similar to action designators, but unlike action designators, motion designators represent atomic\nlow-level motions. Motion designators only take the parameter that they should execute and not a list of possible\nparameters, like the other designators. Like action designators, motion designators can be performed, performing motion\ndesignator verifies the parameter and passes the designator to the respective process module.\n\nSince motion designators perform a motion on the robot, we need a robot which we can use. Therefore, we will create a\nBulletWorld as well as a PR2 robot.\n\n```python\nfrom pycram.worlds.bullet_world import BulletWorld\nfrom pycram.world_concepts.world_object import Object\nfrom pycram.datastructures.enums import ObjectType, WorldMode\n\nworld = BulletWorld(WorldMode.GUI)\npr2 = Object(\"pr2\", ObjectType.ROBOT, \"pr2.urdf\")\n```\n\n## Move\n\nMove is used to let the robot drive to the given target pose. Motion designator are used in the same way as the other\ndesignator, first create a description then resolve it to the actual designator and lastly, perform the resolved\ndesignator.\n\n```python\nfrom pycram.datastructures.pose import Pose\nfrom pycram.designators.motion_designator import MoveMotion\nfrom pycram.process_module import simulated_robot\n\nwith simulated_robot:\n    motion_description = MoveMotion(target=Pose([1, 0, 0], [0, 0, 0, 1]))\n\n    motion_description.perform()\n```\n\n```python\nworld.reset_world()\n```\n\n## MoveTCP\n\nMoveTCP is used to move the tool center point (TCP) of the given arm to the target position specified by the parameter.\nLike any designator we start by creating a description and then resolving and performing it.\n\n```python\nfrom pycram.designators.motion_designator import MoveTCPMotion\nfrom pycram.process_module import simulated_robot\nfrom pycram.datastructures.enums import Arms\n\nwith simulated_robot:\n    motion_description = MoveTCPMotion(target=Pose([0.5, 0.6, 0.6], [0, 0, 0, 1]), arm=Arms.LEFT)\n\n    motion_description.perform()\n```\n\n## Looking\n\nLooking motion designator adjusts the robot state such that the cameras point towards the target pose. Although this\nmotion designator takes the target as position and orientation, in reality only the position is used.\n\n```python\nfrom pycram.designators.motion_designator import LookingMotion\nfrom pycram.process_module import simulated_robot\n\nwith simulated_robot:\n    motion_description = LookingMotion(target=Pose([1, 1, 1], [0, 0, 0, 1]))\n\n    motion_description.perform()\n```\n\n## Move Gripper\n\nMove gripper moves the gripper of an arm to one of two states. The states can be {attr}`~pycram.datastructures.enums.GripperState.OPEN`  and {attr}`~pycram.datastructures.enums.GripperState.CLOSE`, which open\nand close the gripper respectively.\n\n```python\nfrom pycram.designators.motion_designator import MoveGripperMotion\nfrom pycram.process_module import simulated_robot\nfrom pycram.datastructures.enums import Arms, GripperState\n\nwith simulated_robot:\n    motion_description = MoveGripperMotion(motion=GripperState.OPEN, gripper=Arms.LEFT)\n\n    motion_description.perform()\n```\n\n## Detecting\n\nThis is the motion designator implementation of detecting, if an object with the given object type is in the field of\nview (FOV) this motion designator will return an object designator describing the object.\n\nSince we need an object that we can detect, we will spawn a milk for this.\n\n```python\nmilk = Object(\"milk\", ObjectType.MILK, \"milk.stl\", pose=Pose([1.5, 0, 1]))\n```\n\n```python\nfrom pycram.designators.motion_designator import DetectingMotion, LookingMotion\nfrom pycram.process_module import simulated_robot\n\nwith simulated_robot:\n    LookingMotion(target=Pose([1.5, 0, 1], [0, 0, 0, 1])).perform()\n\n    motion_description = DetectingMotion(object_type=ObjectType.MILK)\n\n    obj = motion_description.perform()\n\n    print(obj)\n```\n\n## Move Arm Joints\n\nThis motion designator moves one or both arms. Movement targets are a dictionary with joint name as key and target pose\nas value.\n\n```python\nfrom pycram.designators.motion_designator import MoveArmJointsMotion\nfrom pycram.process_module import simulated_robot\n\nwith simulated_robot:\n    motion_description = MoveArmJointsMotion(right_arm_poses={\"r_shoulder_pan_joint\": -0.7})\n\n    motion_description.perform()\n```\n\n## World State Detecting\n\nWorld state detecting is also used to detect objects, however, the object is not required to be in the FOV of the robot.\nAs long as the object is somewhere in the belief state (BulletWorld) a resolved object designator will be returned.\n\nSine we want to detect something we will spawn an object that we can detect. If you already spawned the milk from the\nprevious example, you can skip this step.\n\n```python\nmilk = Object(\"milk\", ObjectType.MILK, \"milk.stl\", pose=Pose([-1, 0, 1]))\n```\n\n```python\nfrom pycram.designators.motion_designator import WorldStateDetectingMotion\nfrom pycram.process_module import simulated_robot\n\nwith simulated_robot:\n    motion_description = WorldStateDetectingMotion(object_type=ObjectType.MILK)\n\n    obj = motion_description.perform()\n\n    print(obj)\n```\n\n## Move Joints\n\nMove joints can move any number of joints of the robot, the designator takes two lists as parameter. The first list are\nthe names of all joints that should be moved and the second list are the positions to which the joints should be moved.\n\n```python\nfrom pycram.designators.motion_designator import MoveJointsMotion\nfrom pycram.process_module import simulated_robot\n\nwith simulated_robot:\n    motion_description = MoveJointsMotion(names=[\"torso_lift_joint\", \"r_shoulder_pan_joint\"], positions=[0.2, -1.2])\n\n    motion_description.perform()\n```\n\nThe following cell can be used after testing the examples, to close the BulletWorld.\n\n```python\nworld.exit()\n```"}]}
{"messages": [{"role": "system", "content": "You are a coding expert and know how to write PyCRAM Plan Code."}, {"role": "user", "content": "What are the key features of Object Relational Mapping in PyCRAM?"}, {"role": "assistant", "content": "Hands on Object Relational Mapping in PyCram\n\nThis tutorial will walk you through the serialization of a minimal plan in pycram.\nFirst we will import sqlalchemy, create an in memory database and connect a session to it.\n\n```python\nimport sqlalchemy\nimport sqlalchemy.orm\n\nengine = sqlalchemy.create_engine(\"sqlite+pysqlite:///:memory:\", echo=False)\nsession = sqlalchemy.orm.Session(bind=engine)\nsession\n```\n\nNext we create the database schema using the sqlalchemy functionality. For that we need to import the base class of pycram.orm.\n\n```python\nimport pycram.orm.base\nimport pycram.orm.action_designator\npycram.orm.base.Base.metadata.create_all(engine)\nsession.commit()\n```\n\nNext we will write a simple plan where the robot parks his arms and then moves somewhere. We will construct a TaskTree around it such that we can serialize it later. As usual, we first create a world and then define the plan. By doing so, we obtain the task tree.\n\n```python\nfrom pycram.designators.action_designator import *\nfrom pycram.designators.location_designator import *\nfrom pycram.process_module import simulated_robot\nfrom pycram.datastructures.enums import Arms, ObjectType, Grasp, WorldMode\nfrom pycram.tasktree import with_tree\nimport pycram.tasktree\nfrom pycram.worlds.bullet_world import BulletWorld\nfrom pycram.world_concepts.world_object import Object\nfrom pycram.designators.object_designator import *\nfrom pycram.datastructures.pose import Pose\nimport anytree\n\nworld = BulletWorld(WorldMode.GUI)\npr2 = Object(\"pr2\", ObjectType.ROBOT, \"pr2.urdf\")\nkitchen = Object(\"kitchen\", ObjectType.ENVIRONMENT, \"kitchen.urdf\")\nmilk = Object(\"milk\", ObjectType.MILK, \"milk.stl\", pose=Pose([1.3, 1, 0.9]))\ncereal = Object(\"cereal\", ObjectType.BREAKFAST_CEREAL, \"breakfast_cereal.stl\", pose=Pose([1.3, 0.7, 0.95]))\nmilk_desig = ObjectDesignatorDescription(names=[\"milk\"])\ncereal_desig = ObjectDesignatorDescription(names=[\"cereal\"])\nrobot_desig = ObjectDesignatorDescription(names=[\"pr2\"]).resolve()\nkitchen_desig = ObjectDesignatorDescription(names=[\"kitchen\"])\n\n@with_tree\ndef plan():\n    with simulated_robot:\n        ParkArmsActionPerformable(Arms.BOTH).perform()\n        MoveTorsoAction([0.2]).resolve().perform()\n        pickup_pose = CostmapLocation(target=cereal_desig.resolve(), reachable_for=robot_desig).resolve()\n        pickup_arm = pickup_pose.reachable_arms[0]\n        NavigateAction(target_locations=[pickup_pose.pose]).resolve().perform()\n        PickUpAction(object_designator_description=cereal_desig, arms=[pickup_arm], grasps=[Grasp.FRONT]).resolve().perform()\n        ParkArmsAction([Arms.BOTH]).resolve().perform()\n\n        place_island = SemanticCostmapLocation(\"kitchen_island_surface\", kitchen_desig.resolve(),\n                                           cereal_desig.resolve()).resolve()\n\n        place_stand = CostmapLocation(place_island.pose, reachable_for=robot_desig, reachable_arm=pickup_arm).resolve()\n\n        NavigateAction(target_locations=[place_stand.pose]).resolve().perform()\n\n        PlaceAction(cereal_desig, target_locations=[place_island.pose], arms=[pickup_arm]).resolve().perform()\n\n        ParkArmsActionPerformable(Arms.BOTH).perform()\n\nplan()\n\n# set description of what we are doing\npycram.orm.base.ProcessMetaData().description = \"Tutorial for getting familiar with the ORM.\"\ntask_tree = pycram.tasktree.task_tree\nprint(anytree.RenderTree(task_tree))\n```\n\nNext we serialize the task tree by recursively inserting from its root.\n\n```python\ntask_tree.root.insert(session)\n```\n\nWe can look at our experiment (Process)MetaData to get some context on the data we just created.\n\n```python\nfrom sqlalchemy import select\n\nprint(*session.scalars(select(pycram.orm.base.ProcessMetaData)).all())\n```\n\nLastly we can look at various table to see how the structures got logged.\nFor example, we can get all the navigate actions that occurred.\n\n```python\nnavigations = session.scalars(select(pycram.orm.action_designator.NavigateAction)).all()\nprint(*navigations, sep=\"\\n\")\n```\n\nDue to the inheritance mapped in the ORM package, we can also obtain all executed actions with just one query. \n\n```python\nactions = session.scalars(select(pycram.orm.action_designator.Action)).all()\nprint(*actions, sep=\"\\n\")\n```\n\nOf course all relational algebra operators, such as filtering and joining also work in pycram.orm queries. Let's say we need all the poses of objects, that were picked up by a robot. Since we defined a relationship between the PickUpAction table and the Object table and between the Object table and the Pose table in the ORM class schema, we can just use the join operator without any further specification:\n\n```python\nobject_actions = (session.scalars(select(pycram.orm.base.Pose)\n                  .join(pycram.orm.action_designator.PickUpAction.object)\n                  .join(pycram.orm.object_designator.Object.pose))\n                  .all())\nprint(*object_actions, sep=\"\\n\")\n\n```\n\nDid you notice, that for the joins we did not join the tables together in a typical sql kind of way, but rather used the relationships defined in the ORM classes and wrote joins like PickUpAction.object or Object.pose? This is because the ORM package automatically creates the joins for us, so we only have to join on the attributes that hold the relationship. This is a huge advantage over writing sql queries by hand, since we do not have to worry about the join conditions. \nThis is a strong tool, but it is crucial to use it properly. Very important to note: The order of the joins matters! For instance, if we joined the Pose table with the Object table first, and placed the join between the PickUpAction table and the Object table second, sqlalchemy would have selected the Pose not from the join between all three tables, but rather from a join between the Pose and the Object table + from a join between the PickUpAction table and the Object table. These mistakes can lead to wrong results or even to errors (the above-mentioned example would actually lead to an error due to the Object table being accessed twice in two separate joins in the same query and therefore the column names of the Object tables would have been ambiguous and could not be used by sqlalchemy to join).\n\nMake sure to check out the other examples of ORM querying.\n\n\nIf we want to filter for all successful tasks we can just add the filter operator:\n\n```python\nfrom pycram.orm.tasktree import TaskTreeNode\n\nsuccessful_tasks = session.scalars(select(TaskTreeNode).where(TaskTreeNode.status == \"SUCCEEDED\"))\nprint(*successful_tasks, sep=\"\\n\")\n```\n\nAs expected all but the root node succeeded, since the root node is still running.\n\nWriting an extension to the ORM package is also done with ease. We need to create a new ActionDesignator class and its ORM equivalent, where we define our new table. Let's say we want to log all the things the robot says. We will create a new ActionDesignator class called Saying and its ORM equivalent called ORMSaying. \n\n```python\nfrom sqlalchemy.orm import Mapped, mapped_column, Session\nfrom pycram.orm.action_designator import Action\nfrom dataclasses import dataclass\n\n\n# define ORM class from pattern in every pycram.orm class\nclass ORMSaying(Action):\n\n    id: Mapped[int] = mapped_column(sqlalchemy.ForeignKey(f'{Action.__tablename__}.id'), primary_key=True, init=False)\n    # since we do not want to add any custom specifications to our column, we don't even need to define mapped_column, sqlalchemy does this internally.\n    text: Mapped[str] \n\n# define brand new action designator\n\n@dataclass \nclass SayingActionPerformable(ActionDesignatorDescription.Action):\n    \n    text: str\n        \n    @with_tree\n    def perform(self) -> None:\n        print(self.text)\n\n    def to_sql(self) -> ORMSaying:\n        return ORMSaying(self.text)\n\n    def insert(self, session: Session, *args, **kwargs) -> ORMSaying:\n        action = super().insert(session)\n        session.add(action)\n        session.commit()\n        return action\n\n```\n\nNow we got our new ActionDesignator called Saying and its ORM version. Since this class got created after all other classes got inserted into the database (in the beginning of the notebook) we have to insert it manually. \n\n```python\nORMSaying.metadata.create_all(bind=engine)\n```\n\nNow we can create and insert a Saying action. Since this is the last part where we interact with the BulletWorld, we can also close it.\n\n```python\n# create a saying action and insert it\nSayingActionPerformable(\"Patchie, Patchie; Where is my Patchie?\").perform()\npycram.tasktree.task_tree.root.insert(session)\nsession.commit()\n\nworld.exit()\n```\n\nIt is notable that committing the object to the session fills its primary key. Hence, there is no worries about assigning unique IDs manually.\nFinally, we can double-check that our object exists in the database.\n\n```python\nsession.scalars(select(ORMSaying)).all()\n```"}]}
{"messages": [{"role": "system", "content": "You are a coding expert and know how to write PyCRAM Plan Code."}, {"role": "user", "content": "What information can I find about Pose in the PyCRAM documentation tutorial?"}, {"role": "assistant", "content": "Pose\n\nPoses in PyCRAM are represented by the Pose class which inherits from the PoseStamped message of ROS. This makes PyCRAMs\nposes compatible with everything in ROS like services, topics or TF.\n\nThis notebook will provide an overview about poses, how to use them and what they can do. We will start by simply\ncreating a pose.\n\nBefore we start a few words about naming convention of Poses in PyCRAM. Naming convention is similar to the PoseStamped\nmessage so if you are familiar with that this should be easy.\n\n* **Position:** A position means the position in cartesian space, so the x, y, and z coordinates.\n* **Orientation:** An orientation is the rotation in all three axes represented as a quaternion with x, y, z, w.\n* **Pose:** A pose is the combination of a position and an orientation. Poses in PyCRAM also contain a frame of\n  reference to which the position and orientation are relative.\n\n```python\nfrom pycram.datastructures.pose import Pose\n\nexample_pose = Pose([1, 2, 3], [0, 0, 0, 1], \"map\")\nprint(example_pose)\n```\n\nAs you can see we created the ```example_pose``` with a position of ```[1, 2, 3]``` and an orientation\nof ```[0, 0, 0, 1]``` in the frame ```map```. But we don't need to provide all these parameters for a Pose, in case\nthere is no parameter the Pose will use default parameter.\n\n```python\nfrom pycram.datastructures.pose import Pose\n\ndefault_pose = Pose()\nprint(default_pose)\n```\n\nIn case no parameter is provided the defualt parameter are:\n\n* position: ```[0, 0, 0]```\n* orientation: ```[o, 0, 0, 1]```\n* frame: ```map```\n\nThe following example will show how to access the data stored in a pose.\n\n```python\nfrom pycram.datastructures.pose import Pose\n\nexample_pose = Pose([1, 2, 3], [0, 0, 0, 1], \"map\")\n\nprint(f\"Access to a component of the position: {example_pose.position.y}\")\n\nprint(f\"Access to a component of the rotation: {example_pose.orientation.x}\")\n\nprint(f\"Get the whole position as geometry_msgs/Pose:\\n{example_pose.position}\")\n\nprint(f\"You can also get position or orientation as a list: {example_pose.position_as_list()}\")\n\nprint(f\"Same with the whole pose: {example_pose.to_list()}\")\n\nprint(f\"Access the reference frame: {example_pose.frame}\")\n```\n\n## Editing a pose\n\nYou can also edit the data saved in a Pose, similar to how you access it.\n\n```python\nfrom pycram.datastructures.pose import Pose\n\nexample_pose = Pose([1, 2, 3], [0, 0, 0, 1], \"map\")\n\n# Edit a single component of the position \nexample_pose.position.x = 3\nprint(f\"Edit only one component:\\n{example_pose.position}\", \"\\n\")\n\n# Edit the whole position\nexample_pose.position = [0, 0, 1]\nprint(f\"Edit the whole position:\\n{example_pose.position}\", \"\\n\")\n\nexample_pose.frame = \"new_frame\"\nprint(f\"Set a new frame:\\n{example_pose.frame}\", \"\\n\")\n\nexample_pose.set_position([3, 2, 1])\nprint(f\"Set the position via method:\\n{example_pose.position}\", \"\\n\")\n```\n\n## Copy Poses\n\nYou can also copy Poses to create a new Pose with the same data. This can be useful if you have a method which would\nneed to alter the Pose, since poses are passed by reference to a method every change done to the Pose in the method\nwould affect the instanced passed to the method.\n\n```python\nfrom pycram.datastructures.pose import Pose\n\nexample_pose = Pose([1, 2, 3], [0, 0, 0, 1], \"map\")\n\ncopy_pose = example_pose.copy()\n\nprint(example_pose, \"\\n\")\nprint(copy_pose)\n```\n\n## Convert to Transform\n\nPyCRAM also has its own transform at which we will take a look in the next section. However, here we will take a look at\nhow to convert a Pose into a Transform.\n\nFor this example we will take a Pose which represents the current pose of a milk object and convert it into a Transform\nwhich represents the transformation from the ```map``` frame to the ```milk``` frame.\n\n```python\nfrom pycram.datastructures.pose import Pose\n\nmilk_pose = Pose([3, 4, 1], [1, 0, 0, 1], \"map\")\n\nmilk_transform = milk_pose.to_transform(\"milk\")\n\nprint(milk_transform)\n```\n\n# Transforms\n\nTransforms are similar to Poses but instead of representing a Pose in a frame of reference they represent a\ntransformation from one frame of reference to another. For this purpose Transforms have an additional parameter\ncalled ```child_frame_id``` which is the frame of reference to which the Transform is pointing.\n\nTransforms in PyCRAM inherit from the TransformStamped message of ROS which makes them, like Poses, compatible to ROS\nservices and topics that expect a TransformStamped message. Therefore, the naming conventions of Transforms are the same\nas of TransformStamped which.\n\n* **Translation:** The vector describing the transformation in cartesian space.\n* **Rotation:** The quaternion describing the transformation of rotation.\n* **Transform:** The combination of translation and rotation\n\n```python\nfrom pycram.datastructures.pose import Transform\n\nexample_transform = Transform([1, 2, 2], [0, 0, 0, 1], \"map\", \"object\")\n\nprint(example_transform)\n```\n\nTransforms have the same methods to get and set values as Poses have, therefore only a short showcase will be given. For\nmore details please look at the Pose example or the API documentation.\n\n```python\nfrom pycram.datastructures.pose import Transform\n\nexample_transform = Transform([2, 5, 1], [0, 0, 1, 1], \"map\", \"object\")\n\nprint(f\"Access the rotation:\\n{example_transform.rotation}\", \"\\n\")\n\nprint(f\"Access the child_frane: {example_transform.child_frame_id}\", \"\\n\")\n\n# changing translation and rotation is exactly like with Poses.\n\nexample_transform.translation = [1, 1, 1]\nprint(f\"New translation:\\n{example_transform.translation}\")\n```\n\n## Convert to Pose and Copy\n\nAnalog to Poses Transforms have a method that converts a Transform to a Pose, in this process the ```child_frame_id```\nwill be lost.\n\nAlso like in Poses Transforms have a ```copy``` method which creates an exact copy of this Transform.\n\n```python\nfrom pycram.datastructures.pose import Transform\n\nmilk_transform = Transform([1, 1, 1], [0, 0, 0, 1], \"map\", \"milk\")\n\nmilk_pose = milk_transform.to_pose()\n\nprint(f\"The converted pose:\\n{milk_pose}\", \"\\n\")\n\nexample_transform = Transform([1, 1, 1], [0, 0, 0, 1], \"map\", \"milk\")\n\ncopy_transform = example_transform.copy()\n\nprint(f\"The copied transform:\\n{copy_transform}\")\n```\n\n## Operations on Transforms\n\nTransforms have, unlike Poses, operations that can be done. These operations are:\n\n* Multiplication\n* Invert\n* InverseTimes\n\n### Multiplication\n\nWe will first take a look at the multiplication of Transforms. We will use an example were we have two Transforms, the\nfirst from ```map``` to a ```hand``` frame and the second from the ```hand``` to a ```milk``` frame. By multiplying\nthese two we get the Transform from ```map``` to ```milk``` frame.\n\n```python\nfrom pycram.datastructures.pose import Transform\n\nmap_to_hand = Transform([1, 1, 1], [0, 0, 0, 1], \"map\", \"hand\")\n\nhand_to_milk = Transform([0.1, 0.05, 0], [0, 0, 0, 1], \"hand\", \"milk\")\n\nmap_to_milk = map_to_hand * hand_to_milk\n\nprint(map_to_milk)\n```\n\n### Invert\n\nThis inverts a Transform, so in we have a transform from ```map``` to ```milk``` then inverting it results in a\nTransform from ```milk``` to ```map``` .\n\n```python\nfrom pycram.datastructures.pose import Transform\n\nmap_to_milk = Transform([1, 1, 0.5], [0, 0, 0, 1], \"map\", \"milk\")\n\nmilk_to_map = map_to_milk.invert()\n\nprint(milk_to_map)\n```\n\n### Inverse Times\n\nInverse times combines the inverting and multiplication of Transforms, this results in a 'minus' for Transforms. We will\nagain use the example of a hand holding a milk, but this time we have the Transforms from ```map``` to ```milk```\nand ```hand``` to ```milk```.\n\n```python\nfrom pycram.datastructures.pose import Transform\n\nmap_to_milk = Transform([1.1, 1.05, 1], [0, 0, 0, 1], \"map\", \"milk\")\n\nhand_to_milk = Transform([0.1, 0.05, 0], [0, 0, 0, 1], \"hand\", \"milk\")\n\nmap_to_milk = map_to_milk.inverse_times(hand_to_milk)\n\nprint(map_to_milk)\n```"}]}
{"messages": [{"role": "system", "content": "You are a coding expert and know how to write PyCRAM Plan Code."}, {"role": "user", "content": "What information can I find on the Robot Description page of the PyCRAM documentation?"}, {"role": "assistant", "content": "Robot Description\n\n(robot_description_header)=\nThe robot description contains semantic information about the robot which can not be extracted from the URDF in a\ngeneral way. This inludes kinematic chains, end-effectors, cameras and their parameter, etc.\n\nIn genral a Robot Description consists a number of different descriptions, these are:\n\n* RobotDescription\n* KinematicChainDescription\n* EndEffectorDescription\n* CameraDescription\n\nIn this example we will create a robot description step-by-step and describe the different components on the way. The\nrobot we will use as an example will be the PR2, the complete PR2 description can also be seen in\n{meth}`pycram.robot_descriptions.pr2_description`.\n\n## Robot Description Class\n\nWe start by creating an instance of the {class}`~pycram.robot_description.RobotDescription` class, this will serve as a\nthe main component to which all other descriptions will be added.\n\nTo initialize a {class}`~pycram.robot_description.RobotDescription` we need a few parameter which are:\n\n* Name\n* base_link\n* torso_link\n* torso_joint\n* Path to a URDF file\n\n```python\nfrom pycram.robot_description import RobotDescription\nimport rospkg\n\nrospack = rospkg.RosPack()\nfilename = rospack.get_path('pycram') + '/resources/robots/' + \"pr2\" + '.urdf'\n\npr2_description = RobotDescription(\"pr2\", \"base_link\", \"torso_lift_link\", \"torso_lift_joint\", filename)\n```\n\n## Kinematic Chain Description\n\nThe kinematic chain description describes a chain of links and joints of the robot which might be interesting when\nworking with the robot. An example of such a chain would be the arm of the robot, when programming for the robot it is\nimportant to know which links and joints exactly make up the arm, however, these can not be extracted from the URDF\nautomatically.\n\nThe kinematic chain is based upon the URDF, meaning when initializing the description one only needs to specify the\nfirst and last link of the chain.\n\nWe will now create the kinematic chain description for the right arm of the PR2. For initializing\nthe {class}`~pycram.robot_description.KinematicChainDescription` the following parameter are needed:\n\n* Name\n* first link\n* last link\n* URDF object\n* Arm type\n\nThe arm type specifies which arm this kinematic chain describes, this is needed when one wants to access only the arms\nof the robot.\n\n```python\nfrom pycram.robot_description import KinematicChainDescription\nfrom pycram.datastructures.enums import Arms\n\nright_arm = KinematicChainDescription(\"right\", \"torso_lift_link\", \"r_wrist_roll_link\",\n                                      pr2_description.urdf_object, arm_type=Arms.RIGHT)\n```\n\nThe created {class}`~pycram.robot_description.KinematicChainDescription` can now be added to the robot description.\n\n```python\npr2_description.add_kinematic_chain_description(right_arm)\n```\n\n## End Effector Description\n\nSince kinematic chains only describe a moveable chain of links and joints like arms these do not represent end-effectors\nwhich can be used to do manipulation tasks.\n\nTo represent end-effectors we will create an {class}`~pycram.robot_description.EndEffectorDescription` which contains the information of the respective\nend-effector. When creating an {class}`~pycram.robot_description.EndEffectorDescription` we need the following parameter:\n\n* Name\n* first link\n* tool_frame\n* URDF object\n\nYou might have noticed that the end-efftor only has a first link but no last link, this is the case since end-effectors\nare at the end of the arms. Therefore, all links and joints below a certain link can be seen as part of the\nend-effector.\n\n```python\nfrom pycram.robot_description import EndEffectorDescription\n\nright_gripper = EndEffectorDescription(\"right_gripper\", \"r_gripper_palm_link\", \"r_gripper_tool_frame\",\n                                       pr2_description.urdf_object)\n```\n\nThe gripper can no be added to the previously created {class}`~pycram.robot_description.KinematicChainDescription`.\n\n```python\nright_arm.end_effector = right_gripper\n```\n\n## Camera Description\n\nThe camera description contains all parameters of a camera, which is mounted on the robot. The parameter for\nthe {class}`~pycram.robot_description.CameraDescription` are:\n\n* Name\n* Link name\n* minimal height\n* maximal height\n* horizontal angle\n* vertical angle\n\n```python\nfrom pycram.robot_description import CameraDescription\nfrom pycram.datastructures.enums import Grasp\n\ncamera = CameraDescription(\"kinect_camera\", \"wide_stereo_optical_frame\", 1.27,\n                           1.60, 0.99483, 0.75049)\n```\n\nThe finished camera description can now be added to the robot description.\n\n```python\npr2_description.add_camera_description(camera)\n```\n\n## Grasps\n\nGrasps define how a robot interacts with objects. The grasps defined in the robot description define for each grasp (\nright, left, top, front) the orientation of the end-effector, relative to the base_frame of the robot, to achieve the\nrespective grasp.\n\n```python\npr2_description.add_grasp_orientations({Grasp.FRONT: [0, 0, 0, 1],\n                                        Grasp.LEFT: [0, 0, -1, 1],\n                                        Grasp.RIGHT: [0, 0, 1, 1],\n                                        Grasp.TOP: [0, 1, 0, 1]})\n```\n\n## Register Robot Description\n\nLastly, you need to register the robot description to the {class}`~pycram.robot_description.RobotDescriptionManager`. As you can see the code to\nregister the robot description has to be executed at the start of PyCRAM, if you put your file with the robot\ndescription in the {class}`pycram.robot_descriptions` directory it will be executed upon the start of PyCRAM.\n\n```python\nfrom pycram.robot_description import RobotDescriptionManager\n\nrdm = RobotDescriptionManager()\nrdm.register_description(pr2_description)\n```"}]}
{"messages": [{"role": "system", "content": "You are a coding expert and know how to write PyCRAM Plan Code."}, {"role": "user", "content": "What are the key features of the action designator in the PyCRAM documentation?"}, {"role": "assistant", "content": "pycram.designators.action_designator\n### **Classes**\n\n#### **MoveTorsoAction**\n- **Description**: Action Designator for Moving the torso of the robot up and down.\n\n#### **SetGripperAction**\n- **Description**: Set the gripper state of the robot.\n\n#### **ReleaseAction**\n- **Description**: Releases an Object from the robot.\n\n#### **GripAction**\n- **Description**: Grip an object with the robot.\n\n#### **ParkArmsAction**\n- **Description**: Park the arms of the robot.\n\n#### **PickUpAction**\n- **Description**: Designator to let the robot pick up an object.\n\n#### **PlaceAction**\n- **Description**: Places an Object at a position using an arm.\n\n#### **NavigateAction**\n- **Description**: Navigates the Robot to a position.\n\n#### **TransportAction**\n- **Description**: Transports an object to a position using an arm.\n\n#### **LookAtAction**\n- **Description**: Lets the robot look at a position.\n\n#### **DetectAction**\n- **Description**: Detects an object that fits the object description and returns an object designator describing the object.\n\n#### **OpenAction**\n- **Description**: Opens a container like object.\n\n#### **CloseAction**\n- **Description**: Closes a container like object.\n\n#### **GraspingAction**\n- **Description**: Grasps an object described by the given Object Designator description.\n\n#### **ActionAbstract**\n- **Description**: Base class for performable actions.\n\n#### **MoveTorsoActionPerformable**\n- **Description**: Move the torso of the robot up and down.\n\n#### **SetGripperActionPerformable**\n- **Description**: Set the gripper state of the robot.\n\n#### **ReleaseActionPerformable**\n- **Description**: Releases an Object from the robot.\n\n#### **GripActionPerformable**\n- **Description**: Grip an object with the robot.\n\n#### **ParkArmsActionPerformable**\n- **Description**: Park the arms of the robot.\n\n#### **PickUpActionPerformable**\n- **Description**: Let the robot pick up an object.\n\n#### **PlaceActionPerformable**\n- **Description**: Places an Object at a position using an arm.\n\n#### **NavigateActionPerformable**\n- **Description**: Navigates the Robot to a position.\n\n#### **TransportActionPerformable**\n- **Description**: Transports an object to a position using an arm.\n\n#### **LookAtActionPerformable**\n- **Description**: Lets the robot look at a position.\n\n#### **DetectActionPerformable**\n- **Description**: Detects an object that fits the object description and returns an object designator describing the object.\n\n#### **OpenActionPerformable**\n- **Description**: Opens a container like object.\n\n#### **CloseActionPerformable**\n- **Description**: Closes a container like object.\n\n#### **GraspingActionPerformable**\n- **Description**: Grasps an object described by the given Object Designator description.\n\n#### **FaceAtPerformable**\n- **Description**: Turn the robot chassis such that it faces the pose and then perform a look at action.\n\n#### **MoveAndPickUpPerformable**\n- **Description**: Navigate to standing position, turn towards the object, and pick it up.\n\n---\n\n### **Module Contents**\n\n#### **pycram.designators.action_designator.MoveTorsoAction**\n- **Inherits**: `pycram.designator.ActionDesignatorDescription`\n- **Description**: Action Designator for Moving the torso of the robot up and down.\n- **Attributes**:\n  - **positions**: `List[float]`\n- **Methods**:\n  - **ground()**: Returns a `MoveTorsoActionPerformable`.\n  - **__iter__()**: Iterates over possible values and returns a `MoveTorsoActionPerformable`.\n\n#### **pycram.designators.action_designator.SetGripperAction**\n- **Inherits**: `pycram.designator.ActionDesignatorDescription`\n- **Description**: Set the gripper state of the robot.\n- **Attributes**:\n  - **grippers**: `List[pycram.datastructures.enums.GripperState]`\n  - **motions**: `List[pycram.datastructures.enums.Arms]`\n- **Methods**:\n  - **ground()**: Returns a `SetGripperActionPerformable`.\n  - **__iter__()**: Iterates over possible combinations and returns a `SetGripperActionPerformable`.\n\n#### **pycram.designators.action_designator.ReleaseAction**\n- **Inherits**: `pycram.designator.ActionDesignatorDescription`\n- **Description**: Releases an Object from the robot.\n- **Attributes**:\n  - **grippers**: `List[pycram.datastructures.enums.Arms]`\n  - **object_designator_description**\n- **Methods**:\n  - **ground()**: Returns a `ReleaseActionPerformable`.\n\n#### **pycram.designators.action_designator.GripAction**\n- **Inherits**: `pycram.designator.ActionDesignatorDescription`\n- **Description**: Grip an object with the robot.\n- **Attributes**:\n  - **grippers**: `List[pycram.datastructures.enums.Arms]`\n  - **object_designator_description**: `pycram.designators.object_designator.ObjectDesignatorDescription`\n  - **efforts**: `List[float]`\n- **Methods**:\n  - **ground()**: Returns a `GripActionPerformable`.\n\n#### **pycram.designators.action_designator.ParkArmsAction**\n- **Inherits**: `pycram.designator.ActionDesignatorDescription`\n- **Description**: Park the arms of the robot.\n- **Attributes**:\n  - **arms**: `List[pycram.datastructures.enums.Arms]`\n- **Methods**:\n  - **ground()**: Returns a `ParkArmsActionPerformable`.\n\n#### **pycram.designators.action_designator.PickUpAction**\n- **Inherits**: `pycram.designator.ActionDesignatorDescription`\n- **Description**: Designator to let the robot pick up an object.\n- **Attributes**:\n  - **object_designator_description**: `Union[pycram.designators.object_designator.ObjectDesignatorDescription, pycram.designators.object_designator.Object]`\n  - **arms**: `List[pycram.datastructures.enums.Arms]`\n  - **grasps**: `List[pycram.datastructures.enums.Grasp]`\n- **Methods**:\n  - **ground()**: Returns a `PickUpActionPerformable`.\n\n#### **pycram.designators.action_designator.PlaceAction**\n- **Inherits**: `pycram.designator.ActionDesignatorDescription`\n- **Description**: Places an Object at a position using an arm.\n- **Attributes**:\n  - **object_designator_description**: `Union[pycram.designators.object_designator.ObjectDesignatorDescription, pycram.designators.object_designator.Object]`\n  - **target_locations**: `List[pycram.datastructures.pose.Pose]`\n  - **arms**: `List[pycram.datastructures.enums.Arms]`\n- **Methods**:\n  - **ground()**: Returns a `PlaceActionPerformable`.\n\n#### **pycram.designators.action_designator.NavigateAction**\n- **Inherits**: `pycram.designator.ActionDesignatorDescription`\n- **Description**: Navigates the Robot to a position.\n- **Attributes**:\n  - **target_locations**: `List[pycram.datastructures.pose.Pose]`\n- **Methods**:\n  - **ground()**: Returns a `NavigateActionPerformable`.\n\n#### **pycram.designators.action_designator.TransportAction**\n- **Inherits**: `pycram.designator.ActionDesignatorDescription`\n- **Description**: Transports an object to a position using an arm.\n- **Attributes**:\n  - **object_designator_description**: `Union[pycram.designators.object_designator.ObjectDesignatorDescription, pycram.designators.object_designator.Object]`\n  - **arms**: `List[pycram.datastructures.enums.Arms]`\n  - **target_locations**: `List[pycram.datastructures.pose.Pose]`\n- **Methods**:\n  - **ground()**: Returns a `TransportActionPerformable`.\n\n#### **pycram.designators.action_designator.LookAtAction**\n- **Inherits**: `pycram.designator.ActionDesignatorDescription`\n- **Description**: Lets the robot look at a position.\n- **Attributes**:\n  - **targets**: `List[pycram.datastructures.pose.Pose]`\n- **Methods**:\n  - **ground()**: Returns a `LookAtActionPerformable`.\n\n#### **pycram.designators.action_designator.DetectAction**\n- **Inherits**: `pycram.designator.ActionDesignatorDescription`\n- **Description**: Detects an object that fits the object description and returns an object designator describing the object.\n- **Attributes**:\n  - **object_designator_description**: `pycram.designators.object_designator.ObjectDesignatorDescription`\n- **Methods**:\n  - **ground()**: Returns a `DetectActionPerformable`.\n\n#### **pycram.designators.action_designator.OpenAction**\n- **Inherits**: `pycram.designator.ActionDesignatorDescription`\n- **Description**: Opens a container like object.\n- **Attributes**:\n  - **object_designator_description**: `pycram.designators.object_designator.ObjectPart`\n  - **\n\narms**: `List[pycram.datastructures.enums.Arms]`\n- **Methods**:\n  - **ground()**: Returns an `OpenActionPerformable`.\n\n#### **pycram.designators.action_designator.CloseAction**\n- **Inherits**: `pycram.designator.ActionDesignatorDescription`\n- **Description**: Closes a container like object.\n- **Attributes**:\n  - **object_designator_description**: `pycram.designators.object_designator.ObjectPart`\n  - **arms**: `List[pycram.datastructures.enums.Arms]`\n- **Methods**:\n  - **ground()**: Returns a `CloseActionPerformable`.\n\n#### **pycram.designators.action_designator.GraspingAction**\n- **Inherits**: `pycram.designator.ActionDesignatorDescription`\n- **Description**: Grasps an object described by the given Object Designator description.\n- **Attributes**:\n  - **arms**: `List[pycram.datastructures.enums.Arms]`\n  - **object_description**: `Union[pycram.designators.object_designator.ObjectDesignatorDescription, pycram.designators.object_designator.ObjectPart]`\n- **Methods**:\n  - **ground()**: Returns a `GraspingActionPerformable`.\n\n#### **pycram.designators.action_designator.ActionAbstract**\n- **Inherits**: `pycram.designator.ActionDesignatorDescription.Action, abc.ABC`\n- **Description**: Base class for performable actions.\n- **Attributes**:\n  - **orm_class**: `Type[pycram.orm.action_designator.Action]`\n- **Methods**:\n  - **abstract perform()**: Must be overwritten by each action.\n  - **to_sql()**: Converts the action to its ORM equivalent.\n  - **insert(session: sqlalchemy.orm.Session, **kwargs)**: Inserts the action into the database.\n\n#### **pycram.designators.action_designator.MoveTorsoActionPerformable**\n- **Inherits**: `ActionAbstract`\n- **Description**: Move the torso of the robot up and down.\n- **Attributes**:\n  - **position**: `float`\n- **Methods**:\n  - **perform()**: Overwritten by each action.\n\n#### **pycram.designators.action_designator.SetGripperActionPerformable**\n- **Inherits**: `ActionAbstract`\n- **Description**: Set the gripper state of the robot.\n- **Attributes**:\n  - **gripper**: `pycram.datastructures.enums.Arms`\n  - **motion**: `pycram.datastructures.enums.GripperState`\n- **Methods**:\n  - **perform()**: Overwritten by each action.\n\n#### **pycram.designators.action_designator.ReleaseActionPerformable**\n- **Inherits**: `ActionAbstract`\n- **Description**: Releases an Object from the robot.\n- **Attributes**:\n  - **gripper**: `pycram.datastructures.enums.Arms`\n  - **object_designator**: `pycram.designators.object_designator.ObjectDesignatorDescription.Object`\n- **Methods**:\n  - **perform()**: Overwritten by each action.\n\n#### **pycram.designators.action_designator.GripActionPerformable**\n- **Inherits**: `ActionAbstract`\n- **Description**: Grip an object with the robot.\n- **Attributes**:\n  - **gripper**: `pycram.datastructures.enums.Arms`\n  - **object_designator**: `pycram.designators.object_designator.ObjectDesignatorDescription.Object`\n  - **effort**: `float`\n- **Methods**:\n  - **perform()**: Overwritten by each action.\n\n#### **pycram.designators.action_designator.ParkArmsActionPerformable**\n- **Inherits**: `ActionAbstract`\n- **Description**: Park the arms of the robot.\n- **Attributes**:\n  - **arm**: `pycram.datastructures.enums.Arms`\n- **Methods**:\n  - **perform()**: Overwritten by each action.\n\n#### **pycram.designators.action_designator.PickUpActionPerformable**\n- **Inherits**: `ActionAbstract`\n- **Description**: Let the robot pick up an object.\n- **Attributes**:\n  - **object_designator**: `pycram.designators.object_designator.ObjectDesignatorDescription.Object`\n  - **arm**: `pycram.datastructures.enums.Arms`\n  - **grasp**: `pycram.datastructures.enums.Grasp`\n  - **object_at_execution**: `Optional[pycram.designators.object_designator.ObjectDesignatorDescription.Object]`\n- **Methods**:\n  - **perform()**: Overwritten by each action.\n\n#### **pycram.designators.action_designator.PlaceActionPerformable**\n- **Inherits**: `ActionAbstract`\n- **Description**: Places an Object at a position using an arm.\n- **Attributes**:\n  - **object_designator**: `pycram.designators.object_designator.ObjectDesignatorDescription.Object`\n  - **arm**: `pycram.datastructures.enums.Arms`\n  - **target_location**: `pycram.datastructures.pose.Pose`\n- **Methods**:\n  - **perform()**: Overwritten by each action.\n\n#### **pycram.designators.action_designator.NavigateActionPerformable**\n- **Inherits**: `ActionAbstract`\n- **Description**: Navigates the Robot to a position.\n- **Attributes**:\n  - **target_location**: `pycram.datastructures.pose.Pose`\n- **Methods**:\n  - **perform()**: Overwritten by each action.\n\n#### **pycram.designators.action_designator.TransportActionPerformable**\n- **Inherits**: `ActionAbstract`\n- **Description**: Transports an object to a position using an arm.\n- **Attributes**:\n  - **object_designator**: `pycram.designators.object_designator.ObjectDesignatorDescription.Object`\n  - **arm**: `pycram.datastructures.enums.Arms`\n  - **target_location**: `pycram.datastructures.pose.Pose`\n- **Methods**:\n  - **perform()**: Overwritten by each action.\n\n#### **pycram.designators.action_designator.LookAtActionPerformable**\n- **Inherits**: `ActionAbstract`\n- **Description**: Lets the robot look at a position.\n- **Attributes**:\n  - **target**: `pycram.datastructures.pose.Pose`\n- **Methods**:\n  - **perform()**: Overwritten by each action.\n\n#### **pycram.designators.action_designator.DetectActionPerformable**\n- **Inherits**: `ActionAbstract`\n- **Description**: Detects an object that fits the object description and returns an object designator describing the object.\n- **Attributes**:\n  - **object_designator**: `pycram.designators.object_designator.ObjectDesignatorDescription.Object`\n- **Methods**:\n  - **perform()**: Overwritten by each action.\n\n#### **pycram.designators.action_designator.OpenActionPerformable**\n- **Inherits**: `ActionAbstract`\n- **Description**: Opens a container like object.\n- **Attributes**:\n  - **object_designator**: `pycram.designators.object_designator.ObjectPart.Object`\n  - **arm**: `pycram.datastructures.enums.Arms`\n- **Methods**:\n  - **perform()**: Overwritten by each action.\n\n#### **pycram.designators.action_designator.CloseActionPerformable**\n- **Inherits**: `ActionAbstract`\n- **Description**: Closes a container like object.\n- **Attributes**:\n  - **object_designator**: `pycram.designators.object_designator.ObjectPart.Object`\n  - **arm**: `pycram.datastructures.enums.Arms`\n- **Methods**:\n  - **perform()**: Overwritten by each action.\n\n#### **pycram.designators.action_designator.GraspingActionPerformable**\n- **Inherits**: `ActionAbstract`\n- **Description**: Grasps an object described by the given Object Designator description.\n- **Attributes**:\n  - **arm**: `pycram.datastructures.enums.Arms`\n  - **object_desig**: `Union[pycram.designators.object_designator.ObjectDesignatorDescription.Object, pycram.designators.object_designator.ObjectPart.Object]`\n- **Methods**:\n  - **perform()**: Overwritten by each action.\n\n#### **pycram.designators.action_designator.FaceAtPerformable**\n- **Inherits**: `ActionAbstract`\n- **Description**: Turn the robot chassis such that it faces the pose and then perform a look at action.\n- **Attributes**:\n  - **pose**: `pycram.datastructures.pose.Pose`\n- **Methods**:\n  - **perform()**: Overwritten by each action.\n\n#### **pycram.designators.action_designator.MoveAndPickUpPerformable**\n- **Inherits**: `ActionAbstract`\n- **Description**: Navigate to standing position, turn towards the object, and pick it up.\n- **Attributes**:\n  - **standing_position**: `pycram.datastructures.pose.Pose`\n  - **object_designator**: `pycram.designators.object_designator.ObjectDesignatorDescription.Object`\n  - **arm**: `pycram.datastructures.enums.Arms`\n  - **grasp**: `pycram.datastructures.enums\n\n.Grasp`\n- **Methods**:\n  - **perform()**: Overwritten by each action."}]}
{"messages": [{"role": "system", "content": "You are a coding expert and know how to write PyCRAM Plan Code."}, {"role": "user", "content": "What are the key features of the pose data structure in the PyCRAM documentation?"}, {"role": "assistant", "content": "pycram.datastructures.pose\n### Classes\n\n**Pose**\n- Pose representation for PyCRAM, this class extends the PoseStamped ROS message from geometry_msgs. Thus making it compatible with every ROS service and message expecting a PoseStamped message.\n\n**Transform**\n- Represents a Transformation from one TF frame to another in PyCRAM. Like with Poses, this class inherits from the ROS message TransformStamped from geometry_msgs, making it compatible with ROS services and messages requiring a TransformStamped message.\n\n### Functions\n\n**get_normalized_quaternion( geometry_msgs.msg.Quaternion)**\n- Normalizes a given quaternion such that it has a magnitude of 1.\n\n### Module Contents\n\n**pycram.datastructures.pose.get_normalized_quaternion(quaternion: numpy.ndarray)  geometry_msgs.msg.Quaternion**\n- Normalizes a given quaternion such that it has a magnitude of 1.\n\n  **Parameters:**\n  - quaternion: The quaternion that should be normalized\n\n  **Returns:**\n  - The normalized quaternion\n\n**class pycram.datastructures.pose.Pose(position: typing_extensions.Optional[typing_extensions.List[float]] = None, orientation: typing_extensions.Optional[typing_extensions.List[float]] = None, frame: str = 'map', time: rospy.Time = None)**\n- Bases: geometry_msgs.msg.PoseStamped\n- Pose representation for PyCRAM, this class extends the PoseStamped ROS message from geometry_msgs, making it compatible with every ROS service and message expecting a PoseStamped message.\n\n  **Naming convention for Poses:**\n  - Pose: Instances of this class, representing a cartesian position and a quaternion for orientation\n  - Position: Only the cartesian position in xyz\n  - Orientation: Only the quaternion as xyzw\n\n  **Properties:**\n  - **frame:** str\n    - Property for the frame_id such that it is easier accessible. Instead of Pose.header.frame_id, it is Pose.frame\n    - **Returns:** The TF frame of this Pose\n\n  - **position:** geometry_msgs.msg.Point\n    - Property that points to the position of this pose\n\n  - **orientation:** geometry_msgs.msg.Quaternion\n    - Property that points to the orientation of this pose\n\n  **Methods:**\n  - **static from_pose_stamped(pose_stamped: geometry_msgs.msg.PoseStamped)  Pose**\n    - Converts a geometry_msgs/PoseStamped message to a Pose object. Should be used for compatibility with ROS.\n\n    **Parameters:**\n    - pose_stamped: The pose stamped message which should be converted\n\n    **Returns:**\n    - A Pose object with the same information as the given message\n\n  - **to_list()  typing_extensions.List[typing_extensions.List[float]]**\n    - Returns the position and orientation of this pose as a list containing two lists.\n\n    **Returns:**\n    - The position and orientation as lists\n\n  - **to_transform(child_frame: str)  Transform**\n    - Converts this pose to a Transform from the TF frame of the pose to the given child_frame.\n\n    **Parameters:**\n    - child_frame: Child frame id to which the Transform points\n\n    **Returns:**\n    - A new Transform\n\n  - **copy()  Pose**\n    - Creates a deep copy of this pose.\n\n    **Returns:**\n    - A copy of this pose\n\n  - **position_as_list()  typing_extensions.List[float]**\n    - Returns only the position as a list of xyz.\n\n    **Returns:**\n    - The position as a list\n\n  - **orientation_as_list()  typing_extensions.List[float]**\n    - Returns only the orientation as a list of a quaternion\n\n    **Returns:**\n    - The orientation as a quaternion with xyzw\n\n  - **dist(other_pose: Pose)  float**\n    - Calculates the Euclidean distance between this Pose and the given one. For distance calculation, only the position is used.\n\n    **Parameters:**\n    - other_pose: Pose to which the distance should be calculated\n\n    **Returns:**\n    - The distance between the Poses\n\n  - **__eq__(other: Pose)  bool**\n    - Overloads the == operator to check for equality between two Poses. Only compares the position, orientation, and frame. Timestamps of Poses are not taken into account.\n\n    **Parameters:**\n    - other: Other pose which should be compared\n\n    **Returns:**\n    - True if both Poses have the same position, orientation, and frame. False otherwise\n\n  - **set_position(new_position: typing_extensions.List[float])  None**\n    - Sets the position of this Pose to the given position. Position has to be given as a vector in cartesian space.\n\n    **Parameters:**\n    - new_position: New position as a vector of xyz\n\n  - **set_orientation(new_orientation: typing_extensions.List[float])  None**\n    - Sets the orientation to the given quaternion. The new orientation has to be given as a quaternion.\n\n    **Parameters:**\n    - new_orientation: New orientation as a quaternion with xyzw\n\n  - **to_sql()  pycram.orm.base.Pose**\n\n  - **insert(session: sqlalchemy.orm.Session)  pycram.orm.base.Pose**\n\n  - **multiply_quaternions(quaternion: typing_extensions.List)  None**\n    - Multiply the quaternion of this Pose with the given quaternion. The result will be the new orientation of this Pose.\n\n    **Parameters:**\n    - quaternion: The quaternion by which the orientation of this Pose should be multiplied\n\n  - **set_orientation_from_euler(axis: typing_extensions.List, euler_angles: typing_extensions.List[float])  None**\n    - Convert axis-angle to quaternion.\n\n    **Parameters:**\n    - axis: (x, y, z) tuple representing rotation axis.\n    - angle: rotation angle in degree\n\n    **Returns:**\n    - The quaternion representing the axis angle\n\n**class pycram.datastructures.pose.Transform(translation: typing_extensions.Optional[typing_extensions.List[float]] = None, rotation: typing_extensions.Optional[typing_extensions.List[float]] = None, frame: typing_extensions.Optional[str] = 'map', child_frame: typing_extensions.Optional[str] = '', time: rospy.Time = None)**\n- Bases: geometry_msgs.msg.TransformStamped\n- Represents a Transformation from one TF frame to another in PyCRAM. Like with Poses, this class inherits from the ROS message TransformStamped from geometry_msgs, making it compatible with ROS services and messages requiring a TransformStamped message.\n\n  **Naming Convention for Transforms:**\n  - Transform: Instances of this class, representing a translation and rotation from frame_id to child_frame_id\n  - Translation: A vector representing the conversion in cartesian space\n  - Rotation: A quaternion representing the conversion of rotation between both frames\n\n  **Properties:**\n  - **frame:** str\n    - Property for the frame_id such that it is easier accessible. Instead of Pose.header.frame_id, it is Pose.frame\n\n    **Returns:**\n    - The TF frame of this Pose\n\n  - **translation:** None\n    - Property that points to the translation of this Transform\n\n  - **rotation:** None\n    - Property that points to the rotation of this Transform\n\n  **Methods:**\n  - **classmethod from_pose_and_child_frame(pose: Pose, child_frame_name: str)  Transform**\n\n  - **static from_transform_stamped(transform_stamped: geometry_msgs.msg.TransformStamped)  Transform**\n    - Creates a Transform instance from a geometry_msgs/TransformStamped message. Should be used for compatibility with ROS.\n\n    **Parameters:**\n    - transform_stamped: The transform stamped message that should be converted\n\n    **Returns:**\n    - A Transform with the same information as the transform stamped message\n\n  - **copy()  Transform**\n    - Creates a deep copy of this pose.\n\n    **Returns:**\n    - A copy of this pose\n\n  - **translation_as_list()  typing_extensions.List[float]**\n    - Returns the translation of this Transform as a list.\n\n    **Returns:**\n    - The translation as a list of xyz\n\n  - **rotation_as_list()  typing_extensions.List[float]**\n    - Returns the rotation of this Transform as a list representing a quaternion.\n\n    **Returns:**\n    - The rotation of this Transform as a list with xyzw\n\n  - **to_pose()  Pose**\n    - Converts this Transform to a Pose, in this process the child_frame_id is lost.\n\n    **Returns:**\n    - A new pose with the same translation as position and rotation as orientation\n\n  - **invert()  Transform**\n    - Inverts this Transform. The new Transform points from the child_frame_id to the frame_id.\n\n    **Returns:**\n    - A new inverted Transform\n\n  - **__mul__(other: Transform)  typing_extensions.Union[Transform, None]**\n    - Multiplies this Transform with another one. The resulting Transform points from the frame_id of this Transform to the child_frame_id of the other Transform.\n\n    **Parameters:**\n    - other: The Transform which should be multiplied with this one.\n\n    **Returns:**\n    - The resulting Transform from the multiplication\n\n  - **inverse_times(other_transform: Transform)  Transform**\n    - Like a minus for Transforms, subtracts the other_transform from this one.\n\n    **Parameters:**\n    - other_transform: Transform which should be subtracted from this one\n\n    **Returns:**\n    - The resulting Transform from the calculation\n\n  - **__eq__(other: Transform)  bool**\n    - Overloads the == operator to check for equality between two Transforms. Only compares the translation, rotation, frame, and child frame. Timestamps of Poses are not taken into account.\n\n    **Parameters:**\n    - other: Other pose which should be compared\n\n    **Returns:**\n    - True if both Transforms have the same translation, rotation, frame, and child frame. False otherwise\n\n  - **set_translation(new_translation: typing_extensions.List[float])  None**\n    - Sets the translation of this Transform to the newly given one. Translation has to be a vector in cartesian space.\n\n    **Parameters:**\n    - new_translation: The new translation as a vector with xyz.\n\n  - **set_rotation(new_rotation: typing_extensions.List[float])  None**\n    - Sets the rotation of this Transform to the newly given one. Rotation has to be a quaternion.\n\n    **Parameters:**\n    - new_rotation: The new rotation as a quaternion with xyzw"}]}
{"messages": [{"role": "system", "content": "You are a coding expert and know how to write PyCRAM Plan Code."}, {"role": "user", "content": "What are the key features of the location designator in the PyCRAM documentation?"}, {"role": "assistant", "content": "pycram.designators.location_designator \n===========================================\n\n### **Classes**\n\n- **Location**  \n  Default location designator which only wraps a pose.\n\n- **ObjectRelativeLocation**  \n  Location relative to an object.\n\n- **CostmapLocation**  \n  Uses Costmaps to create locations for complex constraints.\n\n- **AccessingLocation**  \n  Location designator that describes poses used for opening drawers.\n\n- **SemanticCostmapLocation**  \n  Locations over semantic entities, like a table surface.\n\n**Module Contents**  \n### **Location**  \nBases: `pycram.designator.LocationDesignatorDescription`\n\nDefault location designator which only wraps a pose.\n\n- **Class Location**  \n  Bases: `pycram.designator.LocationDesignatorDescription.Location`\n\n  Resolved location that represents a specific point in the world which satisfies the constraints of the location designator description.\n\n  - **Attributes:**\n    - **pose**: `pycram.datastructures.pose.Pose`\n\n  - **Methods:**\n    - **ground()  Location**  \n      Default specialized designator that returns a resolved designator containing the pose given in `__init__`.\n      - **Returns:** A resolved designator.\n\n### **ObjectRelativeLocation**  \nBases: `pycram.designator.LocationDesignatorDescription`\n\nLocation relative to an object.\n\n- **Class Location**  \n  Bases: `pycram.designator.LocationDesignatorDescription.Location`\n\n  Resolved location that represents a specific point in the world which satisfies the constraints of the location designator description.\n\n  - **Attributes:**\n    - **relative_pose**: `pycram.datastructures.pose.Pose`  \n      Pose relative to the object.\n    - **reference_object**: `pycram.designators.object_designator.ObjectDesignatorDescription.Object`  \n      Object to which the pose is relative.\n\n  - **Methods:**\n    - **ground()  Location**  \n      Default specialized designator that returns a resolved location based on the description input. The resolved location is the first result from the iteration of this instance.\n      - **Returns:** A resolved location.\n\n    - **__iter__()  typing_extensions.Iterable[Location]**  \n      Iterates over all possible solutions for a resolved location relative to the given object.\n      - **Yields:** An instance of `ObjectRelativeLocation.Location` with the relative pose.\n\n### **CostmapLocation**  \nBases: `pycram.designator.LocationDesignatorDescription`\n\nUses Costmaps to create locations for complex constraints.\n\n- **Class Location**  \n  Bases: `pycram.designator.LocationDesignatorDescription.Location`\n\n  Resolved location that represents a specific point in the world which satisfies the constraints of the location designator description.\n\n  - **Attributes:**\n    - **reachable_arms**: `typing_extensions.List[pycram.datastructures.enums.Arms]`  \n      List of arms with which the pose can be reached, used when the `reachable_for` parameter is applied.\n\n  - **Methods:**\n    - **ground()  Location**  \n      Default specialized designator that returns the first result from the iteration of this instance.\n      - **Returns:** A resolved location.\n\n    - **__iter__()**  \n      Generates positions from a costmap based on given constraints and returns them. The generation is based on a costmap created by merging costmaps, each for a different purpose. The base is always an occupancy costmap, with additional costmaps (e.g., visibility or Gaussian) merged according to the constraints. The generator produces pose candidates from the costmap. Each candidate is validated against the constraints, and valid poses are yielded.\n      - **Yields:** An instance of `CostmapLocation.Location` with a valid position that satisfies the given constraints.\n\n### **AccessingLocation**  \nBases: `pycram.designator.LocationDesignatorDescription`\n\nLocation designator that describes poses used for opening drawers.\n\n- **Class Location**  \n  Bases: `pycram.designator.LocationDesignatorDescription.Location`\n\n  Resolved location that represents a specific point in the world which satisfies the constraints of the location designator description.\n\n  - **Attributes:**\n    - **arms**: `typing_extensions.List[pycram.datastructures.enums.Arms]`  \n      List of arms that can be used for accessing from this pose.\n\n  - **Methods:**\n    - **ground()  Location**  \n      Default specialized designator for this location designator, returning the first element from the iteration.\n      - **Returns:** A location designator for a pose from which the drawer can be opened.\n\n    - **__iter__()  Location**  \n      Creates poses from which the robot can open the drawer specified by the `ObjectPart` designator describing the handle. Poses are validated by checking if the robot can grasp the handle while the drawer is closed and if the handle can be grasped when the drawer is open.\n      - **Yields:** A location designator containing the pose and the arms that can be used.\n\n### **SemanticCostmapLocation**  \nBases: `pycram.designator.LocationDesignatorDescription`\n\nLocations over semantic entities, like a table surface.\n\n- **Class Location**  \n  Bases: `pycram.designator.LocationDesignatorDescription.Location`\n\n  Resolved location that represents a specific point in the world which satisfies the constraints of the location designator description.\n\n  - **Attributes:**\n    - **urdf_link_name**: `str`\n    - **part_of**: `pycram.designators.object_designator.ObjectDesignatorDescription.Object`\n    - **for_object**: `typing_extensions.Optional[pycram.designators.object_designator.ObjectDesignatorDescription.Object]`\n\n  - **Methods:**\n    - **ground()  Location**  \n      Default specialized designator that returns the first element of the iterator of this instance.\n      - **Returns:** A resolved location.\n\n    - **__iter__()**  \n      Creates a costmap on top of a link of an object and generates positions from it. If there is a specific object for which the position should be found, a height offset will be calculated to ensure that the bottom of the object is at the position in the costmap, not the origin of the object, which is usually in the center.\n      - **Yields:** An instance of `SemanticCostmapLocation.Location` with the valid position from the costmap."}]}
{"messages": [{"role": "system", "content": "You are a coding expert and know how to write PyCRAM Plan Code."}, {"role": "user", "content": "What are the key features of the object designator in the PyCRAM documentation?"}, {"role": "assistant", "content": "pycram.designators.object_designator  \n====================================\n\nClasses  \n\n\n- **BelieveObject**: Description for Objects that are only believed in.\n- **ObjectPart**: Object Designator Descriptions for Objects that are part of some other object.\n- **LocatedObject**: Description for KnowRob located objects.\n- **RealObject**: Object designator representing an object in the real world, when resolving this object designator description ]\n\nModule Contents  \n\n**pycram.designators.object_designator.BelieveObject**  \nBases: `pycram.external_interfaces.robokudo.ObjectDesignatorDescription`\n\nDescription for objects that are only believed in.\n\n- **class Object**  \n  Bases: `pycram.external_interfaces.robokudo.ObjectDesignatorDescription.Object`\n\n  Concrete object that is believed in.\n\n  - **to_sql()  pycram.orm.object_designator.BelieveObject**  \n    Creates an ORM object that corresponds to this description.\n    - **Returns:** The created ORM object.\n\n  - **insert(session: sqlalchemy.orm.session.Session)  pycram.orm.object_designator.BelieveObject**  \n    Adds and commits this and all related objects to the session. Auto-incrementing primary keys and foreign keys have to be filled by this method.\n    - **Parameters:** \n      - **session:** Session with a database that is used to add and commit the objects\n    - **Returns:** The completely instanced ORM object.\n\n**pycram.designators.object_designator.ObjectPart**  \nBases: `pycram.external_interfaces.robokudo.ObjectDesignatorDescription`\n\nObject Designator Description for objects that are part of some other object.\n\n- **class Object**  \n  Bases: `pycram.external_interfaces.robokudo.ObjectDesignatorDescription.Object`\n\n  A single element that fits the description.\n\n  - **Attributes:**\n    - **part_pose:** `pycram.external_interfaces.robokudo.Pose`\n\n  - **to_sql()  pycram.orm.object_designator.ObjectPart**  \n    Creates an ORM object that corresponds to this description.\n    - **Returns:** The created ORM object.\n\n  - **insert(session: sqlalchemy.orm.session.Session)  pycram.orm.object_designator.ObjectPart**  \n    Adds and commits this and all related objects to the session. Auto-incrementing primary keys and foreign keys have to be filled by this method.\n    - **Parameters:** \n      - **session:** Session with a database that is used to add and commit the objects\n    - **Returns:** The completely instanced ORM object.\n\n  - **ground()  Object**  \n    Default specialized designator, returns the first result of the iterator of this instance.\n    - **Returns:** A resolved object designator.\n\n  - **__iter__()**  \n    Iterates through every possible solution for the given input parameter.\n    - **Yields:** A resolved object designator.\n\n  - **Additional Attributes:**\n    - **type:** `pycram.external_interfaces.robokudo.Optional[str]`\n    - **names:** `pycram.external_interfaces.robokudo.Optional[pycram.external_interfaces.robokudo.List[str]]`\n    - **part_of**\n\n**pycram.designators.object_designator.LocatedObject**  \nBases: `pycram.external_interfaces.robokudo.ObjectDesignatorDescription`\n\nDescription for KnowRob located objects.  \n**Currently has no specialized designators.**\n\n- **class Object**  \n  Bases: `pycram.external_interfaces.robokudo.ObjectDesignatorDescription.Object`\n\n  A single element that fits the description.\n\n  - **Attributes:**\n    - **reference_frame:** `str`\n      - Reference frame in which the position is given.\n    - **timestamp:** `float`\n      - Timestamp at which the position was valid.\n    - **Additional Attributes:**\n      - **reference_frames:** `pycram.external_interfaces.robokudo.List[str]`\n      - **timestamps:** `pycram.external_interfaces.robokudo.List[float]`\n\n**pycram.designators.object_designator.RealObject**  \nBases: `pycram.external_interfaces.robokudo.ObjectDesignatorDescription`\n\nObject designator representing an object in the real world. When resolving this object designator description, RoboKudo is queried to perceive an object fitting the given criteria. Afterward, the specialized designator tries to match the found object to an object in the world.\n\n- **class Object**  \n  Bases: `pycram.external_interfaces.robokudo.ObjectDesignatorDescription.Object`\n\n  A single element that fits the description.\n\n  - **Attributes:**\n    - **pose:** `pycram.external_interfaces.robokudo.Pose`\n      - Pose of the perceived object.\n    - **types:** `pycram.external_interfaces.robokudo.Optional[pycram.external_interfaces.robokudo.List[str]]`\n    - **names:** `pycram.external_interfaces.robokudo.Optional[pycram.external_interfaces.robokudo.List[str]]`\n    - **world_object:** `pycram.world_concepts.world_object.Object`\n\n  - **__iter__()**  \n    Queries RoboKudo for objects that fit the description and then iterates over all World objects that have the same type to match a World object to the real object.\n    - **Yields:** A resolved object designator with reference to the world object."}]}
{"messages": [{"role": "system", "content": "You are a coding expert and know how to write PyCRAM Plan Code."}, {"role": "user", "content": "What are the key features of the motion designator in the PyCRAM documentation?"}, {"role": "assistant", "content": "pycram.designators.motion_designator  \n==========================================\n\n### **Classes**\n\n- **MoveMotion**  \n  Moves the robot to a designated location.\n\n- **MoveTCPMotion**  \n  Moves the Tool Center Point (TCP) of the robot.\n\n- **LookingMotion**  \n  Lets the robot look at a point.\n\n- **MoveGripperMotion**  \n  Opens or closes the gripper.\n\n- **DetectingMotion**  \n  Tries to detect an object in the Field of View (FOV) of the robot.\n\n- **MoveArmJointsMotion**  \n  Moves the joints of each arm into the given position.\n\n- **WorldStateDetectingMotion**  \n  Detects an object based on the world state.\n\n- **MoveJointsMotion**  \n  Moves any joint on the robot.\n\n- **OpeningMotion**  \n  Designator for opening a container.\n\n- **ClosingMotion**  \n  Designator for closing a container.\n\n- **TalkingMotion**  \n  Talking Motion, lets the robot say a sentence.\n\n**Module Contents**  \n\n### **Class Details**\n\n#### **MoveMotion**\n   Bases: `pycram.designator.BaseMotion`  \n   Moves the robot to a designated location.\n\n- **Attributes:**\n  - `target`: `pycram.datastructures.pose.Pose`  \n    Location to which the robot should be moved.\n\n- **Methods:**\n  - `perform()`: Passes this designator to the process module for execution.\n  - `to_sql() -> pycram.orm.motion_designator.MoveMotion`: Creates an ORM object corresponding to this description.\n  - `insert(session, *args, **kwargs) -> pycram.orm.motion_designator.MoveMotion`: Adds and commits this and all related objects to the session.\n\n#### **MoveTCPMotion**\n   Bases: `pycram.designator.BaseMotion`  \n   Moves the Tool Center Point (TCP) of the robot.\n\n- **Attributes:**\n  - `target`: `pycram.datastructures.pose.Pose`  \n    Target pose to which the TCP should be moved.\n  - `arm`: `pycram.datastructures.enums.Arms`  \n    Arm with the TCP that should be moved to the target.\n  - `allow_gripper_collision`: `typing_extensions.Optional[bool] = None`  \n    If the gripper can collide with something.\n\n- **Methods:**\n  - `perform()`: Passes this designator to the process module for execution.\n  - `to_sql() -> pycram.orm.motion_designator.MoveTCPMotion`: Creates an ORM object corresponding to this description.\n  - `insert(session: sqlalchemy.orm.Session, *args, **kwargs) -> pycram.orm.motion_designator.MoveTCPMotion`: Adds and commits this and all related objects to the session.\n\n#### **LookingMotion**\n   Bases: `pycram.designator.BaseMotion`  \n   Lets the robot look at a point.\n\n- **Attributes:**\n  - `target`: `pycram.datastructures.pose.Pose`  \n\n- **Methods:**\n  - `perform()`: Passes this designator to the process module for execution.\n  - `to_sql() -> pycram.orm.motion_designator.LookingMotion`: Creates an ORM object corresponding to this description.\n  - `insert(session: sqlalchemy.orm.Session, *args, **kwargs) -> pycram.orm.motion_designator.LookingMotion`: Adds and commits this and all related objects to the session.\n\n#### **MoveGripperMotion**\n   Bases: `pycram.designator.BaseMotion`  \n   Opens or closes the gripper.\n\n- **Attributes:**\n  - `motion`: `pycram.datastructures.enums.GripperState`  \n    Motion that should be performed, either 'open' or 'close'.\n  - `gripper`: `pycram.datastructures.enums.Arms`  \n    Name of the gripper that should be moved.\n  - `allow_gripper_collision`: `typing_extensions.Optional[bool] = None`  \n    If the gripper is allowed to collide with something.\n\n- **Methods:**\n  - `perform()`: Passes this designator to the process module for execution.\n  - `to_sql() -> pycram.orm.motion_designator.MoveGripperMotion`: Creates an ORM object corresponding to this description.\n  - `insert(session: sqlalchemy.orm.Session, *args, **kwargs) -> pycram.orm.motion_designator.MoveGripperMotion`: Adds and commits this and all related objects to the session.\n\n#### **DetectingMotion**\n   Bases: `pycram.designator.BaseMotion`  \n   Tries to detect an object in the FOV of the robot.\n\n- **Attributes:**\n  - `object_type`: `pycram.datastructures.enums.ObjectType`  \n    Type of the object that should be detected.\n\n- **Methods:**\n  - `perform()`: Passes this designator to the process module for execution.\n  - `to_sql() -> pycram.orm.motion_designator.DetectingMotion`: Creates an ORM object corresponding to this description.\n  - `insert(session: sqlalchemy.orm.Session, *args, **kwargs) -> pycram.orm.motion_designator.DetectingMotion`: Adds and commits this and all related objects to the session.\n\n#### **MoveArmJointsMotion**\n   Bases: `pycram.designator.BaseMotion`  \n   Moves the joints of each arm into the given position.\n\n- **Attributes:**\n  - `left_arm_poses`: `typing_extensions.Optional[typing_extensions.Dict[str, float]] = None`  \n    Target positions for the left arm joints.\n  - `right_arm_poses`: `typing_extensions.Optional[typing_extensions.Dict[str, float]] = None`  \n    Target positions for the right arm joints.\n\n- **Methods:**\n  - `perform()`: Passes this designator to the process module for execution.\n  - `to_sql() -> pycram.orm.motion_designator.Motion`: Creates an ORM object corresponding to this description.\n  - `insert(session: sqlalchemy.orm.Session, *args, **kwargs) -> pycram.orm.motion_designator.Motion`: Adds and commits this and all related objects to the session.\n\n#### **WorldStateDetectingMotion**\n   Bases: `pycram.designator.BaseMotion`  \n   Detects an object based on the world state.\n\n- **Attributes:**\n  - `object_type`: `pycram.datastructures.enums.ObjectType`  \n    Object type that should be detected.\n\n- **Methods:**\n  - `perform()`: Passes this designator to the process module for execution.\n  - `to_sql() -> pycram.orm.motion_designator.Motion`: Creates an ORM object corresponding to this description.\n  - `insert(session: sqlalchemy.orm.Session, *args, **kwargs) -> pycram.orm.motion_designator.Motion`: Adds and commits this and all related objects to the session.\n\n#### **MoveJointsMotion**\n   Bases: `pycram.designator.BaseMotion`  \n   Moves any joint on the robot.\n\n- **Attributes:**\n  - `names`: `list`  \n    List of joint names that should be moved.\n  - `positions`: `list`  \n    Target positions of joints, should correspond to the list of names.\n\n- **Methods:**\n  - `perform()`: Passes this designator to the process module for execution.\n  - `to_sql() -> pycram.orm.motion_designator.Motion`: Creates an ORM object corresponding to this description.\n  - `insert(session: sqlalchemy.orm.Session, *args, **kwargs) -> pycram.orm.motion_designator.Motion`: Adds and commits this and all related objects to the session.\n\n#### **OpeningMotion**\n   Bases: `pycram.designator.BaseMotion`  \n   Designator for opening a container.\n\n- **Attributes:**\n  - `object_part`: `pycram.designators.object_designator.ObjectPart.Object`  \n    Object designator for the drawer handle.\n  - `arm`: `pycram.datastructures.enums.Arms`  \n    Arm that should be used.\n\n- **Methods:**\n  - `perform()`: Passes this designator to the process module for execution.\n  - `to_sql() -> pycram.orm.motion_designator.OpeningMotion`: Creates an ORM object corresponding to this description.\n  - `insert(session: sqlalchemy.orm.Session, *args, **kwargs) -> pycram.orm.motion_designator.OpeningMotion`: Adds and commits this and all related objects to the session.\n\n#### **ClosingMotion**\n   Bases: `pycram.designator.BaseMotion`  \n   Designator for closing a container.\n\n- **Attributes:**\n  - `object_part`: `pycram.designators.object_designator.ObjectPart.Object`  \n    Object designator for the drawer handle.\n  - `arm`: `pycram.datastructures.enums.Arms`  \n    Arm that should be used.\n\n- **Methods:**\n  - `perform()`: Passes this designator to the process module for execution.\n  - `to_sql() -> pycram.orm.motion_designator.ClosingMotion`: Creates an ORM object corresponding to this description.\n  - `insert(session: sqlalchemy.orm.Session, *args, **kwargs) -> pycram.orm.motion_designator.ClosingMotion`: Adds and commits this and all related objects to the session.\n\n#### **TalkingMotion**\n   Bases: `pycram.designator.BaseMotion`  \n   Talking Motion, lets the robot say a sentence.\n\n- **Attributes:**\n  - `cmd`: `str`  \n    Sentence that the robot should say.\n\n- **Methods:**\n  - `perform()`: Passes this designator to the process module for execution.\n  - `to_sql() -> pycram.orm.motion_designator.Motion`: Creates an ORM object corresponding to this description.\n  -\n\n `insert(session: sqlalchemy.orm.Session, *args, **kwargs) -> pycram.orm.motion_designator.Motion`: Adds and commits this and all related objects to the session."}]}
{"messages": [{"role": "system", "content": "You are a coding expert and know how to write PyCRAM Plan Code."}, {"role": "user", "content": "What functionalities are provided by the costmaps in the PyCRAM documentation?"}, {"role": "assistant", "content": "pycram.costmaps \n====================================\n\n**Attributes**  \n\n- **cmap**\n\n**Classes**  \n\n- **Rectangle**  \n  A rectangle described by lower and upper x and y values.\n\n- **Costmap**  \n  The base class of all Costmaps, implementing the visualization of costmaps.\n\n- **OccupancyCostmap**  \n  Represents a map of the environment where obstacles or inaccessible positions for a robot have a value of -1.\n\n- **VisibilityCostmap**  \n  Represents the visibility of a specific point for every position around it.\n\n- **GaussianCostmap**  \n  Represents 2D Gaussian distributions around the origin with a given mean and sigma.\n\n- **SemanticCostmap**  \n  Represents a 2D distribution over a link of an object, such as a table surface.\n\n**Functions**  \n\n\n- **plot_grid(data: numpy.ndarray)  None**  \n  An auxiliary method only used for debugging. It plots a 2D numpy array using Matplotlib.\n\n**Module Contents** \n\n### **Rectangle**\n\nA rectangle described by lower and upper x and y values.\n\n- **Attributes:**\n  - **x_lower**: float\n  - **x_upper**: float\n  - **y_lower**: float\n  - **y_upper**: float\n\n- **Methods:**\n  - **translate(x: float, y: float)**  \n    Translates the rectangle by x and y.\n\n  - **scale(x_factor: float, y_factor: float)**  \n    Scales the rectangle by x_factor and y_factor.\n\n### **Costmap**  \nBases: Costmap\n\nThe base class of all Costmaps, implementing the visualization of costmaps in the World.\n\n- **Attributes:**\n  - **world**\n  - **resolution**: float\n  - **size**: int\n  - **height**: int\n  - **width**: int\n  - **local_transformer**\n  - **origin**: pycram.datastructures.pose.Pose\n  - **map**: numpy.ndarray\n  - **vis_ids**: typing_extensions.List[int] = []\n\n- **Methods:**\n  - **visualize()  None**  \n    Visualizes a costmap in the BulletWorld by subdividing the costmap into rectangles, which are then visualized as pybullet visual shapes.\n\n  - **_chunks(lst: typing_extensions.List, n: int)  typing_extensions.List**  \n    Yields successive n-sized chunks from lst.\n\n    - **Parameters:**\n      - **lst**: The list from which chunks should be yielded.\n      - **n**: Size of the chunks.\n    - **Returns:** A list of size n from lst.\n\n  - **close_visualization()  None**  \n    Removes the visualization from the World.\n\n  - **_find_consecutive_line(start: typing_extensions.Tuple[int, int], map: numpy.ndarray)  int**  \n    Finds the number of consecutive entries in the costmap that are greater than zero.\n\n    - **Parameters:**\n      - **start**: The indices in the costmap from which the consecutive line should be found.\n      - **map**: The costmap in which the line should be found.\n    - **Returns:** The length of the consecutive line of entries greater than zero.\n\n  - **_find_max_box_height(start: typing_extensions.Tuple[int, int], length: int, map: numpy.ndarray)  int**  \n    Finds the maximum height for a rectangle with a given width in a costmap.\n\n    - **Parameters:**\n      - **start**: The indices in the costmap from which the method should start.\n      - **length**: The given width for the rectangle.\n      - **map**: The costmap in which the search should be conducted.\n    - **Returns:** The height of the rectangle.\n\n  - **merge(other_cm: Costmap)  Costmap**  \n    Merges the values of two costmaps and returns a new costmap with the merged values of both inputs. To merge two costmaps, they must fulfill three constraints:\n    1. Same size.\n    2. Same x and y coordinates in the origin.\n    3. Same resolution.\n\n    - **Parameters:**\n      - **other_cm**: The other costmap to be merged.\n    - **Returns:** A new costmap with merged values.\n\n  - **__add__(other: Costmap)  Costmap**  \n    Overloads the \"+\" operator for merging Costmaps. Ensures that 'other' is a Costmap and raises a ValueError if not. Refer to **merge** for more information.\n\n    - **Parameters:**\n      - **other**: Another Costmap.\n    - **Returns:** A new Costmap with merged values from both Costmaps.\n\n  - **partitioning_rectangles()  typing_extensions.List[Rectangle]**  \n    Partitions the map attached to this costmap into rectangles. The rectangles are axis-aligned, exhaustive, and disjoint.\n\n    - **Returns:** A list containing the partitioning rectangles.\n\n### **OccupancyCostmap**  \nBases: Costmap\n\nRepresents a map of the environment where obstacles or inaccessible positions for a robot have a value of -1.\n\n- **Attributes:**\n  - **world**\n\n- **Methods:**\n  - **_calculate_diff_origin(height: int, width: int)  pycram.datastructures.pose.Pose**  \n    Calculates the difference between the origin of the costmap as stated by the metadata and the actual middle of the costmap used by PyCRAM for visualization.\n\n    - **Parameters:**\n      - **height**: The height of the costmap.\n      - **width**: The width of the costmap.\n    - **Returns:** The difference between the actual origin and center of the costmap.\n\n  - **_get_map()  numpy.ndarray**  \n    Receives the map array from the map server and converts it into a numpy array.\n\n    - **Returns:** The costmap as a numpy array.\n\n  - **_get_map_metadata()  nav_msgs.msg.MapMetaData**  \n    Receives metadata about the costmap from the map server. Metadata includes height, width, origin, and resolution.\n\n    - **Returns:** The metadata for the costmap array.\n\n  - **_convert_map(map: numpy.ndarray)  numpy.ndarray**  \n    Converts the Occupancy Map from ROS to be consistent with PyCRAM's handling of costmaps. Valid cells for a robot are set to one, and others to zero, considering the distance to obstacle parameter.\n\n    - **Parameters:**\n      - **map**: The map to be converted, represented as a 2D numpy array.\n    - **Returns:** The converted map as a 2D numpy array.\n\n  - **create_sub_map(sub_origin: pycram.datastructures.pose.Pose, size: int)  Costmap**  \n    Creates a smaller map centered around a specified point with the size \"size\". The resolution remains the same.\n\n    - **Parameters:**\n      - **sub_origin**: The point in the global coordinate frame around which the sub-costmap should be centered.\n      - **size**: The size the sub-costmap should have.\n    - **Returns:** The sub-costmap as a 2D numpy array.\n\n  - **_create_from_world(size: int, resolution: float)  numpy.ndarray**  \n    Creates an Occupancy Costmap for the specified World. Marks every position as valid that has no object above it, applying the distance to obstacle parameter afterward.\n\n    - **Parameters:**\n      - **size**: The size of the costmap. Specifies the length of one side of the costmap, created as a square.\n      - **resolution**: The resolution of the costmap, determining how much meter a pixel represents.\n\n  - **_chunks(lst: typing_extensions.List, n: int)  typing_extensions.List**  \n    Yields successive n-sized chunks from lst.\n\n    - **Parameters:**\n      - **lst**: The list from which chunks should be yielded.\n      - **n**: Size of the chunks.\n    - **Returns:** A list of size n from lst.\n\n### **VisibilityCostmap**  \nBases: Costmap\n\nRepresents the visibility of a specific point for every position around it. Refer to the PhD Thesis (page 173) for a detailed explanation.\n\n- **Attributes:**\n  - **world**\n  - **map**\n  - **size**\n  - **resolution**\n  - **max_height**: float\n  - **min_height**: float\n  - **origin**: pycram.datastructures.pose.Pose\n\n- **Methods:**\n  - **_create_images()  typing_extensions.List[numpy.ndarray]**  \n    Creates four depth images in every direction around the point for which the costmap should be created. The depth images represent the distance to the next object in meters.\n\n    - **Returns:** A list of four depth images, represented as 2D arrays.\n\n  - **_depth_buffer_to_meter(buffer: numpy.ndarray)  numpy.ndarray**  \n    Converts the depth images generated by the World to represent each position in meters.\n\n    - **Returns:** The depth image in meters.\n\n  - **_generate_map()**  \n    Generates the resulting density map using the algorithm in Lorenz Msenlechner's PhD Thesis (page 178). The map is then saved to `self.map`.\n\n### **GaussianCostmap**  \nBases: Costmap\n\nGaussian Costmaps are 2D Gaussian distributions around the origin with the given mean and sigma.\n\n- **Attributes:**\n  - **gau**: numpy.ndarray\n  - **map**: numpy.ndarray\n  - **size**: float\n  - **origin**: pycram.datas\n\ntructures.pose.Pose\n\n- **Methods:**\n  - **_gaussian_window(mean: int, std: float)  numpy.ndarray**  \n    Creates a window of values with a Gaussian distribution of size \"mean\" and standard deviation \"std\". Code is based on Scipy.\n\n### **SemanticCostmap**  \nBases: Costmap\n\nSemantic Costmaps represent a 2D distribution over a link of an object, such as a table surface.\n\n- **Attributes:**\n  - **world**: pycram.datastructures.world.World\n  - **object**: pycram.world_concepts.world_object.Object\n  - **link**: pycram.description.Link\n  - **resolution**: float\n  - **origin**: pycram.datastructures.pose.Pose\n  - **height**: int = 0\n  - **width**: int = 0\n  - **map**: numpy.ndarray = []\n\n- **Methods:**\n  - **generate_map()  None**  \n    Generates the semantic costmap according to the provided parameters using the axis-aligned bounding box (AABB) for the link. The height and width of the final Costmap correspond to the x and y sizes of the AABB.\n\n  - **get_aabb_for_link()  pycram.datastructures.dataclasses.AxisAlignedBoundingBox**  \n    Returns the AABB of the link provided when creating the costmap. The object is rotated to align the link in the identity orientation."}]}
{"messages": [{"role": "system", "content": "You are a coding expert and know how to write PyCRAM Plan Code."}, {"role": "user", "content": "What information is available about plan failures in the PyCRAM documentation?"}, {"role": "assistant", "content": "pycram.plan_failures  \n====================================\n\n### **Exceptions**\n\n- **PlanFailure**  \n  Implementation of plan failures.\n\n- **NotALanguageExpression**  \n  Implementation of plan failures.\n\n- **FailureDiagnosis**  \n  Implementation of plan failures.\n\n- **LowLevelFailure**  \n  Failure thrown by low-level modules: robot or projection PMs.\n\n- **ActionlibActionTimeout**  \n  Failure thrown by low-level modules: robot or projection PMs.\n\n- **HighLevelFailure**  \n  Failure thrown by high-level modules, i.e., plans.\n\n- **DeliveringFailed**  \n  Thrown when a delivering plan completely gives up.\n\n- **ManipulationLowLevelFailure**  \n  Thrown when a low-level, i.e., hardware-related, failure is detected in a manipulation action.\n\n- **EnvironmentManipulationGoalNotReached**  \n  Thrown when the door/drawer opening/closing goal is still not reached.\n\n- **EnvironmentManipulationImpossible**  \n  Thrown when environment manipulation cannot be achieved.\n\n- **EnvironmentUnreachable**  \n  Thrown when environment manipulation is in collision or unreachable.\n\n- **FetchingFailed**  \n  Thrown when a fetching plan completely gives up.\n\n- **GripperLowLevelFailure**  \n  Thrown when a failure involving the gripper hardware occurs.\n\n- **GripperClosedCompletely**  \n  Thrown when the gripper closes completely, despite not being expected to do so (e.g., because it should have grasped something).\n\n- **GripperGoalNotReached**  \n  Thrown when the gripper does not reach its goal.\n\n- **LookingHighLevelFailure**  \n  High-level failure produced when looking for an object, i.e., it is not a hardware issue but one relating to the looking task, its parameters, and how they relate to the environment.\n\n- **ManipulationGoalInCollision**  \n  Thrown when executing a manipulation action results in a collision.\n\n- **ManipulationGoalNotReached**  \n  Thrown when, after executing the action, the goal is still not reached.\n\n- **IKError**  \n  Thrown when no inverse kinematics solution could be found.\n\n- **ManipulationPoseUnreachable**  \n  Thrown when no IK solution can be found.\n\n- **NavigationHighLevelFailure**  \n  High-level failure produced while navigating the robot, i.e., it is not a hardware issue but one relating to the navigation task, its parameters, and how they relate to the environment.\n\n- **NavigationGoalInCollision**  \n  Navigation goal cannot be reached because the goal itself is already occupied by some other object.\n\n- **NavigationLowLevelFailure**  \n  Low-level failure produced while navigating the robot, i.e., some kind of hardware issue.\n\n- **NavigationGoalNotReached**  \n  Thrown when the base moved as a result of the navigation action, but the goal was not reached.\n\n- **NavigationPoseUnreachable**  \n  Thrown when the goal pose for navigation is computed to be unreachable.\n\n- **ObjectNowhereToBeFound**  \n  Thrown when the robot cannot find an object of a given description in its surroundings.\n\n- **ObjectUndeliverable**  \n  Thrown when no base positioning can assure a reachable pose to place the object from.\n\n- **ObjectUnfetchable**  \n  Thrown when no base positioning can assure a reachable pose to grasp the object from.\n\n- **ObjectUnreachable**  \n  Thrown when no IK solution is found for a particular base pose.\n\n- **PerceptionLowLevelFailure**  \n  Low-level failure produced while perceiving, i.e., some kind of hardware issue.\n\n- **PerceptionObjectNotFound**  \n  Thrown when an attempt to find an object by perception fails -- and this can still be interpreted as the robot not looking in the right direction, as opposed to the object being absent.\n\n- **PerceptionObjectNotInWorld**  \n  Thrown when an attempt to find an object by perception fails -- and this is because the object can be assumed absent or perhaps is known to be absent due to the setup of a simulation.\n\n- **SearchingFailed**  \n  Thrown when a searching plan completely gives up.\n\n- **TorsoLowLevelFailure**  \n  Low-level failure produced while moving the torso, i.e., some kind of hardware issue.\n\n- **TorsoGoalNotReached**  \n  Thrown when the torso moved as a result of a torso action, but the goal was not reached.\n\n- **TorsoGoalUnreachable**  \n  Thrown when the goal for the torso is computed to be unreachable.\n\n- **Task**  \n  Implementation of plan failures.\n\n- **Grasping**  \n\n- **Looking**  \n\n- **ObjectPoseMisestimation**  \n  Implementation of plan failures.\n\n- **SuccessfulCompletion**  \n  Implementation of plan failures.\n\n- **ObjectNotFound**  \n  Implementation of plan failures.\n\n- **LocomotorFailure**  \n  Implementation of plan failures.\n\n- **ArmFailure**  \n  Implementation of plan failures.\n\n- **ObjectLost**  \n  Implementation of plan failures.\n\n- **SensorFailure**  \n  Implementation of plan failures.\n\n- **IllPosedGoalFailure**  \n  Implementation of plan failures.\n\n- **CapabilityAbsenceFailure**  \n  Implementation of plan failures.\n\n- **ReachabilityFailure**  \n  Implementation of plan failures.\n\n- **TorsoFailure**  \n  Implementation of plan failures.\n\n- **ConfigurationNotReached**  \n  Implementation of plan failures.\n\n- **Timeout**  \n  Implementation of plan failures.\n\n- **EndEffectorFailure**  \n  Implementation of plan failures.\n\n- **ObjectUnavailable**  \n  Implementation of plan failures.\n\n- **SustainedFailure**  \n  Implementation of plan failures.\n\n- **ReasoningError**  \n  Implementation of plan failures.\n\n- **CollisionError**  \n  Implementation of plan failures.\n\n**Module Contents**  \n\n### **py:exception:: PlanFailure(*args, **kwargs)**  \nBases: `Exception`\n\nImplementation of plan failures.\n\n### **py:exception:: NotALanguageExpression(*args, **kwargs)**  \nBases: `PlanFailure`\n\nImplementation of plan failures.\n\n### **py:exception:: FailureDiagnosis(*args, **kwargs)**  \nBases: `PlanFailure`\n\nImplementation of plan failures.\n\n### **py:exception:: LowLevelFailure(*args, **kwargs)**  \nBases: `FailureDiagnosis`\n\nFailure thrown by low-level modules: robot or projection PMs.\n\n### **py:exception:: ActionlibActionTimeout(*args, **kwargs)**  \nBases: `LowLevelFailure`\n\nFailure thrown by low-level modules: robot or projection PMs.\n\n### **py:exception:: HighLevelFailure(*args, **kwargs)**  \nBases: `FailureDiagnosis`\n\nFailure thrown by high-level modules, i.e., plans.\n\n### **py:exception:: DeliveringFailed(*args, **kwargs)**  \nBases: `HighLevelFailure`\n\nThrown when a delivering plan completely gives up.\n\n### **py:exception:: ManipulationLowLevelFailure(*args, **kwargs)**  \nBases: `LowLevelFailure`\n\nThrown when a low-level, i.e., hardware-related, failure is detected in a manipulation action.\n\n### **py:exception:: EnvironmentManipulationGoalNotReached(*args, **kwargs)**  \nBases: `ManipulationLowLevelFailure`\n\nThrown when the door/drawer opening/closing goal is still not reached.\n\n### **py:exception:: EnvironmentManipulationImpossible(*args, **kwargs)**  \nBases: `HighLevelFailure`\n\nThrown when environment manipulation cannot be achieved.\n\n### **py:exception:: EnvironmentUnreachable(*args, **kwargs)**  \nBases: `HighLevelFailure`\n\nThrown when environment manipulation is in collision or unreachable.\n\n### **py:exception:: FetchingFailed(*args, **kwargs)**  \nBases: `HighLevelFailure`\n\nThrown when a fetching plan completely gives up.\n\n### **py:exception:: GripperLowLevelFailure(*args, **kwargs)**  \nBases: `LowLevelFailure`\n\nThrown when a failure involving the gripper hardware occurs.\n\n### **py:exception:: GripperClosedCompletely(*args, **kwargs)**  \nBases: `GripperLowLevelFailure`\n\nThrown when the gripper closes completely, despite not being expected to do so (e.g., because it should have grasped something).\n\n### **py:exception:: GripperGoalNotReached(*args, **kwargs)**  \nBases: `GripperLowLevelFailure`\n\nThrown when the gripper does not reach its goal.\n\n### **py:exception:: LookingHighLevelFailure(*args, **kwargs)**  \nBases: `HighLevelFailure`\n\nHigh-level failure produced when looking for an object, i.e., it is not a hardware issue but one relating to the looking task, its parameters, and how they relate to the environment.\n\n### **py:exception:: ManipulationGoalInCollision(*args, **kwargs)**  \nBases: `HighLevelFailure`\n\nThrown when executing a manipulation action results in a collision.\n\n### **py:exception:: ManipulationGoalNotReached(*args, **kwargs)**  \nBases: `ManipulationLowLevelFailure`\n\nThrown when, after executing the action, the goal is still not reached.\n\n### **py:exception:: IKError(pose, base_frame, tip_frame)**  \nBases: `PlanFailure`\n\nThrown when no inverse kinematics solution could be found.\n- **Attributes:**\n  - **message**\n\n### **py:exception:: ManipulationPoseUnreachable(*args, **kwargs)**  \nBases: `ManipulationLowLevelFailure`\n\nThrown when no IK solution can be found.\n\n### **py:exception:: NavigationHighLevelFailure(*args, **kwargs)**  \nBases: `HighLevelFailure`\n\nHigh-level failure produced while navigating the robot, i.e., it is not a hardware issue but one relating to the navigation task, its parameters, and how they relate to the environment.\n\n### **py:exception:: Navigation\n\nGoalInCollision(*args, **kwargs)**  \nBases: `NavigationHighLevelFailure`\n\nNavigation goal cannot be reached because the goal itself is already occupied by some other object.\n\n### **py:exception:: NavigationLowLevelFailure(*args, **kwargs)**  \nBases: `LowLevelFailure`\n\nLow-level failure produced while navigating the robot, i.e., some kind of hardware issue.\n\n### **py:exception:: NavigationGoalNotReached(*args, **kwargs)**  \nBases: `NavigationLowLevelFailure`\n\nThrown when the base moved as a result of the navigation action, but the goal was not reached.\n\n### **py:exception:: NavigationPoseUnreachable(*args, **kwargs)**  \nBases: `NavigationLowLevelFailure`\n\nThrown when the goal pose for navigation is computed to be unreachable.\n\n### **py:exception:: ObjectNowhereToBeFound(*args, **kwargs)**  \nBases: `HighLevelFailure`\n\nThrown when the robot cannot find an object of a given description in its surroundings.\n\n### **py:exception:: ObjectUndeliverable(*args, **kwargs)**  \nBases: `HighLevelFailure`\n\nThrown when no base positioning can assure a reachable pose to place the object from.\n\n### **py:exception:: ObjectUnfetchable(*args, **kwargs)**  \nBases: `HighLevelFailure`\n\nThrown when no base positioning can assure a reachable pose to grasp the object from.\n\n### **py:exception:: ObjectUnreachable(*args, **kwargs)**  \nBases: `HighLevelFailure`\n\nThrown when no IK solution is found for a particular base pose.\n\n### **py:exception:: PerceptionLowLevelFailure(*args, **kwargs)**  \nBases: `LowLevelFailure`\n\nLow-level failure produced while perceiving, i.e., some kind of hardware issue.\n\n### **py:exception:: PerceptionObjectNotFound(*args, **kwargs)**  \nBases: `PerceptionLowLevelFailure`\n\nThrown when an attempt to find an object by perception fails -- and this can still be interpreted as the robot not looking in the right direction, as opposed to the object being absent.\n\n### **py:exception:: PerceptionObjectNotInWorld(*args, **kwargs)**  \nBases: `PerceptionLowLevelFailure`\n\nThrown when an attempt to find an object by perception fails -- and this is because the object can be assumed absent or perhaps is known to be absent due to the setup of a simulation.\n\n### **py:exception:: SearchingFailed(*args, **kwargs)**  \nBases: `HighLevelFailure`\n\nThrown when a searching plan completely gives up.\n\n### **py:exception:: TorsoLowLevelFailure(*args, **kwargs)**  \nBases: `LowLevelFailure`\n\nLow-level failure produced while moving the torso, i.e., some kind of hardware issue.\n\n### **py:exception:: TorsoGoalNotReached(*args, **kwargs)**  \nBases: `TorsoLowLevelFailure`\n\nThrown when the torso moved as a result of a torso action, but the goal was not reached.\n\n### **py:exception:: TorsoGoalUnreachable(*args, **kwargs)**  \nBases: `TorsoLowLevelFailure`\n\nThrown when the goal for the torso is computed to be unreachable.\n\n### **py:exception:: Task(*args, **kwargs)**  \nBases: `PlanFailure`\n\nImplementation of plan failures.\n\n### **py:exception:: Grasping(*args, **kwargs)**  \nBases: `Task`\n\nImplementation of plan failures.\n\n### **py:exception:: Looking(*args, **kwargs)**  \nBases: `Task`\n\nImplementation of plan failures.\n\n### **py:exception:: ObjectPoseMisestimation(*args, **kwargs)**  \nBases: `PlanFailure`\n\nImplementation of plan failures.\n\n### **py:exception:: SuccessfulCompletion(*args, **kwargs)**  \nBases: `PlanFailure`\n\nImplementation of plan failures.\n\n### **py:exception:: ObjectNotFound(*args, **kwargs)**  \nBases: `PlanFailure`\n\nImplementation of plan failures.\n\n### **py:exception:: LocomotorFailure(*args, **kwargs)**  \nBases: `PlanFailure`\n\nImplementation of plan failures.\n\n### **py:exception:: ArmFailure(*args, **kwargs)**  \nBases: `PlanFailure`\n\nImplementation of plan failures.\n\n### **py:exception:: ObjectLost(*args, **kwargs)**  \nBases: `PlanFailure`\n\nImplementation of plan failures.\n\n### **py:exception:: SensorFailure(*args, **kwargs)**  \nBases: `PlanFailure`\n\nImplementation of plan failures.\n\n### **py:exception:: IllPosedGoalFailure(*args, **kwargs)**  \nBases: `PlanFailure`\n\nImplementation of plan failures.\n\n### **py:exception:: CapabilityAbsenceFailure(*args, **kwargs)**  \nBases: `PlanFailure`\n\nImplementation of plan failures.\n\n### **py:exception:: ReachabilityFailure(*args, **kwargs)**  \nBases: `PlanFailure`\n\nImplementation of plan failures.\n\n### **py:exception:: TorsoFailure(*args, **kwargs)**  \nBases: `PlanFailure`\n\nImplementation of plan failures.\n\n### **py:exception:: ConfigurationNotReached(*args, **kwargs)**  \nBases: `PlanFailure`\n\nImplementation of plan failures.\n\n### **py:exception:: Timeout(*args, **kwargs)**  \nBases: `PlanFailure`\n\nImplementation of plan failures.\n\n### **py:exception:: EndEffectorFailure(*args, **kwargs)**  \nBases: `PlanFailure`\n\nImplementation of plan failures.\n\n### **py:exception:: ObjectUnavailable(*args, **kwargs)**  \nBases: `PlanFailure`\n\nImplementation of plan failures.\n\n### **py:exception:: SustainedFailure(*args, **kwargs)**  \nBases: `PlanFailure`\n\nImplementation of plan failures.\n\n### **py:exception:: ReasoningError(**kwargs)**  \nBases: `PlanFailure`\n\nImplementation of plan failures.\n\n### **py:exception:: CollisionError(**kwargs)**  \nBases: `PlanFailure`\n\nImplementation of plan failures."}]}
{"messages": [{"role": "system", "content": "You are a coding expert and know how to write PyCRAM Plan Code."}, {"role": "user", "content": "What functionalities are covered in the pycram.world_reasoning section of the PyCRAM documentation?"}, {"role": "assistant", "content": "pycram.world_reasoning  \n\n### **Functions**\n\n- **stable(obj: pycram.world_concepts.world_object.Object)  bool**  \n  Checks if an object is stable in the world. Stability means that its position will not change after simulating physics in the World. This is done by simulating the world for 10 seconds and comparing the previous coordinates with the coordinates after the simulation.\n\n  - **Parameters:**\n    - **obj**: The object to be checked.\n  - **Returns:** True if the object is stable in the world; False otherwise.\n\n- **contact(object1: pycram.world_concepts.world_object.Object, object2: pycram.world_concepts.world_object.Object, return_links: bool = False)  typing_extensions.Union[bool, typing_extensions.Tuple[bool, typing_extensions.List]]**  \n  Checks if two objects are in contact. If `return_links` is True, the output will also contain a list of tuples where the first element is the link name of `object1`, and the second element is the link name of `object2`.\n\n  - **Parameters:**\n    - **object1**: The first object.\n    - **object2**: The second object.\n    - **return_links**: If the respective links on the objects in contact should be returned.\n  - **Returns:** True if the two objects are in contact; False otherwise. If links are returned, a list of links in contact.\n\n- **get_visible_objects(camera_pose: pycram.datastructures.pose.Pose, front_facing_axis: typing_extensions.Optional[typing_extensions.List[float]] = None)  typing_extensions.Tuple[numpy.ndarray, pycram.datastructures.pose.Pose]**  \n  Returns a segmentation mask of the objects visible from the given camera pose and the front-facing axis.\n\n  - **Parameters:**\n    - **camera_pose**: The pose of the camera in the world coordinate frame.\n    - **front_facing_axis**: The axis of the camera frame that faces the front of the robot, given as a list of xyz.\n  - **Returns:** A segmentation mask of the visible objects and the pose of the point exactly 2 meters in front of the camera in the direction of the front-facing axis with respect to the world coordinate frame.\n\n- **visible(obj: pycram.world_concepts.world_object.Object, camera_pose: pycram.datastructures.pose.Pose, front_facing_axis: typing_extensions.Optional[typing_extensions.List[float]] = None, threshold: float = 0.8)  bool**  \n  Checks if an object is visible from a given position. This is achieved by rendering the object alone and counting the visible pixels, then rendering the complete scene and comparing the visible pixels with the absolute count of pixels.\n\n  - **Parameters:**\n    - **obj**: The object for which visibility should be checked.\n    - **camera_pose**: The pose of the camera in the map frame.\n    - **front_facing_axis**: The axis of the camera frame that faces the front of the robot, given as a list of xyz.\n    - **threshold**: The minimum percentage of the object that needs to be visible for this method to return True.\n  - **Returns:** True if the object is visible from the camera position; False otherwise.\n\n- **occluding(obj: pycram.world_concepts.world_object.Object, camera_pose: pycram.datastructures.pose.Pose, front_facing_axis: typing_extensions.Optional[typing_extensions.List[float]] = None)  typing_extensions.List[pycram.world_concepts.world_object.Object]**  \n  Lists all objects that are occluding the given object. This works similarly to the `visible` function. First, the object alone will be rendered and the position of its pixels in the picture will be saved. Afterward, the complete scene will be rendered, and the saved pixel positions will be compared to the actual pixels. If another object is visible in one of the saved pixel positions, it will be marked as occluding.\n\n  - **Parameters:**\n    - **obj**: The object for which occlusion should be checked.\n    - **camera_pose**: The pose of the camera in the world coordinate frame.\n    - **front_facing_axis**: The axis of the camera frame that faces the front of the robot, given as a list of xyz.\n  - **Returns:** A list of occluding objects.\n\n- **reachable(pose_or_object: typing_extensions.Union[pycram.world_concepts.world_object.Object, pycram.datastructures.pose.Pose], robot: pycram.world_concepts.world_object.Object, gripper_name: str, threshold: float = 0.01)  bool**  \n  Checks if the robot can reach a given position. To determine this, inverse kinematics are calculated and applied. Afterward, the distance between the position and the given end effector is calculated. If it is smaller than the threshold, the reasoning query returns True; otherwise, it returns False.\n\n  - **Parameters:**\n    - **pose_or_object**: The position and rotation or object for which reachability should be checked.\n    - **robot**: The robot that should reach for the position.\n    - **gripper_name**: The name of the end effector.\n    - **threshold**: The threshold between the end effector and the position.\n  - **Returns:** True if the end effector is closer than the threshold to the target position; False otherwise.\n\n- **blocking(pose_or_object: typing_extensions.Union[pycram.world_concepts.world_object.Object, pycram.datastructures.pose.Pose], robot: pycram.world_concepts.world_object.Object, gripper_name: str, grasp: str = None)  typing_extensions.Union[typing_extensions.List[pycram.world_concepts.world_object.Object], None]**  \n  Checks if any objects are blocking another object when a robot tries to pick it up. This works similarly to the `reachable` function. First, the inverse kinematics between the robot and the object will be calculated and applied. Then it will be checked if the robot is in contact with any object except the given one. If the given pose or object is not reachable, None will be returned.\n\n  - **Parameters:**\n    - **pose_or_object**: The object or pose for which blocking objects should be found.\n    - **robot**: The robot object that reaches for the object.\n    - **gripper_name**: The name of the end effector of the robot.\n    - **grasp**: The grasp type with which the object should be grasped.\n  - **Returns:** A list of objects the robot is in collision with when reaching for the specified object, or None if the pose or object is not reachable.\n\n- **supporting(object1: pycram.world_concepts.world_object.Object, object2: pycram.world_concepts.world_object.Object)  bool**  \n  Checks if one object is supporting another object. An object supports another object if they are in contact and the second object is above the first one (e.g., a bottle will be supported by a table).\n\n  - **Parameters:**\n    - **object1**: The object that is supported.\n    - **object2**: The object that supports the first object.\n  - **Returns:** True if the second object is in contact with the first one and is above it; False otherwise.\n\n- **link_pose_for_joint_config(obj: pycram.world_concepts.world_object.Object, joint_config: typing_extensions.Dict[str, float], link_name: str)  pycram.datastructures.pose.Pose**  \n  Returns the pose a link would be in if the given joint configuration were applied to the object. This is done by using the respective object in the prospection world and applying the joint configuration to this one. After applying the joint configuration, the link position is taken from there.\n\n  - **Parameters:**\n    - **obj**: The object of which the link is a part.\n    - **joint_config**: Dictionary with the goal joint configuration.\n    - **link_name**: Name of the link for which the pose should be returned.\n  - **Returns:** The pose of the link after applying the joint configuration."}]}
{"messages": [{"role": "system", "content": "You are a coding expert and know how to write PyCRAM Plan Code."}, {"role": "user", "content": "What are the main features of the pycram.language API as described in the PyCRAM documentation?"}, {"role": "assistant", "content": "pycram.language\n\n\n### **Classes**\n\n- **Language**  \n  Parent class for language expressions. Implements operators as well as methods to reduce the resulting language tree.\n\n- **Repeat**  \n  Executes all children a given number of times.\n\n- **Monitor**  \n  Monitors a Language Expression and interrupts it when the given condition is evaluated to True.\n\n- **Sequential**  \n  Executes all children sequentially; an exception while executing a child does not terminate the whole process.\n\n- **TryInOrder**  \n  Executes all children sequentially; an exception while executing a child does not terminate the whole process.\n\n- **Parallel**  \n  Executes all children in parallel by creating a thread for each child and executing them in the respective threads. All exceptions during execution will be caught, saved to a list, and returned upon completion.\n\n- **TryAll**  \n  Executes all children in parallel by creating a thread for each child and executing them in the respective threads. All exceptions during execution will be caught, saved to a list, and returned upon completion.\n\n- **Code**  \n  Executable code block in a plan.\n\n**Module Contents**  \n\n### **Language**  \nBases: `anytree.NodeMixin`\n\nParent class for language expressions. Implements operators as well as methods to reduce the resulting language tree.\n\n- **Attributes:**\n  - **parallel_blocklist**: `['PickUpAction', 'PlaceAction', 'OpenAction', 'CloseAction', 'TransportAction', 'GraspingAction']`\n  - **do_not_use_giskard**: `['SetGripperAction', 'MoveGripperMotion', 'DetectAction', 'DetectingMotion']`\n  - **block_list**: `typing_extensions.List[int]`  \n    List of thread IDs which should be blocked from execution.\n  - **parent**\n  - **exceptions**\n  - **state**: `None`\n  - **executing_thread**\n  - **threads**: `typing_extensions.List[threading.Thread]`\n  - **interrupted**: `False`\n  - **name**\n\n- **Methods:**\n  - **resolve()  Language**  \n    Dummy method for compatibility with designator descriptions.\n    - **Returns:** Self-reference.\n    \n  - **perform()**  \n    This method should be overwritten in subclasses to implement the behavior of the language expression regarding each child.\n    \n  - **__add__(other: Language)  Sequential**  \n    Language expression for sequential execution.\n    - **Parameters:** `other`: Another Language expression.\n    - **Returns:** A `Sequential` object which is the new root node of the language tree.\n    \n  - **__sub__(other: Language)  TryInOrder**  \n    Language expression for trying in order.\n    - **Parameters:** `other`: Another Language expression.\n    - **Returns:** A `TryInOrder` object which is the new root node of the language tree.\n    \n  - **__or__(other: Language)  Parallel**  \n    Language expression for parallel execution.\n    - **Parameters:** `other`: Another Language expression.\n    - **Returns:** A `Parallel` object which is the new root node of the language tree.\n    \n  - **__xor__(other: Language)  TryAll**  \n    Language expression for trying all executions.\n    - **Parameters:** `other`: Another Language expression.\n    - **Returns:** A `TryAll` object which is the new root node of the language tree.\n    \n  - **__rshift__(other: Language)**  \n    Operator for Monitors, making the Monitor the parent of the other expression.\n    - **Parameters:** `other`: Another Language expression.\n    - **Returns:** The Monitor, which is now the new root node.\n    \n  - **__mul__(other: int)**  \n    Language expression for repeated execution. The other attribute of this operator must be an integer.\n    - **Parameters:** `other`: An integer stating how often the Language expression should be repeated.\n    - **Returns:** A `Repeat` object which is the new root node of the language tree.\n    \n  - **__rmul__(other: int)**  \n    Language expression for repeated execution. The other attribute of this operator must be an integer. This is the reversed operator of `__mul__`, allowing you to write:\n    \n    ```python\n    2 * ParkAction()\n    ```\n    - **Parameters:** `other`: An integer stating how often the Language expression should be repeated.\n    - **Returns:** A `Repeat` object which is the new root node of the language tree.\n    \n  - **simplify()  Language**  \n    Simplifies the language tree by merging nodes that have a parent-child relation and are of the same type.\n    \n  - **merge_nodes(node1: anytree.Node, node2: anytree.Node)  None**  \n    Merges `node1` with `node2` in a tree. The children of `node2` will be re-parented to `node1`, and `node2` will be deleted from the tree.\n    - **Parameters:** \n      - **node1**: Node that should be left in the tree.\n      - **node2**: Node whose children should be appended to `node1` and then deleted.\n    \n  - **interrupt()  None**  \n    Base method for interrupting the execution of a Language expression. To be overwritten in a sub-class.\n\n### **Repeat**  \nBases: `Language`\n\nExecutes all children a given number of times.\n\n- **Attributes:**\n  - **repeat**: `int`\n\n- **Methods:**\n  - **perform()**  \n    Executes all children in a loop as often as stated on initialization.\n    \n  - **interrupt()  None**  \n    Stops the execution of this language expression by setting the `interrupted` variable to True, adding this thread to the block_list in ProcessModule, and interrupting the current Giskard goal.\n\n### **Monitor**  \nBases: `Language`\n\nMonitors a Language Expression and interrupts it when the given condition is evaluated to True.\n\n- **Attributes:**\n  - **kill_event**\n  - **exception_queue**\n\n- **Methods:**\n  - **perform()  typing_extensions.Tuple[pycram.datastructures.enums.State, typing_extensions.Any]**  \n    Starts a new thread that checks the condition and then performs the attached language expression.\n    - **Returns:** The state of the attached language expression, and a list of the results of the children.\n    \n  - **interrupt()  None**  \n    Calls interrupt for each child.\n\n### **Sequential**  \nBases: `Language`\n\nExecutes all children sequentially; an exception while executing a child does not terminate the whole process.\n\n- **Methods:**\n  - **perform()  typing_extensions.Tuple[pycram.datastructures.enums.State, typing_extensions.List[typing_extensions.Any]]**  \n    Calls `perform()` on each child sequentially.\n    - **Returns:** The state and list of results.\n    \n  - **interrupt()  None**  \n    Interrupts the execution of this language expression by setting the `interrupted` variable to True and calling interrupt on the current Giskard goal.\n\n### **TryInOrder**  \nBases: `Language`\n\nExecutes all children sequentially; an exception while executing a child does not terminate the whole process.\n\n- **Methods:**\n  - **perform()  typing_extensions.Tuple[pycram.datastructures.enums.State, typing_extensions.List[typing_extensions.Any]]**  \n    Calls `perform()` on each child sequentially and catches raised exceptions.\n    - **Returns:** The state and list of results.\n    \n  - **interrupt()  None**  \n    Interrupts the execution of this language expression by setting the `interrupted` variable to True, adding the current thread to the block_list in Language, and interrupting the current Giskard goal.\n\n### **Parallel**  \nBases: `Language`\n\nExecutes all children in parallel by creating a thread for each child and executing them in the respective threads.\n\n- **Methods:**\n  - **perform()  typing_extensions.Tuple[pycram.datastructures.enums.State, typing_extensions.List[typing_extensions.Any]]**  \n    Creates a new thread for each child and calls `perform()` of the child in the respective thread.\n    - **Returns:** The state and list of results.\n    \n  - **interrupt()  None**  \n    Interrupts the execution of this language expression by setting the `interrupted` variable to True, adding the thread ID of all parallel execution threads to the block_list in Language, and interrupting the current Giskard goal.\n\n### **TryAll**  \nBases: `Language`\n\nExecutes all children in parallel by creating a thread for each child and executing them in the respective threads.\n\n- **Methods:**\n  - **perform()  typing_extensions.Tuple[pycram.datastructures.enums.State, typing_extensions.List[typing_extensions.Any]]**  \n    Creates a new thread for each child and executes all children in their respective threads.\n    - **Returns:** The state and list of results.\n    \n  - **interrupt()  None**  \n    Interrupts the execution of this language expression by setting the `interrupted` variable to True, adding the thread ID of all parallel execution threads to the block_list in Language, and interrupting the current Giskard goal.\n\n### **Code**  \nBases: `Language`\n\nExecutable code block in a plan.\n\n- **Attributes:**\n  - **function**: `typing_extensions.Callable`  \n  - **kwargs**: `typing_extensions.Dict[str, typing_extensions.Any]`\n\n- **Methods:**\n  - **perform**  \n    This method should be overwritten in subclasses to implement the behavior of the language expression regarding each child.\n    \n  - **execute()  typing_extensions.Any**  \n    Executes the code with its arguments.\n   \n\n - **Returns:** `State.SUCCEEDED`, and anything that the function associated with this object returns.\n    \n  - **interrupt()  None**  \n    Base method for interrupting the execution of a Language expression. To be overwritten in a sub-class."}]}
{"messages": [{"role": "system", "content": "You are a coding expert and know how to write PyCRAM Plan Code."}, {"role": "user", "content": "What functionalities does the pose generator and validator provide in the PyCRAM documentation?"}, {"role": "assistant", "content": "pycram.pose_generator_and_validator  \n========================================\n\n### **Classes**\n\n- **PoseGenerator**  \n  Creates pose candidates from a given costmap. The generator selects the highest values (amount specified by `number_of_samples`) and returns the corresponding positions. Orientations are calculated such that the robot faces the center of the costmap.\n\n### **Functions**\n\n- **visibility_validator( bool)**  \n  Validates if the robot can see the target position from a given pose candidate.\n\n- **_in_contact( bool)**  \n  Checks if a given robot is in contact with a given object.\n\n- **reachability_validator( typing_extensions.Tuple[bool, ...])**  \n  Validates if a target position is reachable for a pose candidate.\n\n- **collision_check(robot, allowed_collision)**  \n  Checks if a given robot collides with any object within the world that it is not allowed to collide with.\n\n**Module Contents**  \n\n### **PoseGenerator**  \nBases: `pycram.costmaps.Costmap`\n\nCreates pose candidates from a given costmap. The generator selects the highest values, the amount is given by `number_of_samples`, and returns the corresponding positions. Orientations are calculated such that the robot faces the center of the costmap.\n\n- **Attributes:**\n  - **current_orientation_generator**: `None`  \n    If no orientation generator is given, this generator is used to generate the orientation of the robot.\n  - **override_orientation_generator**: `None`  \n    Overrides the orientation generator with a custom generator, which will be used regardless of the `current_orientation_generator`.\n  - **costmap**\n  - **number_of_samples**\n  - **orientation_generator**\n\n- **Methods:**\n  - **__iter__()  typing_extensions.Iterable**  \n    A generator that creates pose candidates from a given costmap. The generator selects the highest 100 values and returns the corresponding positions. Orientations are calculated such that the robot faces the center of the costmap.\n    - **Yield:** A tuple of position and orientation.\n    \n  - **height_generator()  float**  \n    - **Static Method**\n    \n  - **generate_orientation(position: typing_extensions.List[float], origin: pycram.datastructures.pose.Pose)  typing_extensions.List[float]**  \n    Generates the orientation for a given position in a costmap. The orientation is calculated such that the robot faces the origin of the costmap. This is done by calculating the arctan between the position in the costmap and the origin of the costmap.\n    - **Parameters:**\n      - **position:** The position in the costmap (already converted to the world coordinate frame).\n      - **origin:** The origin of the costmap (the point which the robot should face).\n    - **Returns:** A quaternion of the calculated orientation.\n\n### **visibility_validator**  \nValidates if the robot can see the target position from a given pose candidate. The target position can either be a position in the world coordinate system or an object in the World. The validation is done by shooting a ray from the camera to the target position and checking that it does not collide with anything else.\n\n- **Parameters:**\n  - **pose:** The pose candidate to be validated.\n  - **robot:** The robot object for which this should be validated.\n  - **object_or_pose:** The target position or object for which the pose candidate should be validated.\n  - **world:** The World instance in which this should be validated.\n- **Returns:** `True` if the target is visible for the robot, otherwise `None`.\n\n### **_in_contact**  \nChecks if a given robot is in contact with a given object.\n\n- **Parameters:**\n  - **robot:** The robot object that should be checked for contact.\n  - **obj:** The object that should be checked for contact with the robot.\n  - **allowed_collision:** A dictionary containing the allowed collisions for links of each object in the world.\n  - **allowed_robot_links:** A list of links of the robot that are allowed to be in contact with the object.\n- **Returns:** `True` if the robot is in contact with the object, otherwise `False`.\n\n### **reachability_validator**  \nValidates if a target position is reachable for a pose candidate. This is done by asking the IK solver if there is a valid solution if the robot stands at the position of the pose candidate. If there is a solution, the validator returns `True`, otherwise `False`.\n\n- **Parameters:**\n  - **pose:** The pose candidate for which reachability should be validated.\n  - **robot:** The robot object in the World for which reachability should be validated.\n  - **target:** The target position or object instance that should be the target for reachability.\n  - **allowed_collision:** A dictionary of objects with which the robot is allowed to collide. Each object correlates to a list of links of which this object consists.\n- **Returns:** A tuple containing `True` if the target is reachable for the robot, otherwise `False`, and a list of results.\n\n### **collision_check**  \nChecks if a given robot collides with any object within the world that it is not allowed to collide with. This is done by iterating over every object within the world and checking if the robot collides with it. Note that the floor will be ignored. If there is a collision with an object not within the allowed collision list, the function returns `True`, otherwise `False`.\n\n- **Parameters:**\n  - **robot:** The robot object in the (Bullet)World where it should be checked for collisions.\n  - **allowed_collision:** A dictionary of objects with which the robot is allowed to collide. Each object correlates to a list of links of which this object consists.\n- **Returns:** `True` if the target is reachable for the robot, otherwise `False`."}]}
{"messages": [{"role": "system", "content": "You are a coding expert and know how to write PyCRAM Plan Code."}, {"role": "user", "content": "What information can I find about the PR2 robot in the PyCRAM documentation?"}, {"role": "assistant", "content": "pycram.robot_descriptions.pr2_description  \n==============================================\n\n### **Attributes**\n\n- **rospack**  \n- **filename**  \n- **pr2_description**  \n- **left_arm**  \n- **left_gripper**  \n- **right_arm**  \n- **right_gripper**  \n- **camera**  \n- **rdm**  \n\n**Module Contents**  \n\n- **pycram.robot_descriptions.pr2_description.rospack**  \n- **pycram.robot_descriptions.pr2_description.filename**  \n- **pycram.robot_descriptions.pr2_description.pr2_description**  \n- **pycram.robot_descriptions.pr2_description.left_arm**  \n- **pycram.robot_descriptions.pr2_description.left_gripper**  \n- **pycram.robot_descriptions.pr2_description.right_arm**  \n- **pycram.robot_descriptions.pr2_description.right_gripper**  \n- **pycram.robot_descriptions.pr2_description.camera**  \n- **pycram.robot_descriptions.pr2_description.rdm**"}]}
{"messages": [{"role": "system", "content": "You are a coding expert and know how to write PyCRAM Plan Code."}, {"role": "user", "content": "What information can I find about the generic object descriptors in the PyCRAM documentation?"}, {"role": "assistant", "content": "pycram.object_descriptors.generic \n======================================\n\n### **Classes**\n\n- **LinkDescription**  \n  A class that represents a link description of an object.\n\n- **JointDescription**  \n  A class that represents the description of a joint.\n\n- **ObjectDescription**  \n  A generic description of an object in the environment. This description can be applied to any object.\n\n**Module Contents**  \n\n### **LinkDescription**  \nBases: `pycram.description.LinkDescription`\n\nA class that represents a link description of an object.\n\n- **Attributes:**\n  - **parsed_description**: `pycram.datastructures.dataclasses.BoxVisualShape`\n  - **_name**: `str`\n\n- **Properties:**\n  - **geometry**: `typing_extensions.Union[pycram.datastructures.dataclasses.VisualShape, None]`  \n    Returns the geometry type of the collision element of this link.\n  - **origin**: `pycram.datastructures.pose.Pose`  \n    Returns the origin of this entity.\n  - **name**: `str`  \n    Returns the name of this entity.\n  - **color**: `pycram.datastructures.dataclasses.Color`\n\n### **JointDescription**  \nBases: `pycram.description.JointDescription`\n\nA class that represents the description of a joint.\n\n- **Properties:**\n  - **type**: `pycram.datastructures.enums.JointType`  \n    Returns the type of this joint.\n  - **axis**: `geometry_msgs.msg.Point`  \n    Returns the axis of this joint, for example, the rotation axis for a revolute joint.\n  - **has_limits**: `bool`  \n    Checks if this joint has limits.  \n    - **Returns:** `True` if the joint has limits, `False` otherwise.\n  - **lower_limit**: `typing_extensions.Union[float, None]`  \n    Returns the lower limit of this joint, or `None` if the joint has no limits.\n  - **upper_limit**: `typing_extensions.Union[float, None]`  \n    Returns the upper limit of this joint, or `None` if the joint has no limits.\n  - **parent_link_name**: `str`  \n    Returns the name of the parent link of this joint.\n  - **child_link_name**: `str`  \n    Returns the name of the child link of this joint.\n  - **origin**: `pycram.datastructures.pose.Pose`  \n    Returns the origin of this entity.\n  - **name**: `str`  \n    Returns the name of this entity.\n\n### **ObjectDescription**  \nBases: `pycram.description.ObjectDescription`\n\nA generic description of an object in the environment. This description can be applied to any object. The current use case involves perceiving objects using RoboKudo and spawning them with specified size and color.\n\n- **Classes:**\n  - **Link**  \n    Bases: `pycram.description.ObjectDescription.Link`, `LinkDescription`  \n    Represents a link of an Object in the World.\n  - **RootLink**  \n    Bases: `pycram.description.ObjectDescription.RootLink`, `Link`  \n    Represents the root link of an Object in the World. It differs from the normal `AbstractLink` class in that the pose and the `tf_frame` is the same as that of the object.\n  - **Joint**  \n    Bases: `pycram.description.ObjectDescription.Joint`, `JointDescription`  \n    Represents a joint of an Object in the World.\n\n- **Attributes:**\n  - **_links**\n\n- **Methods:**\n  - **load_description(path: str)  typing_extensions.Any**  \n    Loads the description from the file at the given path.  \n    - **Parameters:**  \n      - **path:** The path to the source file. If only a filename is provided, the resources directories will be searched.\n  - **generate_from_mesh_file(path: str, name: str)  str**  \n    - **Class Method**  \n    Generates a description file from one of the mesh types defined in the mesh extensions and returns the path of the generated file.  \n    - **Parameters:**  \n      - **path:** The path to the .obj file.  \n      - **name:** The name of the object.  \n    - **Returns:** The path of the generated description file.\n  - **generate_from_description_file(path: str)  str**  \n    - **Class Method**  \n    Preprocesses the given file and returns the preprocessed description string.  \n    - **Parameters:**  \n      - **path:** The path of the file to preprocess.  \n    - **Returns:** The preprocessed description string.\n  - **generate_from_parameter_server(name: str)  str**  \n    - **Class Method**  \n    Preprocesses the description from the ROS parameter server and returns the preprocessed description string.  \n    - **Parameters:**  \n      - **name:** The name of the description on the parameter server.  \n    - **Returns:** The preprocessed description string.\n  - **get_link_by_name(link_name: str)  LinkDescription**  \n    Returns the link description with the given name.\n  - **get_joint_by_name(joint_name: str)  JointDescription**  \n    Returns the joint description with the given name.\n  - **get_root()  str**  \n    Returns the name of the root link of this object.\n  - **get_chain(start_link_name: str, end_link_name: str)  typing_extensions.List[str]**  \n    Returns the chain of links from `start_link_name` to `end_link_name`.\n\n- **Properties:**\n  - **shape_data**: `typing_extensions.List[float]`\n  - **color**: `pycram.datastructures.dataclasses.Color`\n  - **links**: `typing_extensions.List[LinkDescription]`  \n    Returns a list of link descriptions of this object.\n  - **joints**: `typing_extensions.List[JointDescription]`  \n    Returns a list of joint descriptions of this object.\n  - **origin**: `pycram.datastructures.pose.Pose`  \n    Returns the origin of this entity.\n  - **name**: `str`  \n    Returns the name of this entity."}]}
{"messages": [{"role": "system", "content": "You are a coding expert and know how to write PyCRAM Plan Code."}, {"role": "user", "content": "What are the key features of the dataclasses in the pycram.datastructures module of the PyCRAM documentation?"}, {"role": "assistant", "content": "pycram.datastructures.dataclasses  \n======================================\n\n### **Classes**\n\n- **Color**  \n  Dataclass for storing rgba_color as an RGBA value.\n\n- **AxisAlignedBoundingBox**  \n  Dataclass for storing an axis-aligned bounding box.\n\n- **CollisionCallbacks**  \n\n- **MultiBody**  \n\n- **VisualShape**  \n  Helper class that provides a standard way to create an ABC using inheritance.\n\n- **BoxVisualShape**  \n  Helper class that provides a standard way to create an ABC using inheritance.\n\n- **SphereVisualShape**  \n  Helper class that provides a standard way to create an ABC using inheritance.\n\n- **CapsuleVisualShape**  \n  Helper class that provides a standard way to create an ABC using inheritance.\n\n- **CylinderVisualShape**  \n  Helper class that provides a standard way to create an ABC using inheritance.\n\n- **MeshVisualShape**  \n  Helper class that provides a standard way to create an ABC using inheritance.\n\n- **PlaneVisualShape**  \n  Helper class that provides a standard way to create an ABC using inheritance.\n\n- **State**  \n  Helper class that provides a standard way to create an ABC using inheritance.\n\n- **LinkState**  \n  Helper class that provides a standard way to create an ABC using inheritance.\n\n- **JointState**  \n  Helper class that provides a standard way to create an ABC using inheritance.\n\n- **ObjectState**  \n  Helper class that provides a standard way to create an ABC using inheritance.\n\n- **WorldState**  \n  Helper class that provides a standard way to create an ABC using inheritance.\n\n**Module Contents**  \n\n### **Functions**\n\n#### `get_point_as_list(point: pycram.datastructures.pose.Point) -> typing_extensions.List[float]`\n   Returns the point as a list.\n   - **Parameters:**\n     - `point`: The point.\n   - **Returns:**  \n     The point as a list.\n\n### **Class Details**\n\n#### **Color**\n   Dataclass for storing rgba_color as an RGBA value. The values are stored as floats between 0 and 1. The default rgba_color is white. 'A' stands for the opacity.\n\n- **Attributes:**\n  - `R`: `float = 1`\n  - `G`: `float = 1`\n  - `B`: `float = 1`\n  - `A`: `float = 1`\n\n- **Methods:**\n  - `from_list(color: typing_extensions.List[float])`: Sets the rgba_color from a list of RGBA values.\n  - `from_rgb(rgb: typing_extensions.List[float])`: Sets the rgba_color from a list of RGB values.\n  - `from_rgba(rgba: typing_extensions.List[float])`: Sets the rgba_color from a list of RGBA values.\n  - `get_rgba() -> typing_extensions.List[float]`: Returns the rgba_color as a list of RGBA values.\n  - `get_rgb() -> typing_extensions.List[float]`: Returns the rgba_color as a list of RGB values.\n\n#### **AxisAlignedBoundingBox**\n   Dataclass for storing an axis-aligned bounding box.\n\n- **Attributes:**\n  - `min_x`: `float`\n  - `min_y`: `float`\n  - `min_z`: `float`\n  - `max_x`: `float`\n  - `max_y`: `float`\n  - `max_z`: `float`\n\n- **Methods:**\n  - `from_min_max(min_point: typing_extensions.List[float], max_point: typing_extensions.List[float])`: Sets the axis-aligned bounding box from a minimum and maximum point.\n  - `get_min_max_points() -> typing_extensions.Tuple[pycram.datastructures.pose.Point, pycram.datastructures.pose.Point]`: Returns the axis-aligned bounding box as a tuple of minimum and maximum points.\n  - `get_min_point() -> pycram.datastructures.pose.Point`: Returns the axis-aligned bounding box as a minimum point.\n  - `get_max_point() -> pycram.datastructures.pose.Point`: Returns the axis-aligned bounding box as a maximum point.\n  - `get_min_max() -> typing_extensions.Tuple[typing_extensions.List[float], typing_extensions.List[float]]`: Returns the axis-aligned bounding box as a tuple of minimum and maximum points.\n  - `get_min() -> typing_extensions.List[float]`: Returns the minimum point of the axis-aligned bounding box.\n  - `get_max() -> typing_extensions.List[float]`: Returns the maximum point of the axis-aligned bounding box.\n\n#### **CollisionCallbacks**\n- **Attributes:**\n  - `on_collision_cb`: `typing_extensions.Callable`\n  - `no_collision_cb`: `typing_extensions.Optional[typing_extensions.Callable] = None`\n\n#### **MultiBody**\n- **Attributes:**\n  - `base_visual_shape_index`: `int`\n  - `base_pose`: `pycram.datastructures.pose.Pose`\n  - `link_visual_shape_indices`: `typing_extensions.List[int]`\n  - `link_poses`: `typing_extensions.List[pycram.datastructures.pose.Pose]`\n  - `link_masses`: `typing_extensions.List[float]`\n  - `link_inertial_frame_poses`: `typing_extensions.List[pycram.datastructures.pose.Pose]`\n  - `link_parent_indices`: `typing_extensions.List[int]`\n  - `link_joint_types`: `typing_extensions.List[pycram.datastructures.enums.JointType]`\n  - `link_joint_axis`: `typing_extensions.List[pycram.datastructures.pose.Point]`\n  - `link_collision_shape_indices`: `typing_extensions.List[int]`\n\n#### **VisualShape**\n   Bases: `abc.ABC`\n   Helper class that provides a standard way to create an ABC using inheritance.\n\n- **Attributes:**\n  - `rgba_color`: `Color`\n  - `visual_frame_position`: `typing_extensions.List[float]`\n\n- **Methods:**\n  - `shape_data() -> typing_extensions.Dict[str, typing_extensions.Any]`: Abstract method to return the shape data of the visual shape (e.g. half extents for a box, radius for a sphere).\n  - **Properties:**\n    - `visual_geometry_type`: `pycram.datastructures.enums.Shape`: Abstract property to return the visual geometry type of the visual shape (e.g. box, sphere).\n\n#### **BoxVisualShape**\n   Bases: `VisualShape`\n   Helper class that provides a standard way to create an ABC using inheritance.\n\n- **Attributes:**\n  - `half_extents`: `typing_extensions.List[float]`\n\n- **Methods:**\n  - `shape_data() -> typing_extensions.Dict[str, typing_extensions.List[float]]`: Returns the shape data of the visual shape (e.g. half extents for a box, radius for a sphere).\n  - **Properties:**\n    - `visual_geometry_type`: `pycram.datastructures.enums.Shape`: Returns the visual geometry type of the visual shape (e.g. box, sphere).\n    - `size`: `typing_extensions.List[float]`\n\n#### **SphereVisualShape**\n   Bases: `VisualShape`\n   Helper class that provides a standard way to create an ABC using inheritance.\n\n- **Attributes:**\n  - `radius`: `float`\n\n- **Methods:**\n  - `shape_data() -> typing_extensions.Dict[str, float]`: Returns the shape data of the visual shape (e.g. radius for a sphere).\n  - **Properties:**\n    - `visual_geometry_type`: `pycram.datastructures.enums.Shape`: Returns the visual geometry type of the visual shape (e.g. sphere).\n\n#### **CapsuleVisualShape**\n   Bases: `VisualShape`\n   Helper class that provides a standard way to create an ABC using inheritance.\n\n- **Attributes:**\n  - `radius`: `float`\n  - `length`: `float`\n\n- **Methods:**\n  - `shape_data() -> typing_extensions.Dict[str, float]`: Returns the shape data of the visual shape (e.g. radius and length for a capsule).\n  - **Properties:**\n    - `visual_geometry_type`: `pycram.datastructures.enums.Shape`: Returns the visual geometry type of the visual shape (e.g. capsule).\n\n#### **CylinderVisualShape**\n   Bases: `CapsuleVisualShape`\n   Helper class that provides a standard way to create an ABC using inheritance.\n\n- **Properties:**\n  - `visual_geometry_type`: `pycram.datastructures.enums.Shape`: Returns the visual geometry type of the visual shape (e.g. cylinder).\n\n#### **MeshVisualShape**\n   Bases: `VisualShape`\n   Helper class that provides a standard way to create an ABC using inheritance.\n\n- **Attributes:**\n  - `scale`: `typing_extensions.List[float]`\n  - `file_name`: `str`\n\n- **Methods:**\n  - `shape_data() -> typing_extensions.Dict[str, typing_extensions.Union[typing_extensions.List[float], str]]`: Returns the shape data of the visual shape (e.g. scale and file name for a mesh).\n  - **Properties:**\n    - `visual_geometry_type`: `pycram.datastructures.enums.Shape`: Returns the visual geometry type of the visual shape (e.g. mesh).\n\n#### **PlaneVisualShape**\n   Bases: `VisualShape`\n   Helper class that provides a standard way to create an ABC using inheritance.\n\n- **Attributes:**\n  - `normal`: `typing_extensions.List[float]`\n\n- **Methods:**\n  - `shape_data() -> typing_extensions.Dict[str, typing_extensions.List[float]]`: Returns the shape data of the visual shape (e.g. normal vector for a plane).\n  - **Properties:**\n    - `visual_geometry_type`: `pycram.datas\n\ntructures.enums.Shape`: Returns the visual geometry type of the visual shape (e.g. plane).\n\n#### **State**\n   Bases: `abc.ABC`\n   Helper class that provides a standard way to create an ABC using inheritance.\n\n#### **LinkState**\n   Bases: `State`\n   Helper class that provides a standard way to create an ABC using inheritance.\n\n- **Attributes:**\n  - `constraint_ids`: `typing_extensions.Dict[pycram.description.Link, int]`\n\n#### **JointState**\n   Bases: `State`\n   Helper class that provides a standard way to create an ABC using inheritance.\n\n- **Attributes:**\n  - `position`: `float`\n\n#### **ObjectState**\n   Bases: `State`\n   Helper class that provides a standard way to create an ABC using inheritance.\n\n- **Attributes:**\n  - `pose`: `pycram.datastructures.pose.Pose`\n  - `attachments`: `typing_extensions.Dict[pycram.world_concepts.world_object.Object, pycram.world_concepts.constraints.Attachment]`\n  - `link_states`: `typing_extensions.Dict[int, LinkState]`\n  - `joint_states`: `typing_extensions.Dict[int, JointState]`\n\n#### **WorldState**\n   Bases: `State`\n   Helper class that provides a standard way to create an ABC using inheritance.\n\n- **Attributes:**\n  - `simulator_state_id`: `int`\n  - `object_states`: `typing_extensions.Dict[str, ObjectState]`"}]}
{"messages": [{"role": "system", "content": "You are a coding expert and know how to write PyCRAM Plan Code."}, {"role": "user", "content": "What are the available enums in the pycram.datastructures.enums section of the PyCRAM documentation?"}, {"role": "assistant", "content": "pycram.datastructures.enums \n================================\n\nModule holding all enums of PyCRAM.\n\n### **Classes**\n\n- **ExecutionType**  \n  Enum for Execution Process Module types.\n\n- **Arms**  \n  Enum for Arms.\n\n- **TaskStatus**  \n  Enum for readable descriptions of a tasks' status.\n\n- **JointType**  \n  Enum for readable joint types.\n\n- **Grasp**  \n  Enum for Grasp orientations.\n\n- **ObjectType**  \n  Enum for Object types to make it easier to identify different objects.\n\n- **State**  \n  Enumeration that describes the result of a language expression.\n\n- **Shape**  \n  Enum for visual shapes of objects.\n\n- **WorldMode**  \n  Enum for the different modes of the world.\n\n- **AxisIdentifier**  \n  Enum for translating the axis name to a vector along that axis.\n\n- **GripperState**  \n  Enum for the different motions of the gripper.\n\n- **GripperType**  \n  Enum for the different types of grippers.\n\n- **PerceptionTechniques**  \n  Enum for techniques used in perception tasks.\n\n- **ImageEnum**  \n  Enum for image switch views on the HSRB display.\n\n**Module Contents**  \n\n### **Class Details**\n\n#### **ExecutionType**\n   Bases: `enum.Enum`  \n   Enum for Execution Process Module types.\n\n- **Attributes:**\n  - `REAL`\n  - `SIMULATED`\n  - `SEMI_REAL`\n\n#### **Arms**\n   Bases: `enum.Enum`  \n   Enum for Arms.\n\n- **Attributes:**\n  - `LEFT`\n  - `RIGHT`\n  - `BOTH`\n\n#### **TaskStatus**\n   Bases: `enum.Enum`  \n   Enum for readable descriptions of a tasks' status.\n\n- **Attributes:**\n  - `CREATED` = 0\n  - `RUNNING` = 1\n  - `SUCCEEDED` = 2\n  - `FAILED` = 3\n\n#### **JointType**\n   Bases: `enum.Enum`  \n   Enum for readable joint types.\n\n- **Attributes:**\n  - `REVOLUTE` = 0\n  - `PRISMATIC` = 1\n  - `SPHERICAL` = 2\n  - `PLANAR` = 3\n  - `FIXED` = 4\n  - `UNKNOWN` = 5\n  - `CONTINUOUS` = 6\n  - `FLOATING` = 7\n\n#### **Grasp**\n   Bases: `enum.Enum`  \n   Enum for Grasp orientations.\n\n- **Attributes:**\n  - `FRONT` = 0\n  - `LEFT` = 1\n  - `RIGHT` = 2\n  - `TOP` = 3\n\n#### **ObjectType**\n   Bases: `enum.Enum`  \n   Enum for Object types to make it easier to identify different objects.\n\n- **Attributes:**\n  - `METALMUG`\n  - `PRINGLES`\n  - `MILK`\n  - `SPOON`\n  - `BOWL`\n  - `BREAKFAST_CEREAL`\n  - `JEROEN_CUP`\n  - `ROBOT`\n  - `ENVIRONMENT`\n  - `GENERIC_OBJECT`\n  - `HUMAN`\n\n#### **State**\n   Bases: `enum.Enum`  \n   Enumeration that describes the result of a language expression.\n\n- **Attributes:**\n  - `SUCCEEDED` = 1\n  - `FAILED` = 0\n  - `RUNNING` = 2\n  - `INTERRUPTED` = 3\n\n#### **Shape**\n   Bases: `enum.Enum`  \n   Enum for visual shapes of objects.\n\n- **Attributes:**\n  - `SPHERE` = 2\n  - `BOX` = 3\n  - `CYLINDER` = 4\n  - `MESH` = 5\n  - `PLANE` = 6\n  - `CAPSULE` = 7\n\n#### **WorldMode**\n   Bases: `enum.Enum`  \n   Enum for the different modes of the world.\n\n- **Attributes:**\n  - `GUI` = 'GUI'\n  - `DIRECT` = 'DIRECT'\n\n#### **AxisIdentifier**\n   Bases: `enum.Enum`  \n   Enum for translating the axis name to a vector along that axis.\n\n- **Attributes:**\n  - `X` = (1, 0, 0)\n  - `Y` = (0, 1, 0)\n  - `Z` = (0, 0, 1)\n\n#### **GripperState**\n   Bases: `enum.Enum`  \n   Enum for the different motions of the gripper.\n\n- **Attributes:**\n  - `OPEN`\n  - `CLOSE`\n\n#### **GripperType**\n   Bases: `enum.Enum`  \n   Enum for the different types of grippers.\n\n- **Attributes:**\n  - `PARALLEL`\n  - `SUCTION`\n  - `FINGER`\n  - `HYDRAULIC`\n  - `PNEUMATIC`\n  - `CUSTOM`\n\n#### **PerceptionTechniques**\n   Bases: `enum.Enum`  \n   Enum for techniques used in perception tasks.\n\n- **Attributes:**\n  - `ALL`\n  - `HUMAN`\n  - `TYPES`\n\n#### **ImageEnum**\n   Bases: `enum.Enum`  \n   Enum for image switch views on the HSRB display.\n\n- **Attributes:**\n  - `HI` = 0\n  - `TALK` = 1\n  - `DISH` = 2\n  - `DONE` = 3\n  - `DROP` = 4\n  - `HANDOVER` = 5\n  - `ORDER` = 6\n  - `PICKING` = 7\n  - `PLACING` = 8\n  - `REPEAT` = 9\n  - `SEARCH` = 10\n  - `WAVING` = 11\n  - `FOLLOWING` = 12\n  - `DRIVINGBACK` = 13\n  - `PUSHBUTTONS` = 14\n  - `FOLLOWSTOP` = 15\n  - `JREPEAT` = 16\n  - `SOFA` = 17\n  - `INSPECT` = 18\n  - `CHAIR` = 37"}]}
